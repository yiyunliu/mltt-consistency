\newif\ifcomments     %% include author discussion
\newif\ifanonymous    %% include author identities
\newif\ifextended     %% include appendix
\newif\ifsubmission   %% prepare the submitted version
\newif\ifpublic       %% version available for posting / final version

\commentsfalse        %% toggle comments here
\extendedfalse
\anonymoustrue

\submissionfalse     %% at most one of these must be true (neither for draft version)
\publicfalse         %% but if you want to see comments, these should both be off


% If we are going to make a version public, i.e. on arXiv, we should
% make sure that there are no comments and our names are on it.
\ifpublic
\submissionfalse
\commentsfalse
\anonymousfalse
\fi

%% If we are submission, make sure there are no comments and our names
%% are NOT on it.
\ifsubmission
\publicfalse
\commentsfalse
\anonymoustrue
\fi


%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `acmsmall')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen=true,
\ifpublic review=false\else,review=true\fi
  ,anonymous=\ifanonymous true\else false\fi]{acmart}
\usepackage{ottalt}
\usepackage{minted}
\usepackage{xspace}
\usepackage[para]{footmisc}
\newcommand{\dotv}[2]{\href{#1}{\texttt{#1}}{\texttt{:#2}}}
\newcommand{\lang}{$\lambda^{\Pi}$\xspace}
\inputott{rules}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% to allow unicode characters in minted code
\usepackage{newunicodechar}
\newunicodechar{â‡’}{$\Rightarrow$}

\usepackage{draft}

\ifcomments
\newnote{scw}{blue} % Stephanie Weirich
\newnote{yl}{purple} % Yiyun Liu
\else
\newcommand{\scw}[1]{}
\newcommand{\yl}[1]{}
\fi


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Functional Pearl: Short and Mechanized Logical Relation for Dependent Type Theories}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Yiyun Liu}
\orcid{0009-0006-8717-2498}
\affiliation{
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \country{USA}
}
\email{liuyiyun@seas.upenn.edu}

\author{Stephanie Weirich}
\orcid{0000-0002-6756-9168}
\affiliation{
  \institution{University of Pennsylvania}
  \city{Philadelphia}
  \country{USA}
}
\email{sweirich@seas.upenn.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Proof by logical relations is a powerful technique that has been used
to derive metatheoretic properties of type systems, such as
consistency and parametricity. While there exists a
plethora of introductory materials about logical relation in the
context of simply typed or polymorphic lambda calculus, a streamlined
presentation of proof by logical relation for a dependently typed language
is lacking. In this paper, we present a short
consistency proof for a dependently typed language that contains a
rich set of features, including a countable universe
hierarchy, booleans, and an intensional identity type. We show that
the logical relation can be easily extended to prove the existence of
$\beta\eta$-normal forms.
We have
fully mechanized the consistency proof using the Coq proof assistant
in under 1000 lines of code, with 500 lines of additional code for the
$\beta\eta$-normal form extension.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Logical Relation, Dependent Types, Logical Consistency, Coq}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
This paper presents a \emph{short} and \emph{mechanized} proof of
logical consistency for \lang{}, a dependent type theory with a full
predicative universe hierarchy, large eliminations, an intensional identity
type, a boolean base type, and dependent elimination forms.

Our goal with this work is to demonstrate the application of the proof
technique of \emph{syntactic logical relations} to dependent type theories.
Logical relations are a powerful proof technique, and have been used to show
diverse properties such as strong normalization~\citep{girard1989proofs,
  geuvers1994short}, contextual equivalence~\citep{constable1986implementing},
representation independence~\citep{pitts1998existential},
noninterference~\citep{bowman2015noninterference}, compiler
correctness~\citep{benton2009biorthogonality,perconti2014compiler}, and the
decidability of conversion
algorithms~\citep{harper2005equivalence,Abel12,abel2013normalization}.

However, tutorial material on syntactic logical
relations~\citep{skorstengaard2019introduction, harpertait, harperkripke,
  pierce2002types, pierce2004advanced,harper2016practical} is primarily
focused on systems with simple or polymorphic types. In that context,
syntactic logical relations can be defined as simple recursive functions over
the structure of types, or (in the case of recursive types) defined over the
evaluation steps of the computation. Yet, neither of these techniques can be
used to define a logical relation in the context of a predicative dependent
type theory, so a novice researcher might be excused for thinking that proofs
that use logical relations are not applicable for such languages.

But this is not the case. 
% \scw{Add a sentence about pen-and-paper LR proofs
%  for dependent type theories?}\yl{There is a long paragraph in Sec 8 about how
%  pnp proofs like Geuvers' can't be easily mechanized, but I'm not sure if we
%  can also argue that they are hard to read, too (I personally think that's the
%  case, but I don't know if the reviewers will agree)} 
Recent authors have developed tour-de-force
mechanizations for the metatheory of modern proof
assistants~\citep{nbeincoq,decagda,martin-lof-a-la-coq,anand2014towards}, and
have relied on logical relations defined as part of their
developments. However, because these proofs show diverse results about real
systems and algorithms, these developments range in size from 10,000 to
300,000 lines of code. As a result, their uses of logical relations are
difficult to isolate from the surrounding contexts and inaccessible
to casual readers.

Thus, our paper provides a gentle and accessible introduction to a powerful
technique for dependent type theories. To promote the use of machine-assisted
reasoning, our development is accompanied by a short mechanized proof script,
of less than 1000 lines of code, developed using the Coq proof
assistant~\cite{coq}.

We have streamlined our proof through a number of means: the careful selection
of the features that we include in the object type system and the results that
we prove about it, in addition to the judicious use of automation.  Our
language is small, but includes enough to be illustrative. For example, we
eschew inductive datatypes or W-types, but we do include propositional
equality and booleans to capture the challenges presented by indexed types and
dependent pattern matching. We do not show the decidability of type checking,
nor do we develop a PER semantics, but we prove logical consistency, which
states that empty types are not inhabited in an empty context, and demonstrate
how our consistency proof can be extended (at a moderate cost of 500 lines of
code) to show the existence of $\beta\eta$-normal forms for well-typed open
\emph{and} closed terms. We include a full predicative universe hierarchy and
type-level computation to demonstrate the logical strength of the approach.

%%
%% SCW: save specific comparisons with other systems for later
%%
% The big difference in the size of the developments does not
% necessarily imply that our proof technique leads to a more concise
% proof due to the differing expressiveness of the languages and the
% different metatheoretic results being established. For
% example, \citet{anand2014towards} mechanizes the metatheory of
% Nuprl~\citep{constable1986implementing}, which, on top of all the
% features that \lang{} supports, includes W-types and partial types and
% requires a PER semantics to model its extensional typed
% equality. The object language from \citet{decagda} does not support
% identity types and only has one predicative universe, but additionally
% supports $\Sigma$ types. However, for a researcher who wishes to learn
% the underlying techniques for mechanized logical relation for
% dependent type theory, a small development like ours is much easier to
% navigate and understand. Furthermore, we show how our simple
% consistency proof can be easily extended to show the existence of
% $\beta\eta$-normal form for well-typed open \emph{and} closed terms,
% giving us a metatheoretic result almost as strong as the one from
% \citet{decagda} at the moderate cost of around 500 lines of extra code.

More concretely, our paper makes the following contributions.
\begin{itemize}
\item In Section~\ref{sec:spec}, we introduce \lang{}, the dependent type
  theory of interest. A key design choice that impacts our proofs is the use
  of an untyped conversion rule, inspired by Pure Type
  Systems~\citep{barendregt1991introduction}, and specified through parallel
  reduction~\citep{takahashi-parallel-reduction,
    barendregt:lambda-calculi-with-types}.
\item In Section~\ref{sec:logreldep}, we formulate logical consistency for
  \lang{}, the property of interest, to motivate a logical relation. We define
  the relation first inductively and then later prove that it is a partial
  function. Based on this definition, we define semantic typing and prove the
  fundamental theorem, from which consistency follows as a corollary
  (Section~\ref{sec:logrelproof}). Thanks to the design of \lang{}, our proof
  showcases the special treatment required to model many of the most common
  features of dependent type theories, thus making our proof applicable to a
  broad range of type systems.
\item We strengthen our logical relation to prove the existence of
  $\beta$-normal forms (Section~\ref{sec:extension}) and $\beta\eta$-normal
  forms (Section~\ref{sec:betaeta}) for well-typed open terms. The
  modifications made to our initial logical relation are small and closely
  mirror the necessary extensions for a simply-typed language. We use this
  part to show that once we have established the base techniques, we can port
  ideas from proofs about simpler languages to the dependently typed setting.
\item We mechanize all our proofs using the Coq proof assistant, with 957
  lines of code for the consistency proof and a moderate increase to 1568
  lines of code for the normalization proof.  We discuss the specifics related
  to our choice of Coq as our metatheory, including our use of off-the-shelf
  semantic engineering infrastructure and automation tools, in
  Section~\ref{sec:logrelmech}.  Our proof scripts, with comments, are
  available to reviewers as supplementary materials.
\item We compare our work to existing proofs by logical relations and other
  proof techniques for proving consistency and normalization.  We provide an
  overview of this prior work (Section~\ref{sec:relatedwork}) and also give an
  in-depth explanation on how various design decisions affect the size of our
  proof and its extensibility to additional features
  (Section~\ref{sec:discuss}).
\end{itemize}

The result of our work is an artifact that an interested researcher can
navigate and understand. We accompany this short mechanized proof with an
informal description, presented here using set-theoretic notation and
terminology so that it is accessible to readers with a general mathematical
background.  That said, our explanations do not stray too far away from our
proof scripts.  We link each lemma directly to its counterpart in the proof
script, anticipating that readers may wish to see how these results may be
expressed and verified in a proof assistant.  The typeset proofs purposefully
follow the mechanized proofs closely while avoiding, as much as possible,
artifacts specific to Coq.

Not only does this close connection aid readers that wish to, like us, adopt
proof assistants for their day-to-day use, but we also find that this
precision is important for conveying the proof technique itself. Unlike
properties that are derivable through syntactic means, proofs by logical
relations make demands on the strength of the metalogic in which they are
expressed. An informal proof that attempts to be agnostic or ambiguous about
the underlying metatheory requires substantial effort from the reader to
understand whether it is definable in a given ambient logic.

%% SCW: I think we said this already in the 2nd paragraph above.
% Finally, our approach does not result in a verbose proof on paper, thanks to
% the already short mechanization. While we do make a few simplifications
% to our typeset proof, such as avoiding the clunky syntax for well-founded
% recursion in Coq, we keep the overall structure of our typeset proofs
% consistent with our mechanization.


\section{Specification of a Dependent Type Theory}
\label{sec:spec}

% \scw{This section needs to say:
%  \begin{itemize}
% \item definitional equality is untyped so that we can prove
%  properties about it independent of the type system
% \item definitional equality is based on parallel reduction so that we can take
%  advantage of the algorithmic structure later. This definition is easier to
%  invert because there are fewer derivation. That is important when reasoning
%  about our logical relation.
% \item can prove that parallel reduction is equivalent to other definitions of
%  equality using standard means (cite Barendregt's book).  (we go halfway
%  there by showing it is an equivalence relation). Can also prove equivalent
%  to typed relation. (cite Siles, also CoreSpec?)
% \item All of the proofs in this section are standard, well known, and use
%  techniques that are well-suited for proof assistants. In fact, de Bruijn's
%  paper that introduces de Bruijn indices was part of a mechanized confluence
%  proof for the untyped lambda calculus.
% \item extensions: forward reference to eta equivalence for functions in later
%  section. type-directed equivalence is future work
%  \end{itemize}
%}

\begin{figure}[h]
\[
\begin{array}{lcll}
% \mathit{Natural\ numbers}\\
% [[i]],[[j]],[[n]] & \in &  [[SNat]] &  \\ \\
\mathit{Terms}\\
[[a]],[[b]],[[c]],[[p]],[[A]],[[B]] & ::= & [[Set i]]\ |\ [[x]]\  |\ [[Void]]
                  & \mbox{universes, variables, empty type} \\
            & |   & [[Pi x : A . B]]\ |\ [[\ x . a]]\ |\ [[a b]]
                  & \mbox{function types, abstractions, applications} \\
            & |   & [[a ~ b : A ]]\ |\  [[refl]]\ |\ [[J c a b p]]
                  & \mbox{equality types, reflexivity proof, J eliminator} \\
            & |   & [[Bool]]\ |\  [[true]]\ |\  [[false]]\ % |\  [[if a b0 b1]]
                  & \mbox{boolean type, true, false} \\
            & |   & [[if a b0 b1]]
                  & \mbox{conditional expression} \\ \\
\end{array}
\]
\[
\begin{array}{lll}
\mathit{Substitutions}      & \qquad\qquad & \mbox{\emph{Typing Contexts}}  \\
[[rho]] \in [[SVar -> STm]] & & [[G]] ::= [[empty]]\ |\ [[G ++ x : A]] 
\end{array}
\]
  \caption{Syntax of \lang}
  \label{fig:syntax}
\end{figure}


In this section, we present the dependent type theory, \lang{}, whose logical
consistency will be proven in Section~\ref{sec:logrelproof}.

\scw{Need to acknowledge the lack of type annotation on abstractions.}

The syntax of \lang can be found in Figure~\ref{fig:syntax}.  As a dependent
type theory, terms and types are collapsed into the same syntactic
category. The type $[[Set i]]$ represent a universe type, annotated by its
universe level, a natural number $i$.  Abstractions $[[ \ x . a ]]$ and
dependent function types $[[Pi x : A . B]]$ are binding forms for the variable
$x$ in the body of the function and codomain of the function type.
\footnote{In the exposition in this paper, binding forms are equal up to
  $\alpha$-conversion and we adopt the Barendregt Variable
  Convention~\cite{barendregt:lambda-calculus}, which lets us assume that
  bound variables are distinct.  In some places, we are informal about the
  treatment of variables and substitution; our mechanized proofs make these
  notions precise by using de Bruijn indices~\cite{debruijn1994automath}. }
We use the notation $[[A -> B]]$ when the output type $[[B]]$ is not dependent
on the input variable. For simplicity, we omit the type annotations in the abstraction
forms. We discuss how the inclusion of type annotations can affect
our development in Section~\ref{sec:betaeta}, where we extend our
consistency result to the existence of $\beta\eta$-normal forms.
We include in \lang{} the intensional identity type
$[[a ~ b : A]]$ whose proofs can be eliminated by the J-eliminator
$[[J c a b p]]$, where $[[p]]$ is an equality proof between $[[a]]$ and
$[[b]]$, and $[[c]]$ is the term whose type is to be casted.  Finally, \lang{}
includes booleans, with standard syntax.  \scw{Example of what you can use
  identity types for? Or an example of a program that uses J?}


Our reduction and typing relations are defined in terms of \emph{simultaneous
  substitutions}, $[[rho]]$, which are mappings from variables to terms.  We
use $[[idtm]]$ as the identity substitution.  The extension operation,
$[[(rho .: x -> a)]]$, updates the substitution $[[rho]]$ to map the variable
$[[x]]$ to $[[a]]$ rather than $[[rho x]]$.

The substitution operator, which takes the form $[[a {rho}]]$, traverses the
syntax of $[[a]]$ and replaces each variable $[[x]]$ with the term
$[[rho x]]$. When traversing under binders (e.g. in the
$[[(\ x . a) { rho }]]$ case), it must be the case that $[[rho]]$ maps the
bound variable to itself and that the bound variable does not appear freely in
the application of the substitution to any other variable.
%
\scw{Maybe we should handwave more here, and note that we aren't covering all
  of the details of the treatment of variable binding in the text. If readers
  want to understand that part, they should look at the Coq code, which uses
  de Bruijn indices anyways. Our goal is to convey the understanding of the
  Coq proof. }
\yl{I agree. Should mention upfront that the presentation is not
  watertight and rigorous, though we have one in our mechanization}

The substitution operator is referred to as simultaneous substitution as it
substitutes for all variables at once. It is possible to recover single
substitution by composing the extension operator and the identity
substitution: $[[a { b / x }]] := [[a { idtm .: x -> b }]]$.

When reasoning about logical relations, we find it more convenient to
formulate simultaneous substitution directly rather than recovering it from
single substitution.  In particular, this shows up in the definition of
semantic typing in Section~\ref{sec:logrelproof}, which relies on simultaneous
substitution.

\subsection{Definitional equality via parallel reduction}

\begin{figure}[h]
\begin{minipage}{0.9\textwidth}
\drules[P]{$[[a => b]]$}{Parallel Reduction}{AppAbs, IfTrue, IfFalse, JRefl}
% \drules[PS]{$[[a =>+ b]]$}{Transitive Closure of Parallel Reduction}{Refl, Step}
% \drules[C]{$[[a <=> b]]$}{Convertibility}{Intro}
\end{minipage}
\caption{Parallel reduction ($\beta$-rules only) }
\label{fig:par}
\end{figure}

Before we specify the typing rules, we first specify the equational theory
used in the conversion rule (\rref{T-Conv} in Figure~\ref{fig:typing}). The
equivalence relation used in this rule is often referred to
\textit{definitional equality} in dependent type theories because it defines
the equivalence that the syntactic type system works up to. 

In \lang{}, we use a relation called \emph{convertibility} for definitional
equality. Two terms are convertible, if they reduce to a common form. The
reduction that we use is called \emph{parallel reduction}, written
$[[a => b]]$. The notation $[[a =>+ b]]$ indicates the transitive closure of
parallel reduction.

\begin{definition}[Convertibility]
  Two terms $[[a0]]$ and $[[a1]]$ are \emph{convertible}, written
  $[[a0 <=> a1]]$, if there exists some term $[[b]]$ such that $[[a0 =>+ b]]$
  and $[[a1 =>+ b]]$.
\end{definition}

The definition of the parallel reduction relation, appears in
Figure~\ref{fig:par}. (For brevity, the reflexivity and congruence rules of
this relation are omitted from this figure).

We prove, through standard
techniques~\citet{takahashi-parallel-reduction,plfa22.08}, the following
properties of parallel reduction.
\begin{lemma}[Par Refl\footnote{\dotv{join.v}{Par\_refl}}]
  \label{lemma:parrefl}
  For all terms $[[a]]$, $[[a => a]]$.
\end{lemma}
\begin{lemma}[Par cong\footnote{\dotv{join.v}{par\_cong}}]
  \label{lemma:parcong}
  If $[[a0 => a1]]$ and $[[b0 => b1]]$, then $[[a0 { b0 / x } => a1 {
    b1 / x }]]$.
\end{lemma}
\begin{corollary}[Par subst\footnote{\dotv{join.v}{par\_subst}}]
  \label{lemma:parsubst}
  If $[[a0 => a1]]$, then $[[a0 {b / x } => a1 {b / x}]]$ for arbitrary $[[b]]$.
\end{corollary}
\begin{lemma}[Par diamond\footnote{\dotv{join.v}{par\_confluent}}]
  \label{lemma:pardiamond}
  If $[[a => b0]]$ and $[[a => b1]]$, then there exists some term
  $[[c]]$ such that $[[b0 => c]]$ and $[[b1 => c]]$.
\end{lemma}

Convertibility is an equivalence relation. The key step in
proving transitivity is showing the diamond property for parallel reduction.

\begin{lemma}[Convertibility refl\footnote{\dotv{join.v}{Coherent\_reflexive}}]
  \label{lemma:coherencerefl}
  For all terms $[[a]]$, $[[a <=> a]]$.
\end{lemma}
\begin{lemma}[Convertibility sym\footnote{\dotv{join.v}{Coherent\_symmetric}}]
  \label{lemma:coherencesym}
  If $[[a <=> b]]$, then $[[b <=> a]]$.
\end{lemma}
\begin{lemma}[Convertibility trans\footnote{\dotv{join.v}{Coherent\_transitive}}]
  \label{lemma:coherencetrans}
  If $[[a0 <=> a1]]$ and $[[a1 <=> a2]]$, then $[[a0 <=> a2]]$.
\end{lemma}

The convertibility relation that we use for conversion in \lang{} is unusual
in that it is directly defined via parallel reduction, instead of using the
related notion of $\beta$-equivalence~\citep{barendregt1991introduction,coquand1990:cic}. This choice
does not change the language definition; a detailed argument of the
equivalence between $[[a <=> b]]$ and untyped $\beta$-equivalence can be found
in \citet{barendregt:lambda-calculi-with-types} and
\citet{takahashi-parallel-reduction}. However, this choice simplifies later
proofs, as we discuss in Section~\ref{sec:discuss}.

Our definition of equality is untyped: the judgement does not require the two
terms to type check and have the same type. The use of an untyped relation for
conversion is similar to Barendregt's Pure Type
Systems~\cite{barendregt1991introduction} and differs
from MLTT~\citep{Martin-Lof-1973}, where the judgmental
equality takes the form $\Gamma \vdash a \equiv b : A$.
% from which one can
% usually derive $[[G |- a : A]]$ and $[[G |- b : A]]$ after proving subject
% reduction.
By working with an untyped judgement, we can establish its
properties independently from the type system and the logical relation, using
well-established syntactic approaches.
%
\citet{siles2012pure} show the equivalence of Barendregt's Pure Type
System, which employs untyped equality, and
its variant that uses typed judgmental equality. This assures us that we do
not lose generality working with a system with untyped
conversion.
\yl{removed the eta equivalence sentence because it might give the
  wrong message that we extended Siles' work with eta:
 Furthermore, we discuss how this definition can be extended with
 $\eta$-equivalence of functions in Section~\ref{XXX} }
We compare this
definition with type-directed approaches to equality in
Section~\ref{sec:discuss}.


\subsection{Syntactic Typing}

\begin{figure}[h]
\begin{minipage}{0.9\textwidth}
\drules[Ctx]{$[[ |- G]]$}{Context Well-Formedness}{}
\[ \drule{Ctx-Empty} \qquad \drule[width=3in]{Ctx-Cons} \]
\drules[T]{$[[G |-  a : A]]$}{Typing}{Var, Set, Pi, Abs, App, Conv,
 Void, Bool, True, False}
\[ \drule[width=5in]{T-If}\]
\[ \drule[width=3in]{T-Eq} \qquad \drule{T-Refl} \]
\[ \drule[width=4in]{T-J} \]
\end{minipage}
\caption{Syntactic typing for \lang}
\label{fig:typing}
\end{figure}

Figure~\ref{fig:typing} gives the full typing rules for \lang{}. The premises
wrapped in \colorbox{lightgray}{gray} boxes can be shown to be admissible
syntactically, though some of them are required to strengthen the inductive
hypothesis of the fundamental theorem.

The typing rules of \lang{} are standard for dependent
type theories.  The variable rule, \rref{T-Var}, uses the auxiliary relation
$[[x : A in G]]$, that holds when a variable declaration is found in the
typing context.  The typing of universes ensures that each one belongs to the
next higher level.  \Rref{T-Pi} ensures predicative quantification by
requiring that all parts of the type be typeable at the same universe
level. \Rref{T-Abs} ensures that all functions have well-formed dependent
types. In an application (\rref{T-App}) the argument is substituted for the
variable in the result type.

\Rref{T-Conv} uses the convertibility relation from earlier
as our equality judgment for type conversion.

\scw{Cassia did not follow our discussion of dependent pattern matching. We need to expand}
The elimination form for booleans, \rref{T-If}, demonstrates dependent pattern
matching.  The result type of this expression, $[[A {a / x}]]$, is composed of some
\emph{motive} $[[A]]$, a type where its single free variable has been replaced
with the condition of the if expression. When typing the true branch, this
substitution replaces the variable by $[[true]]$, and similarly for the false
branch. As a result, the type system communicates the information gained from
the test to each of the branches of the expression.

A similar sort of dependent pattern matching occurs when eliminating identity
types. Such types are checked for well-formedness with \rref{T-Eq} and
introduced by \rref{T-Refl}. In \rref{T-J}, the elimination form, the subterm
$p$ is a proof of an equality between $[[a]]$ and $[[b]]$. The subterm $[[c]]$
is the body of the elimination form. In this rule, $[[B]]$ is the motive and
has two free variables.  When checking $[[c]]$, the substitution for these
variables changes from $[[b]]$ to $[[a]]$ and from $[[p]]$ to $[[refl]]$,
witnessing the information gained through dependent pattern matching.

The universe hierarchy and the boolean base type gives \lang{} the
ability to compute a type using a term as input, a feature commonly referred to
as large elimination. For
example, we can type check the function $[[\ x . if x Bool Void]]$,
which returns either $[[Bool]]$ or $[[Void]]$ depending on whether its
input is $[[true]]$ or $[[false]]$.
% Working with a system with untyped equality has the huge benefit that
% the confluence result for untyped parallel reduction
% (Lemma~\ref{lemma:pardiamond}) is easily derivable without having to
% resort to the complex syntatic (resp. semantic) technique from
% \citet{siles2012pure} (resp. \citet{decagda}) to resolve
% the circularity of subject reduction and $\Pi$-injectivity.
% Section~\ref{sec:extension} explains how we
% generalize our technique to include $\eta$-law for functions and
% show the existence of normal form for well-typed (open and closed)
% terms, achieving a similar level of expressiveness of the type system
% and strength of metatheoretic property as \citet{decagda}.

% Finally, since our system has an infinite universe hierarchy, we can
% present the system Ã  la Russell by using the same judgment form
%$[[G |- a : A]]$ regardless of whether $[[a]]$ is a term or a type. There
% is no need to distinguish between big types and small types
% and duplicate our typing specification.
% \scw{Is there an advantage for Tarski universes even with infinite
% hierarchy? or no?}




% working with a system with untyped equality not only preserves the
% same level of generality,
% Of course,
% but also enables us to derive confluence
% (Lemma~\ref{lemma:parconfluent}) early on without having to use the
% intricate techniques from \citet{lemma:}







% TODO, where terms ... and ... are known to be
% well-typed. The equivalence of such systems and a system that uses
% untyped equality are explored in detail in ...

% Without fancy eta laws, it is easy to embed a typed language into an
% untyped language.


% We note that a more conventional presentation of
% \rref{T-Conv} would instead use full beta reduction as the base for
% the definition of coherence. However, since full beta reduction
% doesn't satisfy the diamond property, one typically needs parallel
% reduction as an auxilliary definition to derive the confluence of full
% beta reduction. Our formulation of \lang through parallel reduction
% is slightly more economical.

\section{Logical Relation}
\label{sec:logreldep}

% \scw{This section pre-supposes that we want to define a logical relation,
% but doesn't precisely state what we want to use it to prove, and why a logical
% relation is suitable. (And why the property you want to prove is difficult to
% show!) Should add more motivation here.}
\scw{We need to explicitly point out that the key ideas of this paper are
  discussed, here, in this section.
  We need to explicitly remark on why logical relations are difficult to
  define for dependent type theory and explain why this setting is more
  difficult than with simple types (STLC) or with polymorphic types (System F).
  \begin{itemize}
  \item Large eliminations
  \item Definitional equality (not all types look like types)
  \end{itemize}
  Should we be more explicit in our comparison with Girard's trick for polymorphic type?
  There, the definition stays recursive because it doesn't substitute for the variables
  in the function types. But that approach is not available in this setting, because not
  all quantified things are types. And we might need that information to interpret, say,
  identity types in the right way.
}
\scw{ We also need to explicitly point out that our logical relation is untyped.
  This has two benefits: it allows semantic typing to be meaningful independent from
  syntactic typing (cite Derek, forward reference to next section) and it avoids
  significant bookkeeping, especially in the case of Kripke logical relations (we need to define
  what these are).
  Is there a cost to an untyped relation?
}

Before we define our logical relation, we first formally specify the
consistency property that we want to prove.
\begin{theorem}[Logical Consistency]
  \label{theorem:consistency}
  The judgment $[[empty |- a : Void ]]$ is not derivable.
\end{theorem}
The property can be formulated in a simply typed language, where
$[[Void]]$ is similarly defined as a type that has no term. A related
property, referred to as the termination property (for closed terms),
is commonly used in introductory materials such as
\citet{skorstengaard2019introduction}, \citet{pierce2002types}, and
\citet{harpertait} to motivate the need for a logical relation.

A naive attempt to proving Theorem~\ref{theorem:consistency} by
induction on the derivation $[[empty |- a : Void]]$ would succeed at
almost all cases except for \rref{T-App}. In the application
case, we are given $[[empty |- b : Pi x : A . B]]$ and $[[empty |- a : A]]$, and
the equality that $[[B {a / x} = Void]]$. Our goal is to show that
$[[empty |- b a : Void]]$ is not possible. However, note that there is
nothing we know of $[[b]]$ or $[[a]]$ from the induction hypothesis
because neither $[[Pi x : A . B]]$ nor $[[A]]$ is equal to $[[Void]]$.
We have no way of deriving a contradiction from $[[empty |- b a :
Void]]$. The takeaway from this failed attempt is that, in order to
derive the consistency, we need to know something about types other
than $[[Void]]$. From a pragmatic point of view, proof by logical
relation can be seen as a sophisticated way of strengthening the
induction hypothesis. From the strengthened property, the fundamental
theorem, we will be able to derive consistency as a corollary.

The complexity of applying proof by logical relation to dependent types stems
from the fact that the logical relation is much harder to define. In
simply typed languages, the logical relation is defined as a recursive
function over the type $[[A]]$.
In dependent types, the type
$[[A]]$ can take the form $[[(\ x . x ) Bool]]$. To assign meaning to
this type, we need to first reduce it to $[[Bool]]$. However, we
cannot write a function that performs the reduction because we do not
know the termination of well-typed terms a priori. As a result, we
define the logical relation as an inductively defined relation,
reminiscent of how we specify the reduction graph of a partial
function; the functionality of the relation can later be recovered in Lemma~\ref{lemma:logreldeter}.

\subsection{Definition of the Logical Relation}
\begin{figure}[h]
\drules[I]{$[[Interp I i A S]]$}{Logical Relation}{Set, Void, Bool, Eq, Red}
\[ \drule[width=5in]{I-Pi} \]
\caption{Logical relation for \lang}
\label{fig:logrel}
\end{figure}
The logical relation for \lang{}, which takes the form $[[Interp I i A
S]]$, is defined as an inductively generated relation (Figure~\ref{fig:logrel}).
% The logical relation takes the form $[[Interp I i A S]]$.
Metavariables $[[A]]$ and $[[i]]$ stand for terms and natural
numbers respectively, as introduced earlier in
Figure~\ref{fig:syntax}.
% \scw{Many introductory texts define the relation as a recursive function
% over type structure, or step-indices. You use an inductive relation instead, why?}
%\yl{resolved}
% \scw{Can we view this inductive relation as the graph of the partial function
% that is defined recursively over types?} \yl{reolsved}
% \scw{What is this form extensible too? impredicative quantification? recursive
% types? } \yl{resolved? impredicativity is hard}
% \scw{Is it worth observing here that this definition is not over sets of typed
% terms. That it characterizes all terms that look like booleans (i.e. evaluate to
% true or false) or all terms that look like proofs (i.e. evaluate to refl). The
% fact that there is no connection between p and a and b in the I-Eq case is strange
% looking. Need to explain.  }
% \yl{resolved}
The metavariables $[[I]]$ and $[[S]]$ are
sets with the following signatures.
\[
    [[I]] \in [[ { j | j < i  } ->  PowerSet STm ]] \qquad\qquad\qquad
    [[S]] \in [[PowerSet STm]]
\]
The notation $[[PowerSet STm]]$ denotes the powerset of the set of
\lang{} terms.
The function $[[I]]$ is a family of sets of terms indexed by
natural numbers strictly less than the parameter $[[i]]$, which
represents the current universe level.  In \rref{I-Set}, the function
$[[I]]$ is used to define the meaning of
universes that are strictly smaller than the current level $[[i]]$. The
restriction $[[j < i]]$ in \rref{I-Set} ensures the predicativity of the system.

We tie the knot and obtain an interpretation for all universe levels
below. The judgment $[[InterpR i A S]]$ reads that the type $[[A]]$ is a
level-$[[i]]$ type \emph{semantically} inhabited by terms from the set
$[[S]]$. 
\begin{definition}[Logical relation for all universe levels]
\label{fig:logrelrec}
Define $[[InterpR i A S]]$ recursively through the well-foundedness of the $<$
relation on natural numbers.
\begin{equation*}
    [[InterpR i A S]] := [[ Interp I i A S  ]], \text{where } [[I j]]
    := [[{A | exists S , InterpR j A S}]] \text{ for } [[j < i]]
\end{equation*}
\end{definition}
Definition~\ref{fig:logrelrec} explains how the $[[j
< i]]$ constraint in \rref{I-Set} makes our system predicative; the
interpretation of the $[[i]]_{th}$ universe is only dependent on
universes strictly lower than $[[i]]$, which have already been defined.
This restriction ensures that that the relation is well-defined: without 
it the definition of $[[InterpR i A S]]$ would
not be well-founded; $[[Interp I i A S]]$ would call $[[I]]$ on
universe levels greater than or equal to $[[i]]$, which are yet to be defined.
%% SCW: removing the constraing wouldn't result in any system at all
% Removing the ordering constraint would result in a
% system where one can encode Girard's
% paradox~\citep{girard-thesis}. 


By unfolding Definition~\ref{fig:logrelrec}, we can
show that the same introduction rules for $[[Interp I i A S]]$ are
admissible for $[[InterpR i A S]]$. For example, 
we can prove the following derived rules:
\begin{center}
\drule[]{IR-Void} \qquad \drule[]{IR-Set}
\end{center}

In most informal presentations, instead of defining the logical
relation in two steps as we have shown above, the rules for $[[InterpR
i A S]]$ are given directly, with the implicit understanding that the
relation is an inductive definition nested inside a recursive
function over the universe level $[[i]]$. We choose
the more explicit definition not only because it is directly definable
in proof assistants that lack induction-recursion,
% where inductive definitions must appear
%at the top level,
but also because it makes clear the induction
principle we are allowed to use when reasoning about $[[InterpR i A
S]]$.

% The paragraph below might be useful but I don't know how to phrase
% it well

% The most general format of the induction principle over
% $[[InterpR i A S]]$ is first by strong induction over the universe level $[[i]]$
% followed by structural induction over $[[Interp I i A S]]$. As
% examples, ... (\rref{I-Set} and \rref{I-Red}).

We next take a closer look at the inductive relation $[[Interp I i A S]]$, defined 
in Figure~\ref{fig:logrel}.
\Rref{I-Void, I-Bool} capture terms that \emph{behave like} the inhabitants
of the $[[Void]]$ and $[[Bool]]$ types under an empty context. For
example, the $[[Void]]$ type should not have any inhabitants under the
empty context, where as the $[[Bool]]$ type only contains terms that
reduce to $[[true]]$ or $[[false]]$. Note that the characterization
of $[[Bool]]$ (and other inhabited types) in our logical relation does not always correspond to
well-typed or even closed terms. For example, the term $[[if false (#
Void true #)
true]]$ is ill-typed under the empty context but still belongs to the set $[[{ a | a =>+
  true \/ a =>+ false }]]$ since it evaluates to $[[true]]$.
The independence of syntactic typing in our logical relation allows
our semantic typing definition in Section~\ref{sec:logrelproof} to be
meaningful on its own. Furthermore, not having to embed scoping
information into the logical relation avoids extra bookkeeping and the
need for a Kripke-style logical relation when we extend our logical
relation to prove the existence of $\beta$-normal forms (Section~\ref{sec:extension}).
\yl{Not sure what to cite from Derek Dreyer. I know his blog post
  about semantic type soundness but is there a good paper to cite? one
of the rust papers?}

% unsurprising; when considering only closed terms, the empty type
% should not be inhabited and therefore corresponds to the empty set,
% whereas the boolean type is semantically inhabited by terms that
% evaluate to the boolean values $[[true]]$ or $[[false]]$.
\Rref{I-Eq}
says that an equality type $[[a ~ b : A]]$ corresponds to the
set of terms that reduce to $[[refl]]$ when $[[a <=> b]]$ also holds and
otherwise corresponds to the empty set. Conditions like $[[a <=>
b]]$ are typically required for indexed types, of which equality types
are an instance. \Rref{I-Red} enables us to reduce types in order
to assign meanings. Recall the type expression $[[(\ x . x )
Bool]]$. \Rref{I-Red} says that to know that
$[[Interp I i  (\ x . x) Bool S ]]$ for some $[[S]]$, it suffices to
show that $[[Interp I i Bool S]]$ since $[[(\ x . x) Bool =>
Bool]]$. The derivation that $[[Interp I i (\ x . x) Bool { a | a =>+
  true \/ a =>+ false }]]$ therefore follows by composing \rref{I-Red}
and \rref{I-Bool}.


\Rref{I-Pi} is the most complex rule in our logical
relation. Instead of explaining it directly, we first consider the following
simplified version, \rref{I-PiAlt}, that follows directly from \rref{I-Pi}.
\begin{center}
  \drule[]{I-PiAlt}
\end{center}
\Rref{I-PiAlt} directly captures the meaning of
a well-behaved dependent function type. The precondition of
the rule says that
the function type $[[Pi x : A . B]]$ has an interpretation if its input
type $[[A]]$ can be
interpreted as some set $[[S]]$, and for all terms $[[a in S]]$, the
type $[[B {a / x}]]$, obtained by substituting $[[a]]$ into the output type
$[[B]]$, has some semantic interpretation $[[S0]]$. In its conclusion, the 
interpretation of $[[Pi x : A . B]]$ is the set of terms $[[b]]$, such that
for all $[[a in S]]$, where $[[S]]$ is an interpretation of $[[A]]$, the
application form $[[b a]]$ belongs to all possible interpretations of
$[[B {a  / x}]]$ (the pre-condition ensures at least one interpretation
exists for each $[[B {a / x}]]$ where $[[a in S]]$).

\begin{lemma}[I-PiAlt derivability]
  \label{lemma:piintroalt}
  \Rref{I-PiAlt} is derivable from \rref{I-Pi}.
\end{lemma}
\begin{proof}
The precondition $[[forall a, (# a in S
implies (# exists S0 , Interp I i B { a / x } S0 #) #)]]$ from
\rref{I-PiAlt} immediately induces a function $[[F in S -> PowerSet
STm]]$ such that $[[forall a, (# a in S implies Interp I i B { a / x }
F a #)]]$, which is exactly what we need to apply \rref{I-Pi}.
\end{proof} 
In fact, while \rref{I-PiAlt} is an instantiation of \rref{I-Pi},
these two rules are equivalent in the sense that every derivation
involving \rref{I-Pi} can be systematically replaced by
\rref{I-PiAlt}. This equivalence follows directly from the fact
that the logical relation is a partial function, a result we will show
in Lemma~\ref{lemma:logreldeter}. The preconditions of \rref{I-Pi},
when combined with the functionality of the logical relation, uniquely
determine the function $[[F in S -> PowerSet STm]]$ to be the
functional relation
$[[{ (a , S0 ) | a in S implies Interp I  i B { a /x } S0 }]]$. This result is
formally shown through the improved inversion lemma for function types
(Lemma~\ref{lemma:piinvalt}).

Unfortunately, we cannot define the function case of our logical
relation directly using \rref{I-PiAlt} since the occurrence of
$[[Interp I i B {a/x} S0]]$ in its conclusion not only violates the
syntactic strict positivity constraint required in proof assistants,
but is genuinely non-monotone when we treat the inductive definition
as the fixed point of an endofunction over the domain of relations.
Intuitively, the failure of monotonicity stems from the fact
that the witness picked in the precondition is not necessarily the
same witness being referred to in the post condition as the relation
grows, whereas the
function $[[F]]$ in \rref{I-Pi} ``fixes'' the witnesses $[[S0]]$ as
$[[F a]]$ for each $[[a in S]]$,
thus preventing the set of witnesses from growing. While it might
be possible to restrict the domain with additional constraints such as
functionality and inversion properties to justify the well-definedness of our
inductive relation with \rref{I-PiAlt}, we opt for our current
\rref{I-Pi} that immediately produces a
well-defined inductive relation and usable induction principle. The
slight disadvantage of \rref{I-Pi} is that we need to construct the
function $[[F]]$ each time we apply it, though this is mitigated by
the derivability of \rref{I-PiAlt} and the alternative $\Pi$ inversion
principle (Lemma~\ref{lemma:piinvalt}).


\subsection{Properties about the Logical Relation}
In the rest of this section, we develop the theory of our logical relation
with the goal of showing four key facts: irrelevance
(Lemma~\ref{lemma:logrelcoherence}), functionality
(Lemma~\ref{lemma:logreldeter}), cumulativity
(Lemma~\ref{lemma:logrelcumulativity}), and the backward closure property
(Lemma~\ref{lemma:logrelbackclos}).  For the majority of the properties that
we prove in this section, we do not need any information about the
parameterized function $[[I]]$.  Each property about $[[InterpR i A S]]$
follows as a corollary of a property about $[[Interp I i A S]]$ with no or few
assumptions imposed on $[[I]]$. As a result, we usually state our lemmas in
terms of $[[Interp I i A S]]$ without duplicating them in terms of
$[[InterpR i A S]]$.


First, we prove a family of simple properties, which we refer to as
inversion principles for our logical relation. Given $[[Interp I i A
S]]$ where $[[A]]$ is in some head form such as $[[Bool]]$ or $[[Pi x
: A0 .B0]]$, the inversion lemma allows us to say something about the set
$[[S]]$. Its proof is simple, but we sketch out the case for
functions to help readers confirm their understanding of \rref{I-Pi}.
\begin{lemma}[Inversion of the logical relation]
  \label{lemma:interpinv}\leavevmode
  \begin{enumerate}
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Void\_inv}} If $[[Interp I i Void S]]$, then $[[S = emptyset]]$.
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Bool\_inv}} If $[[Interp I i Bool S]]$, then $[[S = { a | a =>+ true \/ a =>+ false   }]]$.
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Eq\_inv}} If $[[Interp I i a ~ b : A S]]$, then $[[S = { p | p =>+ refl /\ a <=> b  }]]$.
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Fun\_inv}} If $[[Interp
    I i Pi x : A . B S1]]$, then there exists $[[S]],[[F]]$ such that:
    \begin{itemize}
    \item $[[Interp I i A S ]]$
    \item $[[F in S -> PowerSet STm]]$
    \item $[[forall a, (# a in S implies Interp I i B { a / x } F a #)]]$
    \item $[[S1 = { b | forall a, (# a in S implies b a in F a #) }]]$
    \end{itemize}
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Univ\_inv}} If $[[Interp I i Set j S]]$, then $[[j < i]]$ and $[[S = I j]]$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  As mentioned earlier, we only show the inversion property for the
  function type.
  We start by inducting over the derivation of $[[Interp I i Pi x : A . B S]]$. There
  are only two possible cases we need to consider.
  \begin{description}
  \item[\Rref{I-Pi}:] Immediate.
  \item[\Rref{I-Red}:] We are given that $[[Interp I i Pi x : A . B S1]]$.
    We know that there exists some $[[A0]]$ and
    $[[B0]]$ such that $[[Pi x : A . B => Pi x : A0 . B0]]$ and $[[Interp I i Pi
    x : A0 . B0 S1]]$. From the
    induction hypothesis, there exists $[[S]]$ and $[[F]]$ such that :
    \begin{itemize}
    \item $[[Interp I i A0 S ]]$
    \item $[[F in S -> PowerSet STm]]$
    \item $[[forall a, (# a in S implies Interp I i B0 { a / x } F a #)]]$
    \item $[[S1 = { b | forall a, (# a in S implies b a in F a #) }]]$
    \end{itemize}
    By inverting the derivation of $[[Pi x : A . B => Pi x : A0 . B0]]$, we derive $[[A => A0]]$ and
    $[[B => B0]]$. By Lemma~\ref{lemma:parsubst}, we have $[[B {a /x} => B0 {a/x} ]]$ for all
    $[[a]]$. As a result, by \rref{I-Red}, the same $[[S]]$ and
    $[[F]]$ additionally satisfy the following properties.
    \begin{itemize}
    \item $[[Interp I i A S ]]$
    \item $[[forall a, (# a in S implies Interp I i B { a / x } F a #)]]$
    \end{itemize}
    These properties are exactly what we need to finish the proof.
  \end{description}
\end{proof}

\Rref{I-Red} bakes into the logical relation the backward preservation
property. That is, given $[[Interp I i A S]]$, if $[[B =>+ A]]$, then
$[[Interp I i B S]]$ also holds. The following property shows that
preservation holds in the usual forward direction too.
\begin{lemma}[Forward preservation\footnote{\dotv{semtyping.v}{InterpExt\_preservation}}]
  \label{lemma:interppreservation}
  If $[[Interp I i A S]]$ and $[[A => B]]$, then $[[Interp I i B S]]$.
\end{lemma}
\begin{proof}
  We carry out the proof by induction over the derivation of $[[Interp
  I i A S]]$.

  The only interesting case is \rref{I-Red}. Given that
  $[[A => B0]]$ and $[[Interp I i B0 S]]$, we need to show
  for all  $[[B1]]$ such that $[[A => B1]]$, we have $[[Interp I i B1
  S]]$. By the diamond property of parallel reduction
  (Lemma~\ref{lemma:pardiamond}), there exists some term $[[B]]$ such
  that $[[B0 => B]]$ and $[[B1 => B]]$. By the induction hypothesis,
  we deduce $[[Interp I i B S]]$ from $[[B0 => B]]$ and $[[Interp I i
  B0 S]]$. By \rref{I-Red} and $[[B1 => B]]$, we conclude that
  $[[Interp I i B S]]$.

  The remaining cases all fall from induction hypotheses and basic
  properties about convertibility and parallel reduction we have
  established in Section~\ref{sec:spec}.
\end{proof}
From Lemma~\ref{lemma:interppreservation} and \rref{I-Red}, we can easily
derive the following corollary that two convertible types can always interpret
into the same set. We adopt the terminology from \citet{martin-lof-a-la-coq}
and refer to this property as irrelevance.
\begin{corollary}[Irrelevance of logical relation\footnote{\dotv{semtyping.v}{InterpUnivN\_Coherent}}]
  \label{lemma:logrelcoherence}
  If $[[Interp I i A S]]$ and $[[A <=> B]]$, then $[[Interp I i B S]]$.
\end{corollary}

Because the definition of our logical relation is an inductive relation,
it is not immediately obvious why each type $[[A]]$ can only uniquely
correspond to one set $[[S]]$. The following lemma shows that our
logical relation is indeed functional.
\begin{lemma}[Logical relation is functional\footnote{\dotv{semtyping.v}{InterpExt\_deterministic}}]
  \label{lemma:logreldeter}
  If $[[Interp I i A S0]]$ and $[[Interp I i A S1]]$, then $[[S0 = S1]]$.
\end{lemma}
\begin{proof}
  The proof proceeds by induction over the derivation of the first
  premise $[[Interp I i A S0]]$.
  All cases that are not \rref{I-Red} follow immediately from
  Lemma~\ref{lemma:interpinv}, the inversion properties.

  For \rref{I-Red}, we are given that there exists some $[[B]]$ such
  that $[[A => B]]$ and $[[Interp I i B S0]]$. Our goal is to show
  that given $[[Interp I i A S1]]$ for some $[[S1]]$, we have $[[S0 =
  S1]]$. By the preservation property
  (Lemma~\ref{lemma:interppreservation}),
  we know that $[[Interp I i B S1]]$ since $[[A => B]]$. The statement
  $[[S0 = S1]]$ then immediately follows from the induction hypothesis.
\end{proof}

Lemma~\ref{lemma:logreldeter} enables us to show the following
improved inversion lemma for function types whose statement is free of
the relation $[[F]]$, analogous to the derivable \rref{I-PiAlt}.
\begin{lemma}[Pi Inversion Alt\footnote{\dotv{semtyping.v}{InterpExt\_Fun\_inv\_nopf}}]
  \label{lemma:piinvalt}
  Suppose $[[Interp I i Pi x : A . B S]]$, then there exists some $[[S0]]$
  such that the following constraints hold:
  \begin{itemize}
  \item $[[Interp I i A S0]]$
  \item $[[forall a, (# a in S0 implies (# exists S1 , Interp I i B {a
    / x}
    S1 #) #)]]$
  \item $[[S = { b | forall a, (# a in S0 implies forall
    S1, (# Interp I i B {a / x} S1 implies  b a in S1 #) #) }]]$
  \end{itemize}
\end{lemma}
\begin{proof}
  Immediate from Lemmas~\ref{lemma:interpinv} and \ref{lemma:logreldeter}.
\end{proof}

The next lemma shows that our logical relation satisfies
cumulativity. That is, if a type has an interpretation at a lower
universe level, then we can obtain the same interpretation at a higher
universe level.
\begin{lemma}[Logical relation cumulativity\footnote{\dotv{semtyping.v}{InterpExt\_cumulative}}]
  \label{lemma:logrelcumulativity}
  If $[[Interp I i0 A S]]$ and $[[i0 < i1]]$, then $[[Interp I i1 A S]]$.
\end{lemma}
\begin{proof}
  Trivial by structural induction over the derivation of $[[Interp I
  i0 A S]]$.
\end{proof}
Note that in the statement of Lemma~\ref{lemma:logrelcumulativity}, we
implicitly assume that $[[I]]$ is defined on the set of natural
numbers less than $[[i1]]$.

\begin{corollary}[Logical relation is functional with different levels\footnote{\dotv{semtyping.v}{InterpExt\_deterministic'}}]
  \label{lemma:logreldeterhet}
  If $[[Interp I i0 A S0]]$ and $[[Interp I i1 A S1]]$, then $[[S0 = S1]]$.
\end{corollary}
\begin{proof}
  Immediate from Lemmas~\ref{lemma:logreldeter} and
  \ref{lemma:logrelcumulativity}.
\end{proof}

\begin{definition}[Sets closed under expansion]
  We say that a set of terms $[[S]]$ is closed under expansion if
  given $[[a in S]]$, then $[[b in S]]$ for all $[[b => a]]$.
\end{definition}
The final property we want to show is that the output set $[[S]]$ from
the logical relation is closed under expansion. Unlike the previous
lemmas, we directly state the lemma
in terms of $[[InterpR i A S]]$ rather than $[[Interp I i A S]]$
because we need to know something about $[[I]]$ for this property to
hold in the \rref{I-Set} case.

\yl{Can lift this lemma somewhere earlier in the section since it doesn't really
  depend on anything}
\scw{Jon didn't understand this proof}
\begin{lemma}[Interpreted sets are closed under expansion\footnote{\dotv{semtyping.v}{InterpUnivN\_back\_clos}}]
  \label{lemma:logrelbackclos}
  If $[[InterpR i A S]]$, then the set $[[S]]$ is closed under expansion.
\end{lemma}
\begin{proof}
  By the definition of $[[InterpR i A S]]$, we unfold $[[InterpR i A S]]$ by one
  step into $[[ Interp I i A S  ]]$ where $[[I j]] := [[{A | exists S
    , InterpR j A S}]]$.
  We then proceed by induction over the derivation of $[[Interp I i A S]]$.

  All cases are trivial except for the \rref{I-Set} case, where we
  want to show that the set $[[I j]]$ is closed under expansion for
  all $[[j < i]]$. However, by the definition of $[[I]]$, we know that
  $[[A in I j]]$ if and only if there exists some $[[S]]$ such that
  $[[Interp I j A S]]$. By \rref{I-Red}, we must also have $[[B in I
  j]]$ for all $[[B => A]]$.
\end{proof}

\section{Semantic Typing and Consistency}
\label{sec:logrelproof}
\scw{Would it make sense to define the notation $a \in [\![A]\!]^i$ when
there exists some $S$ such that $[[InterpR i A S]]$ and $[[a in S]]$ ?}

In this section, we show that all closed, well-typed terms are contained
within their type-indexed sets. In other words, $[[empty |- a : A]]$ implies
$[[InterpR i A S]]$ and $[[a in S]]$.  This result gives us consistency
because we know that $[[InterpR i Void S]]$ is defined, and that $[[S]]$ must
be the empty set. Therefore, if there were some closed, well-typed term of type
$[[Void]]$, it would need to be a member of the empty set, a contradiction.

To prove this result, we define a notion of semantic typing based on the
logical relation we have defined in Section~\ref{sec:logreldep} and prove the
fundamental lemma, which states that syntactic typing implies semantic typing.
Semantic typing extends our logical relation from being a (type-indexed)
family of predicates on closed terms, to a type-indexed family of predicates
on open terms.

The necessity of semantic typing as an extra layer of
definition on top of the logic relation can be understood in simply typed
languages~\citep{skorstengaard2019introduction, harpertait,
  pierce2002types}. In our setting, attempting to show that
$[[empty |- a : A]]$ implies $[[InterpR i A S]]$ and $[[a in S]]$ through induction over the derivation of $[[empty |- a : A]]$ will fail in
\rref{T-Abs}, where the induction hypothesis is not helpful since the body of
the lambda term is typed under a non-empty context. Through the definition of
semantic typing, we can state a strengthened property that is actually
provable.

\begin{definition}[Semantic well-formed substitution\footnote{\dotv{soundness.v}{$\rho$\_ok}}]
Define $[[rho |= G]]$ when
\[ \forall [[x]], [[A]], [[i]], \text { and } [[S]], \text{ if }[[x :
  A in G]]\text{ and
                     } [[InterpR i A { rho } S ]] \text{, then } [[rho x in S]] \]
\end{definition}

% To generalize our logical relation to open
% terms, we define the semantic typing judgment by closing the open
% terms with a substitution whose codomain consists of terms that
% respect the interpretation of the types from the context.
% The full
% definitions of well-formed substitution ($[[rho |= G]]$), semantic
% typing ($[[ G |= a : A]]$), and semantic context well-formedness
% ($[[|= G]]$) are presented in Figure~\ref{fig:semtyping}.

The $[[rho |= G]]$ notation denotes the semantic well-formedness of a
substitution $[[rho]]$ with respect to a context $[[G]]$. For every
variable $[[x]]$ with its associated type $[[A]]$ in the context,
$[[rho x]]$ is a term that inhabits all possible interpretations of the type
$[[A {rho}]]$. The $\forall$ quantifier in its definition might look excessive since we
know from Lemma~\ref{lemma:logreldeter} that each type can have at most
one interpretation. However, since $[[rho |= G]]$ mostly appears in
the position of a hypothesis, the $\forall$ statement is easy to
instantiate and makes our proofs slightly easier. The few cases where we need to prove $[[rho |= G]]$ are
handled by the following two structural properties, the second of
which depends on Lemma~\ref{lemma:logreldeter}.
%
\begin{lemma}[Well-formed $[[rho]]$ empty\footnote{\dotv{soundness.v}{$\rho$\_ok\_nil}}]
  \label{lemma:rhowfempty}
  $[[rho |= G]]$ whenever $[[G]]$ is the empty context.
\end{lemma}
\begin{lemma}[Well-formed $[[rho]]$ cons\footnote{\dotv{soundness.v}{$\rho$\_ok\_cons}}]
  If $[[InterpR i A S]]$, $[[a in S]]$, and $[[rho |= G]]$, then
  $[[rho .: x -> a |= G ++ x : A]]$.
\end{lemma}

We next define semantic well-typedness.

\begin{definition}[Semantic typing\footnote{\dotv{soundness.v}{SemWt}}]
Define $[[G |= a : A]]$  when
\[ \forall [[rho]], \text{ if }[[rho |=
                       G]]\text{ then there exists some } [[j]] \text{
                       and } [[S]] \text{ such that } [[InterpR j A
                       {rho} S]] \text{ and } [[a {rho} in S]]  \]
\end{definition}

This definition says the term $[[a]]$ can be semantically typed $[[A]]$ under
the context $[[G]]$ if for all substitutions $[[rho]]$ such that
$[[rho |= G]]$, the type $[[A { rho }]]$ can be interpreted as the set
$[[S]]$, and $[[a { rho } in S]]$. Our definition of semantic well-typedness
is standard, though dependent types add a small twist that we apply the
$[[rho]]$ to $[[A]]$ and require that $[[A { rho }]]$ has some interpretation.

Finally, we define semantic well-formedness for contexts, analogous to the
relation $[[|-G]]$.

\begin{definition}[Semantic context well-formedness\footnote{\dotv{soundness.v}{SemWff}}]
Define $[[|= G]]$ as follows.
\[ \forall [[x : A in G]],\text{ there exists some } i \text{ such that } [[G |= A : Set i]] \]
\end{definition}
Recall that $[[|- G]]$ is defined inductively in terms of the
syntactic typing judgment. We take a different approach here with its
semantic counterpart $[[|= G]]$. The definition of
$[[|= G]]$ is not telescopic: with $[[|- G]]$, a variable appearing
earlier in the context is well-scoped under a truncated context,
whereas with $[[|= G]]$, the types are only required to be
semantically well-formed with respect to the full context, regardless
of their position in $[[G]]$. Our definition of $[[|= G]]$ could be
strengthened, though the simpler definition is sufficient for showing the
fundamental lemma.

We can recover the structural rules for $[[|= G]]$ as lemmas.
\begin{lemma}[Semantic context well-formedness empty\footnote{\dotv{soundness.v}{SemWff\_nil}}]
  \label{lemma:semwffempty}
  $[[|= G]]$ holds when $[[G]]$ is empty.
\end{lemma}
\begin{lemma}[Semantic context well-formedness cons\footnote{\dotv{soundness.v}{SemWff\_cons}}]
  \label{lemma:semwffcons}
  If $[[|= G]]$ and $[[G |= A : Set i]]$, then $[[|= G ++ x : A]]$.
\end{lemma}

The following lemma makes the statement $[[G |= A : Set i]]$ easier to
work with.
\begin{lemma}[Set Inversion\footnote{\dotv{soundness.v}{SemWt\_Univ}}]
  \label{lemma:setinv}
  The following two statements are equivalent:
  \begin{itemize}
  \item $[[G |= A : Set i]]$
  \item $\forall$ $[[rho]]$, if $[[rho |= G]]$, then there exists
    $[[S]]$ such that $[[InterpR i (A {rho}) S]]$
  \end{itemize}
\end{lemma}
\begin{proof}
  The forward direction is immediate by
  Lemma~\ref{lemma:interpinv}. We now consider the backward direction
  and show that $[[G |= A : Set i]]$ given the second bullet.

  Suppose $[[rho |= G]]$, then we know that there exists some $[[S]]$
  such that $[[InterpR i (A {rho}) S]]$. By the definition of semantic
  typing, it suffices to show that there exists some $[[j]]$ and
  $[[S0]]$ such that  $[[InterpR j Set i S0]]$ and $[[A {rho} in
  S0]]$.
  Pick $[[Suc i]]$ for $[[j]]$ and $[[ { A | exists S , InterpR i A S }
  ]]$ for $[[S0]]$ and it is trivial to verify the conditions hold.
\end{proof}


Next, we show some non-trivial cases of the fundamental theorem as
top-level lemmas. For example, we can define the semantic analogue to the
syntactic typing rule for variables (\rref{T-Var}).
\begin{lemma}[ST-Var]
  \label{lemma:stvar}
  If $[[|= G]]$ and $[[x : A in G]]$, then $[[G |= x : A]]$.
\end{lemma}
\begin{proof}
  Suppose $[[rho |= G]]$. By the definition of semantic typing, we
  need to show that there exists some $[[i]]$ and $[[S]]$ such that
  \begin{itemize}
  \item $[[InterpR i A { rho } S]]$
  \item $[[rho x in S]]$
  \end{itemize}
  By the definition of semantic context well-formedness, we deduce
  from $[[|= G]]$ and $[[x : A in G]]$ that there exists some universe
  level $[[i]]$ such that $[[G |= A : Set i]]$. By
  the equivalence from Lemma~\ref{lemma:setinv}, there exists $[[S]]$
  such that $[[InterpR i A {rho} S]]$. However, by the definition of
  $[[rho |= G]]$, we know that $[[rho x in S]]$, which is exactly what
  we need for the conclusion.
\end{proof}

% Next, we show some non-trivial cases of the fundamental theorem as
% top-level lemmas.
% We first formulate the definition of valid renamings and prove that
% semantic typing satisfies renaming so we can weaken the context when
% reasoning about the variable case of the fundamental lemma
% (Lemma~\ref{lemma:stvar}). Intuitively, given a valuation $[[rho |= G ++ D]]$, it is easy to show that we can extract some valuation
% $[[rho0]]$ such that $[[rho0 |= G]]$, where $[[rho0]]$ is obtained by
% ``truncating'' $[[rho]]$. As a result, if we know that $[[G |= a :A]]$, then we can conclude that $[[G ++ D |= a0 : A0 ]]$, where
% $[[a0]]$ and $[[A0]]$ are obtained by shifting $[[a]]$ and $[[A]]$
% after weakening the context; this implication holds because $[[rho |=G ++ D]]$ induces a context $[[rho0]]$ such that $[[rho0 |= G]]$ so we
% can make use of the premise $[[G |= a : A]]$ to derive what we need
% for the conclusion. We recommend the readers to skip the proofs of
% Lemmas~\ref{lemma:validtruncate} through \ref{lemma:semrenaming}
% during the first read as long as they have an intuitive understanding
% of what the renaming property is meant to capture.

% \scw{Make this a definition? Would it be useful to create notation
%   for the relation, such as $[[xi]]:[[G]]\Rightarrow[[D]]$? }
% We say that $[[xi]]$ is
% valid from the context $[[G]]$ to the context $[[D]]$ if the
% following condition holds.

% \scw{Need a transition here that you are starting to explain the semantic typing
% rules.}


\begin{lemma}[ST-Set]
  \label{lemma:stset}
  If $[[i < j]]$, then $[[G |= Set i : Set j]]$.
\end{lemma}
\begin{proof}
  Immediate by Lemma~\ref{lemma:setinv} and \rref{IR-Set}.
\end{proof}

\begin{lemma}[ST-Pi]
  \label{lemma:stpi}
  If $[[G |= A : Set i]]$ and $[[G ++ x  : A |= B : Set i]]$, then $[[G |= Pi
  x : A . B : Set i]]$.
\end{lemma}
\begin{proof}
  Applying Lemma~\ref{lemma:setinv} to the
  conclusion, it now suffices to show that given $[[rho |= G]]$, there
  exists some $[[S]]$ such that $[[InterpR i (Pi x : A . B){rho} S]]$.
  From Lemma~\ref{lemma:setinv} and $[[G |= A : Set i]]$, we know that
  there exists some set $[[S0]]$ such that $[[InterpR i A {rho} S0]]$.
From $[[G ++ x : A |= B : Set i]]$, we know that there must
exist $[[S]]$ such that $[[InterpR i B {rho .: x -> a} S]]$ for every $[[a
in S0]]$. The conclusion immediately follows from the admissible \rref{I-PiAlt}.
\end{proof}

\begin{lemma}[ST-Abs]
  \label{lemma:stabs}
  If $[[G |= Pi x : A . B : Set i]]$ and $[[G ++ x : A |= b : B]]$, then $[[G |=
  \ x . b : Pi x : A . B]]$.
\end{lemma}
\begin{proof}
  By unfolding the definition of $[[G |= \ x . b : Pi x : A . B]]$, we need to
  show that given some $[[rho |= G]]$, there exists some $[[i]]$ and
  $[[S]]$ such that $[[InterpR i (Pi x : A . B){rho}
  S]]$ and $[[(\ x . b) {rho} in S]]$.

  By Lemma~\ref{lemma:setinv} and the premise $[[G |= Pi x : A . B : Set
  i]]$, there exists some set $[[S]]$ such that
  $[[InterpR i (Pi x : A . B){rho} S]]$. It now suffices to show that
  $[[(\ x . b){rho} in S
  ]]$. By Lemma~\ref{lemma:piinvalt}, the alternative inversion
  principle for \rref{I-Pi}, there exists some $[[S0]]$ such
  that all following conditions hold:
  \begin{itemize}
  \item $[[InterpR i A{rho} S0]]$
  \item $[[forall a, (# a in S0 implies (# exists S1 , InterpR i B
    {rho .: x -> a}
    S1 #) #)]]$
  \item $[[S = { b | forall a, (# a in S0 implies forall
      S1, (# InterpR i B {rho .: x -> a} S1 implies  b a in S1 #) #) }]]$
  \end{itemize}
  To show that $[[(\ x . b){rho} in S]]$, we need to prove
  that given $[[a in S0]]$,
  $[[Interp I i B {rho .: x -> a} S1]]$, we have  $[[(\x . b ){rho}
  a in S1]]$.
  By Lemma~\ref{lemma:logrelbackclos}, the set $[[S1]]$ is closed
  under expansion. Since $[[( \ x . b ){rho}
  a => b {rho .: x -> a}]]$, it suffices to show that
  $[[b {rho .: x -> a} in S1]]$, which is immediate from $[[G ++ x : A |= b :
  B]]$ and the fact that the logical relation is deterministic and
  cumulative (Lemma~\ref{lemma:logreldeterhet}).
\end{proof}

\begin{lemma}[ST-App]
  \label{lemma:stapp}
  If $[[G |= b : Pi x : A . B]]$ and $[[G |= a : A]]$, then $[[G |= b
  a : B {a / x}]]$.
\end{lemma}
\begin{proof}
Suppose $[[rho |= G]]$. The goal is to show that there exists some
$[[i]]$ and $[[S1]]$
such that  $[[b {rho} a {rho} in S1 ]]$ and $[[InterpR i B {a / x} {rho}
S1]]$, or equivalently, $[[InterpR i B {rho .: x -> a {rho}} S1]]$ since
$[[B {a / x}{rho}]] = [[B {rho .: x -> a {rho}}]]$. By the premise $[[G |= b :
Pi x : A . B]]$, Lemma~\ref{lemma:setinv}, and Lemma~\ref{lemma:piinvalt},
there exists some $[[i]]$ and $[[S0]]$ such that:
  \begin{itemize}
  \item $[[InterpR i A{rho} S0]]$
  \item $[[forall a0, (# a0 in S0 implies (# exists S1 , InterpR i B
    {rho .: x ->  a0}
    S1 #) #)]]$
  \item $[[forall a0, (# a0 in S0 implies (# forall
      S1, (# InterpR i B {rho .: x -> a0} S1 implies  b {rho} a0 in S1 #) #) #)]]$
  \end{itemize}
  Instantiating the variable $[[a0]]$ from the last two bullets with
  the term $[[a {rho}]]$, the conclusion immediately follows.
\end{proof}

\begin{theorem}[The Fundamental Theorem\footnote{\dotv{soundness.v}{soundness}}]
  \label{theorem:soundness}\leavevmode
  \begin{itemize}
  \item If $[[G |- a : A]]$, then $[[G |= a : A]]$.
  \item If $[[|- G]]$, then $[[|= G]]$.
  \end{itemize}
\end{theorem}
\begin{proof}
  Proof by mutual induction over the derivation of $[[G |- a :
  A]]$ and $[[|- G]]$.   The cases related to context well-formedness immediately follow
  from Lemmas~\ref{lemma:semwffempty} and \ref{lemma:semwffcons}. The semantic typing rules
(Lemmas~\ref{lemma:stvar},~\ref{lemma:stset},~\ref{lemma:stpi},~\ref{lemma:stabs},~\ref{lemma:stapp})
  can be used to discharge their syntactic counterparts
  (e.g. Lemma~\ref{lemma:stabs} for case \rref{T-Abs}). The remaining
  cases not covered by the lemmas are similar to the ones already
  shown.
\end{proof}

Recall the logical consistency property
(Theorem~\ref{theorem:consistency}), which states that the judgment
$[[empty |- a : Void ]]$ is not derivable. We now give a proof of the
property using the fundamental lemma.

\begin{proof}
  Suppose $[[empty |- a : Void]]$ is derivable, then by the
  fundamental lemma, we have $[[empty |= a : Void]]$, which states
  that for all $[[rho |= empty]]$, and for all $[[j]]$, $[[S]]$ such
  that $[[InterpR j Void S]]$, we have $[[a {rho} in S]]$. By
  Lemma~\ref{lemma:rhowfempty}, any $[[rho]]$ we pick trivially
  satisfies $[[rho |= G]]$. For convenience, we pick $[[rho]]$ as
  $[[idtm]]$, though any $[[rho]]$ would work since
  $[[empty |- a : Void]]$ ensures there is no free variable in
  $[[a]]$. We have $[[a {idtm}]] = [[a in S]]$. By the $[[Void]]$
  case of the inversion property (Lemma~\ref{lemma:interpinv}), we
  know that $[[S]]$ must be the empty set, contradicting
  the assumption that $[[a in S]]$.
\end{proof}

Our soundness theorem also tells us something about closed terms of type 
$[[Bool]]$; they either reduce to $[[true]]$ or $[[false]]$.

\begin{corollary}[Canonicity\footnote{\dotv{soundness.v}{canonicity}}]
If $[[empty |- b : Bool]]$, then either $[[b =>+ true]]$ or $[[b =>+ false]]$.
\end{corollary}
\begin{proof}
  The proof is similar to above, except that we use the $[[Bool]]$ case of the
  inversion property.
\end{proof}

\section{Existence of $\beta$-normal forms}
\label{sec:extension}
In this section, we show how the logical relation from
Section~\ref{sec:logreldep} can be extended to show the existence of $\beta$
normal forms for (open and closed) well-typed terms.  In other words, we prove
that it is possible to repeatedly use the parallel reduction relation to
reduce any term to its unique normal form, where no further (non-identity)
reductions can be applied. This result can be used to show that our type
conversion relation is decidable.% , as computation of the normal form will
% always terminate.\yl{dependent on reduction strategy}

The goal of this section is also to demonstrate that our logical relations
proof technique can be extended to reason about the reduction properties of
open terms, not just the reduction of terms after closing substitutions.
Reasoning about open terms is particularly important for dependently-typed
languages because type checking involves working with open terms.
\scw{
  Add when we can find a reference:
  However, even non dependently-typed languages employ such techniques,
  especially in the case of relational semantics.
}
While this extension employs well-known techniques, it continues to be short and
demonstrates the robustness of our initial framework.

We begin this part with a description of the $\beta$-normal forms
of \lang{}.
%
\begin{figure}[h]
  \[
    \begin{array}{llcl}
       \beta\text{-}\mathit{neutral\ terms} &
      [[e]] & ::= & [[x]]\ |\ [[e f]]\ |\ [[J e f f f]]\ |\ [[if e f
                    f]] \\ \\
      \beta\text{-}\mathit{normal\ terms} &
      [[f]] & ::= & [[e]]\ |\ [[Set i]]\ |\ [[Void]]\ |\ [[Pi x : f . f]]\
                    |\ [[f ~ f : f]]\\
            & & |   & [[\ x . f]]\ |\ [[refl]]\ |\ [[Bool]]\ |\ [[true]]\ |\ [[false]]
    \end{array}
  \]
  \caption{$\beta$-neutral and normal forms}
  \label{fig:nenf}
\end{figure}
%
The syntactic forms $[[e]]$ and $[[f]]$ (Figure~\ref{fig:nenf}) capture the
neutral terms and normal forms with respect to $\beta$-reduction\scw{Why not say parallel reduction here? We can be specific and say that that the only reductions available for these terms are identity reductions and cite
\footnote{\dotv{normalform.v}{nf\_refl}} }.\yl{I always think of
normal form as the specific definition that says the relation can't
step. We can refer to nfrefl but that requires some explanation about
the definition of normal form and the complication that it doesn't
hold in $\eta$ (maybe it's fine if we just don't mention it in the
$\eta$ case)}\scw{A terminal form is a syntactic characterization of terms that don't step 
according to a particular relation. A normal form is a terminal form of a normalizing relation.}
 Instead of the
metavariables $e$ and $f$, we also
use the judgment forms $[[ne a]]$ and $[[nf a]]$ to indicate that there exists
$[[e]]$ or $[[f]]$ such that $[[a = e]]$ or $[[a = f]]$.

The predicates $[[wne a]]$ and $[[wn
a]]$ describe terms that can evaluate into $\beta$-neutral or
$\beta$-normal form through parallel reduction and are defined as
follows.
\[
\begin{array}{llcl}
\mbox{\emph{weakly normalizes to a neutral form}} & [[wne a]] &\iff& \exists [[e]], [[a =>+ e]] \\
\mbox{\emph{weakly normalizes to a normal form}}  & [[wn a]]  &\iff& \exists [[f]], [[a =>+ f]] \\
\end{array}
\]

\begin{figure}[h]
  \drules[I]{$[[Interp I i A S]]$}{Logical Relation}{Ne, VoidNew, BoolNew, EqNew}
  \caption{Extended logical relation (new and changed rules) }
  \label{fig:logrelopen}
\end{figure}

The updated logical relation is shown in
Figure~\ref{fig:logrelopen}.~\footnote{\dotv{semtypingopen.v}{InterpExt}}
There is one new rule in this figure, \rref{I-Ne}. In a non-empty context, a
type itself may evaluate to a neutral term and in turn can only be inhabited
by neutral terms.  Otherwise, the rest of the rules in this figure are updates
to the analogous rules in Figure~\ref{fig:logrel}. Note that, we omit the
rules for the function and universe cases because they are identical to the
original version.

% The idea of a term blocked from $\beta$ reduction is
% captured by $[[e]]$, the set of neutral terms. We augment the
% interpretation of booleans and the empty types with terms that
% evaluate to neutral terms. Furthermore, types that are neutral terms
% can also be assigned a meaning since they may be inhabited by other
% neutral terms.
The changes to \rref{I-Bool} and \rref{I-Void}
follow the same pattern: an open term of type $[[Bool]]$ does not necessarily
reduce to $[[true]]$ or $[[false]]$, but may reduce to a variable, or more
generally, a neutral term. Likewise, while the $[[Void]]$ type remains
uninhabited under an empty context, it may be inhabited when there is a
variable in the context that has type $[[Void]]$ or that can be eliminated to
type $[[Void]]$.

\scw{Cassia didn't understand this part}
The rule for equality type $[[a ~ b : A]]$ is augmented with the precondition that
$[[a]]$, $[[b]]$, and $[[A]]$ are all normal forms because
otherwise our model would include equality types that are themselves
not normalizing.
Furthermore, the condition $[[a <=> b]]$ is only
required when the equality proof reduces to $[[refl]]$. If the proof
term reduces to a neutral term, then there is nothing we need to show
about the relationship between $[[a]]$ and $[[b]]$.

Because we are working with open terms, we need a few additional syntactic
lemmas about reduction. First, a renaming $\xi$ is a generalization of
weakening when working with simultaneous substitutions. It consistently maps the
variables that appears in terms to other variables. If a renamed term has been reduced, we can
always recover the result of the reduction without the renaming.
%
\scw{This lemma is only used to prove \texttt{wn\_antirenaming}, which is then
used more generally. Maybe we should replace it with
that?}\yl{Anti-renaming is mentioned once in Section 6 when we talk abou how confluence with $\eta$
depends on anti-renaming for Par. I don't think that discussion in
Section 6 is very important so I'm fine with replacing it  }
\begin{lemma}[Par anti-renaming\footnote{\dotv{normalform.v}{Par\_antirenaming}}]
  \label{lemma:parantirenaming} If $[[a < xi > => b0]]$, then there
exists some $[[b]]$ such that $[[b < xi > = b0]]$ and $[[a => b]]$.
\end{lemma}

We can show that parallel reduction preserves $\beta$-normal and
neutral forms.
\begin{lemma}[Par preserves $\beta$-neutral and normal forms\footnote{\dotv{normalform.v}{nf\_ne\_preservation}}]
  \label{lemma:parnenf}
  If $[[a => b]]$, then
  \begin{itemize}
  \item $[[ne a]]$ implies $[[ne b]]$
  \item $[[nf a]]$ implies $[[nf b]]$
  \end{itemize}
\end{lemma}
Lemma~\ref{lemma:parnenf} could have been strengthened to say that if
$[[ne a]]$ or $[[nf a]]$ and $[[a => b]]$, then $[[a = b]]$. Since
$\ottkw{ne}$ and $\ottkw{nf}$ captures terms free of $\beta$ redexes, parallel
reduction cannot take any real reduction steps and can only step into a term
itself. However, for the purpose of our proof, Lemma~\ref{lemma:parnenf} is
sufficient.

All the properties we have shown in
Section~\ref{sec:logreldep} and \ref{sec:logrelproof} before the fundamental
lemma can be proven in the same order, where the new cases due to \rref{I-Ne}
and the augmentation of neutral terms to \rref{I-Void, I-Eq, I-Bool} can be
immediately discharged by Lemma~\ref{lemma:parnenf}.

Furthermore, Lemma~\ref{lemma:parnenf}, in its current weaker form, would
still hold after we extend our equational theory with the function $\eta$
rule, where parallel reduction can take $\eta$ steps but still preserves
$\beta$-normal form.

We also need to know that the $[[wne a]]$ and $[[wn a]]$ relations can be
justified compositionally. For example, an application has a neutral form when
the function has a neutral form and the argument has a normal form.

\begin{lemma}[Wne application\footnote{\dotv{normalform.v}{wne\_app}}]
  \label{lemma:wnewn}
  If $[[wne a]]$ and $[[wn b]]$, then $[[wne (a b)]]$.
\end{lemma}
\begin{proof}
  Immediate by induction over the length of the reduction sequences in
  $[[wne a]]$ and $[[wn b]]$.
\end{proof}

Furthermore, if we know that an application of a term to a variable
has a normal form, then we know that the term must have a normal form.
\begin{lemma}[Wn extensionality\footnote{\dotv{normalform.v}{ext\_wn}}]
  \label{lemma:extwn}
  If $[[wn (a x)]]$, then $[[wn a]]$.
\end{lemma}
\begin{proof}
  By induction over the length of the reduction sequence in $[[wn (a
  x)]]$. The conclusion follows from Lemmas~\ref{lemma:parantirenaming} and
  \ref{lemma:parnenf}.
\end{proof}


Before we can prove the fundamental theorem and derive the normalization
property as its corollary, we need to additionally formulate and prove an
\emph{adequacy property} about the logical relation.  This property, that the
interpretation of each type is a \emph{reducibility candidate}, allows us to
conclude that every term in each interpretation has a normal form. In the
previous section, we only needed a property of the interpretation of the
$[[Void]]$ type. However, for this section, we need to know something about
the interpretation of every type.

Furthermore, to prove this adequacy property, we need to strengthen it to also
give us more information about neutral terms as we proceed by induction. In
particular, we need to know that all terms that reduce to neutral forms are
contained within the interpretation.
Therefore, we formally define when a set is a \emph{reducibility candidate}
(shortened as $CR$) as follows. Our definition of $CR$ is inspired by
\citet{girard1989proofs}, but not identical since we only care about
weak normalization.
\begin{definition}[Reducibility Candidates (CR)\footnote{\dotv{semtypingopen.v}{CR}}]
  Let $[[S]]$ be a set of terms. We say that $[[S]] \in CR$
  if and only if conditions $CR_1$ and $CR_2$ hold.
  \begin{itemize}
  \item $[[S]] \in CR_1 \iff\ [[forall a, (#  wne a implies a in S #)]]$
  \item $[[S]] \in CR_2 \iff\ [[forall a, (# a in S implies wn a #)]]$
  \end{itemize}
\end{definition}

We now state and prove the adequacy lemma.
\begin{lemma}[Adequacy\footnote{\dotv{semtypingopen.v}{adequacy}}]
  \label{lemma:adequacy}
  If $[[InterpR i A S]]$, then we have $[[S]] \in CR$.
\end{lemma}
\begin{proof}
  We start by strong induction over $[[i]]$. We are given the
  induction hypothesis that for all $[[j < i]]$, $[[InterpR j A S]]$
  implies $[[S]] \in CR$. Our goal is to show $[[InterpR i A S]]$
  implies $[[S]] \in CR$.

  By Definition~\ref{fig:logrelrec}, we have the
  equality $[[InterpR i A S]] = [[Interp I i A S]]$  where $[[I i]] :=
  [[{A | exists S , InterpR i A S}]]$.
  We then proceed by structural induction over the derivation of $[[Interp I i A
  S]]$. The only interesting cases are \rref{I-Pi} and \rref{I-Set}.
  The function case requires Lemmas~\ref{lemma:extwn} and
  \ref{lemma:wnewn}, which we have shown earlier.

  The \rref{I-Set} case is the most interesting. We must show that
  for all $[[j < i]]$, the set $[[{ A | exists S, InterpR j A S  }]]
  \in CR$. We immediately know that $[[{A | exists S, InterpR j A S
  }]] \in CR_1$ by \rref{I-Ne}. It remains to show that $[[{A | exists S, InterpR j A S
  }]] \in CR_2$, or equivalently, for all $[[A]]$, $[[InterpR j A S]]$
  implies $[[wn A]]$. Suppose $[[InterpR j A S]]$ for an arbitrary
  $[[A]]$. We have $[[InterpR j A S]]=[[Interp I j A S]]$ where $[[I]]$
  has the same definition from earlier but its domain restricted to
  numbers less than $[[j]]$. We perform another induction on
  the derivation of $[[Interp I j A S]]$. All cases are trivial except for the case for
  \rref{I-Pi}. Our induction hypothesis immediately gives us $[[wn
  A]]$. To derive $[[wn (Pi x : A . B)]]$, it remains to show $[[wn B]]$. We
  use the outermost induction hypothesis to show that $[[x]]$
  semantically inhabits $[[A]]$, from which we derive $[[wn (B {x / x})]]$
  and conclude $[[wn B]]$ through antirenaming (Lemma~\ref{lemma:parantirenaming}).
\end{proof}


The formulation of semantic well-typedness and the fundamental lemma from
Section~\ref{sec:logrelproof} remains
unchanged.
The proof of the fundamental lemma\footnote{\dotv{soundnessopen.v}{soundness}} is still carried out by induction
over the typing derivation, where the additional neutral term related
cases are handled by Lemma~\ref{lemma:adequacy}, the adequacy property.

The normalization property then follows as a corollary of the
fundamental theorem.
\begin{corollary}[Existence of $\beta$-normal forms\footnote{\dotv{soundnessopen.v}{mltt\_normalizing}}]
  \label{corollary:exbetanf}
  If $[[G |- a : A]]$, then $[[wn a]]$ and $[[wn A]]$.
\end{corollary}
\begin{proof}
  By the fundamental lemma, we know that $[[G |= a : A]]$. That is,
  for all $[[rho |= G]]$, there exists some $[[i]]$ and $[[S]]$ such
  that $[[InterpR i A {rho} S]]$ and $[[a {rho} in S]]$.
  We pick the $[[rho]]$ to be the identity substitution $[[idtm]]$, which injects
  variables as terms. The side condition $[[idtm |=
  G]]$ is satisfied since Lemma~\ref{lemma:adequacy} says neutral terms,
  including variables, semantically inhabit any $[[S0]]$ where
  $[[S0]]$ is the interpretation of some type. With our choice of
  $[[rho]]$, we have $[[A {rho}]] = [[A {idtm}]] = [[A]]$ and $[[a {rho}]] = [[a{idtm}]] = [[a]]$. Then we
  know that $[[InterpR i A S]]$ and $[[a in S]]$ for some $[[i]]$ and
  $[[S]]$. By Lemma~\ref{lemma:adequacy}, we
  conclude that $[[wn a]]$ and $[[wn A]]$.
\end{proof}

% Due to the non-deterministic nature of parallel reduction, we need to take a
% few more steps to convert the existence of $\beta$-normal form into a decision
% procedure for type conversion. More specifically, we can show that a
% deterministic evaluation strategy such as leftmost-outermost reduction can
% always find the $\beta$-normal form if there exists one. Then the termination
% of that strategy immediately follows from Corollary~\ref{corollary:exbetanf}
% However, we omit such proofs since they can can be formulated on untyped
% lambda terms and thus are orthogonal to the specifics of dependently typed
% systems. Instead, we redirect readers to \citet{factorization-essentially,
%   takahashi-parallel-reduction} for the details.

The extension of our logical relation to prove normalization of open
\emph{and} closed terms closely mirrors the progression from
normalization of closed terms~\citep{harpertait} to normalization of
open terms~\citep{harperkripke} in the simply typed lambda calculus.
Indeed, a mechanization of normalization generalized to open terms
appears in ~\citet{abel2019poplmark}.  In this setting, as above,
adequacy must be proven before the
fundamental theorem so they can handle elimination rules such as
\rref{T-App} where the scrutinee is a neutral term. % In
% \citet{abel2019poplmark}, a variant of Lemma~\ref{lemma:extwn} is used
% in the exact same way to show the normalization of lambda forms
%
Dependent types make the adequacy proof slightly more complicated because we
also need to know that every \emph{type} has a normal form, not just
terms. This complicates our proof specifically in the \rref{I-Set} case for
our adequacy property (Lemma~\ref{lemma:adequacy}).

Overall, despite the dependently typed setting, it is in fact reassuring that
once we have laid the foundational technique for handling dependent types in
our logical relation, the extension to open terms mostly boils down to
properties that can be independently derived from the logical relation through
syntactic means.

\section{Existence of $\beta\eta$-normal forms}
\label{sec:betaeta}
\citet{nbeincoq,decagda,martin-lof-a-la-coq}
include the $\eta$ law for functions in their equational theory and
use relational models to justify its validity.
In our system, we can easily incorporate the function $\eta$ law to the equational
theory of \lang{} by adding the following parallel reduction rule.
\begin{center}
  \drule[width=2.5in]{P-AbsEta}
\end{center}
In this section, we show how we easily extend the existence of $\beta$-normal
forms from Section~\ref{sec:extension} to the existence of
$\beta\eta$-normal forms after this addition. 

First, we recover the same confluence result about parallel reduction using the
standard techniques from \citet{barendregt:lambda-calculi-with-types,
  takahashi-parallel-reduction}, though anti-renaming
(Lemma~\ref{lemma:parantirenaming}) must be proven before the diamond property
(Lemma~\ref{lemma:pardiamond}). Another complication is that
the anti-renaming property and the diamond property for parallel reduction are now proven through
induction on a size metric of lambda terms; \rref{P-AbsEta} reduces a term
that is not a strict subterm.

Note that, after this extension, the specification of our logical
relation does not require
any updates. The proof of the fundamental theorem also remains
identical since the complications introduced by $\eta$ are hidden
behind the proofs of the diamond property and the anti-renaming property.
As before, $\ottkw{ne}$ and $\ottkw{nf}$
represent $\beta$-neutral and $\beta$-normal forms, and the
fundamental lemma shows us that every well-typed term has a
$\beta$-normal form. However, in the presence of the $\eta$ reduction
rule, Lemma~\ref{lemma:parnenf} tells us that $\eta$ reduction
preserves $\beta$-normal forms (i.e. does not produce new
$\beta$-redexes). Furthermore, since the $\eta$ reduction rule for
functions strictly decreases the size of the term, the existence of
$\beta\eta$ normal form trivially follows.
\begin{corollary}[Existence of $\beta\eta$-normal form]
\label{corollary:exbetaeta}
If $[[G |- a : A]]$, then $[[a]]$ has $\beta\eta$-normal form.
\end{corollary}

%% SCW: This is out of place, and redundant with the next paragraph.
% In contrast to our approach, related work \cite{nbeincoq,decagda,martin-lof-a-la-coq}
% employs a relational model to
% justify the $\eta$-law for functions.
% Instead, we prove
% confluence for our parallel reduction extended with $\eta$-law for functions
% and continue using a logical predicate (i.e. a unary logical relation) to justify
% $\eta$-law as part of our equational theory.
A well-known issue with our approach is the failure of syntactic
confluence when the lambda term contains type annotations. A simple
counterexample is $[[\- y : B . ((\- x : A . a) y)]]$ where $[[y notin
fv (\- x : A . a)]]$; depending on
whether \rref{P-AbsEta} is performed on the whole term or
\rref{P-AppAbs} is used on the inner $\beta$ redex, we end up with the
terms $[[\- x : B . a]]$ (after $\alpha$-conversion) or $[[\- x : A . a]]$, where $[[A]]$ and $[[B]]$ are not
necessarily syntactically equal terms. \citet{choudhury:ddc} resolve
this problem by stating their confluence result in terms of an
equivalence relation that quotients out parts of the terms that are
computationally irrelevant; the annotations of lambda terms are
ignored since the behavior of a lambda term is not affected by its
type annotation. We believe the same approach is applicable to our
proof.

The bigger issue is extensions such as $\eta$-laws for unit and
products. Surjective pairing, for example, is not confluent for untyped lambda
terms~\cite{KLOP198997}. The relational, type-annotated, and Kripke-style models from
\citet{nbeincoq,decagda,martin-lof-a-la-coq} can be more easily
extended to support these rules.
We note, however, that the issue with $\eta$ rules is not exclusive to dependently
typed languages and has been studied in more limited languages that
are either simply
typed~\citep{pierce2004advanced,pfenning1997computation} or
dependently typed but without large
eliminations~\citep{harper2005equivalence,
abel2005untypedconvsurjective}. Common workarounds include
type-directed conversion and shifting the focus to obtaining
$\eta$-long forms~\cite{Abel12}.

While not without limitations, our simple proof demonstrates the core
building blocks of more complex arguments, thus paving the way for experimentation
and eventual extension to more expressive systems.\scw{I tried to reword your
sentence, but I am still not happy with it.}

% \scw{Is this specific to $\eta$? Or is this also true for $\beta$? Can we move this
% sentence elsewhere? I'd like to use the previous sentence as a conclusion for
% this section.}
% Finally, due to technical
% details, our mechanized proof about the existence of $\beta\eta$ form does not
% immediately induce a decision procedure for type conversion in
% \lang{}. We discuss this issue and its workaround in
% Section~\ref{sec:conversionalgo}.
\section{Mechanization}
\label{sec:logrelmech}
\begin{figure}[h]
  \begin{minipage}{0.9\textwidth}
  \begin{tabular}{ |l |  c  | c | c| }
    \hline
    & Consistency & Normalization & Syntactic metatheory \\
    \hline
    Syntactic typing         &  83 & =   & = \\
    Untyped reduction        & 344 & =   & = \\
    Neutral and normal forms & -   & 273 & - \\
    Logical relation         & 338 & 430 & - \\
    Semantic soundness       & 192 & 211 & - \\
    Syntactic soundness      & -   & -   & 629 \\
    \hline
    Total                    & 957 & 1341 & 1056 \\
    \hline
  \end{tabular}
  \end{minipage}
  \caption{Nonblank, noncomment lines of code of the Coq Development. The marker = indicates that the line count is the same as the column to the
left. The marker - indicates the file does not
contribute to the total.}
  \label{fig:linecount}
\end{figure}

To demonstrate the scale of our proof scripts, Figure~\ref{fig:linecount}
shows the number of non-blank, non-comment lines of code\footnote{calculated
  by the \texttt{tokei} tool, available from
  \url{https://github.com/XAMPPRocky/tokei}.} for each file of our development,
including the base consistency proof from Section~\ref{sec:logreldep} and
\ref{sec:logrelproof} and the extension to $\beta$-normalization from
Section~\ref{sec:extension}.  For comparison, we have also proven syntactic
type soundness through preservation
\footnote{\dotv{syntactic\_soundness.v}{subject\_reduction}} and
progress\footnote{\dotv{syntactic\_soundness.v}{wt\_progress}}.

The $\beta\eta$-normalization proof from Section~\ref{sec:betaeta} comprises
1568 lines of non-blank, non-comment lines of code. We choose not to include
it in the chart, because of slight differences in lemma dependencies for
untyped reduction and normal forms that make the comparison less
informative. However, when compared to the $\beta$-normalization extension,
the $\beta\eta$ extension has the same line count in the definition of the
logical relation and the semantic soundness proof.

The Autosubst 2 tool takes our 13 line syntax specification, written in
higher-order abstract syntax, and generates the Coq syntax specification,
renaming and substitution functions, and lemmas and tactics that allow
reasoning about those functions. The auto-generated syntax file (291 LOC)
and other Autosubst library files are also not included in the figure.

\paragraph{Axioms}
Our Coq development assumes two axioms: functional extensionality
and propositional extensionality. The former is also required by
the Autosubst 2 libraries. Both axioms are known to be consistent
with Coq's metatheory.
%
These axioms bridge the gap between our mechanization and our informal
proofs. For example, in set theory, to show that two sets $[[S0]]$ and
$[[S1]]$ are equal, it suffices to show the extensional property that
$\forall x, x \in [[S0]] \iff x \in [[S1]]$. We leverage this fact
occasionally in our presented proofs.
%
However, in Coq, sets of terms ($[[PowerSet STm]]$) are encoded as the 
type \texttt{tm
  -> Prop}, a predicate over \lang{} terms.  In axiom-free Coq, predicates do
not come with the extensionality property. Given two predicates $P$ and $Q$,
we cannot conclude that $P = Q$ when given a proof of
$\forall x, P(x) \iff Q(x)$. But this is exactly the statement of 
predicate extensionality, an immediate corollary from functional
extensionality and propositional extensionality.

\paragraph{Encoding the logical relation in Coq}
We next discuss specific details of the Coq encoding of the logical relation
presented in Section~\ref{sec:logreldep}.

% First, this definition requires the use of Coq's impredicatve \texttt{Prop}
% sort. In particular, the function $[[F]]$ in \rref{I-Pi} requires the use of
% impredicativity because $[[F]]$ must later be instantiated into something
% defined in terms of the logical relation itself (e.g. in
% Lemma~\ref{lemma:piintroalt}).
In the Coq mechanized proof, % we encode $[[PowerSet STm]]$ as the type \texttt{tm -> Prop}, a predicate over lambda terms. 
the definition of $[[Interp I i A S]]$ has type
\texttt{Prop}, where \texttt{I} has type \texttt{nat -> tm -> Prop} and \texttt{S} has type \texttt {tm -> Prop}.

% The inductive
% definition of the logical relation in Figure~\ref{fig:logrel} requires
% the impredicativity of Coq's \texttt{Prop} sort since\scw{or ``so that''?} in \rref{I-Pi},
% the function $[[F]]$ can be later instantiated into the logical
% relation itself (e.g. in the proof of Lemma~\ref{lemma:piintroalt}).

However, if desired, we could consistently replace the use of \texttt{Prop}
with Coq's predicative sort \texttt{Type} in the definition of
$[[Interp I i A S]]$. This alternative definition could be part of
the interpretation for any \emph{finite} number of universes. The use of
\texttt{Type} becomes troublesome only when we attempt to define
$[[InterpR i A S]]$, the top-level logical relation 
(Definition~\ref{fig:logrelrec}) that recursively calls itself at smaller universe
levels. Therefore, the one feature of \lang{} that truly requires
impredicativity is its countable universe hierarchy.

The definition of $[[Interp I i A S]]$ has an almost one-to-one
correspondence to the Coq definition. The main difference is the
specification of $[[I]]$. In
Section~\ref{sec:logreldep}, we define $[[I]]$ as a function over
numbers less than $[[i]]$, the universe level. In Coq, we only require
$[[I]]$ to be a function with the set of natural numbers as its domain.
In the Coq encoding of $[[InterpR i A S]]$, we define $[[I]] \in [[SNat -> PowerSet STm]]$ as follows.
\begin{equation*}
  \begin{split}
    [[I j]] &=
     \begin{cases}
      \ [[{A | exists S , InterpR j A S}]] & \text{when } j < i \\
      \ [[emptyset]] & \text{otherwise}
    \end{cases}
  \end{split}
\end{equation*}
Since $[[I]]$ is only applied to numbers strictly less than $[[i]]$ in
\rref{I-Set}, we can retroactively show that the set we return in the $j
\geq i$ case is junk data that does not affect the result of the logical
relation. This property allows us to recover the simple equation for $[[InterpR i A S]]$ shown in Definition~\ref{fig:logrelrec}.

\Rref{I-PiCoq} shows how \rref{I-Pi} is actually encoded in our
mechanized proof.
\begin{center}
  \drule[width=5in]{I-PiCoq}
\end{center}
Compared to \rref{I-Pi}, \rref{I-PiCoq} replaces the function
$[[F]]$ with a total relation $[[R]]$. The equivalence of these two
rules follows from the fact that the logical relation is a
partial function (Lemma~\ref{lemma:logreldeter}). In set-theoretic
notation, \rref{I-Pi} is more readable. However, if we want to encode
the same rule in Coq, we must encode $[[F]]$ as a relation (with type
\texttt{tm -> (tm -> Prop) -> Prop}) that satisfies the functionality constraint:
\mintinline{coq}{forall a S0 S1, F a S0 -> F a S1 -> S0 = S1}.
In comparison, \rref{I-PiCoq} does not require this side condition and
results in a simpler definition.

We note that we cannot ascribe $[[F]]$ the type \texttt{tm -> (tm ->
  Prop)} since Coq requires functions of such type to be
computable. While defining $[[F]]$ as a computable Coq function rather than a
functional relation does result in a concise encoding of \rref{I-Pi},
we will have trouble instantiating $[[F]]$ with the logical relation,
which is defined as a relation that we prove to be functional, rather
than a computable function.


% In Coq, there is a distinction between computable functions and
% relations that can later be proven to be functional. The former can be
% viewed as a strict subset of the latter in axiom-free Coq. To be more
% precise, given a relation \texttt{R : A -> B -> Prop} subject to the
% totality and functionality constraints ($\forall$ \texttt{a} $\in$
% \texttt{A}, there exists a unique \texttt{b} $\in$ \texttt{B} such
% that \texttt{R a b} is inhabited), we do not immediately obtain a function
% \texttt{F : A -> B} such that $\forall$ \texttt{a} $\in$ \texttt{A},
% \texttt{R a (F a)} is inhabited. However, the functional side
% conditions of a relation is clunky to express and tend to block
% automation. A simple workaround is to assume the axiom of unique choice, which
% is known to be consistent with Coq and allows us to induce a function \texttt{F
% : A -> B} once we have shown the relation \texttt{R : A -> B -> Prop}
% is functional. This approach would make our Coq development match
% the text version of our proof from Section~\ref{sec:logrelproof} more
% closely.

% However, we choose instead an axiom-free workaround and define
% \rref{I-Pi} as follows in our Coq mechanization.
% It should be easy to verify that the preconditions of
% \rref{I-Pi}, \rref{I-PiAlt}, and \rref{I-PiCoq} are all
% equivalent. After establishing Lemma~\ref{lemma:logreldeter}, it is
% possible to further show that the conclusions of the rules are
% equivalent, too.\scw{If it is easy, you should have already done it.
% Unless it is really long and boring.}

% This formulation allows us to derive Lemma~\ref{lemma:piintroalt}
% before we even show that our logical relation is
% functional/deterministic, but does not affect the proof structure
% otherwise.
% We choose to keep this discrepancy between the Coq development and the
% description of the logical relation presented in Section~\ref{sec:logreldep} since the
% skolemization process is more intuitively expressed in terms of
% function symbols rather than relation symbols. Otherwise, we do not
% see a clear advantage of \rref{I-PiCoq} over \rref{I-Pi} in set
% theory, where there is no distinction between computable functions and
% functions in general. \scw{not sure I follow this last bit}

\paragraph{Automation}
\label{sec:automation}
Our Coq mechanization heavily uses automation, supported by the tools
Autosubst 2~\citep{autosubst2} and CoqHammer~\citep{czajka2018hammer}.

We use the Autosubst 2 framework to produce Coq syntax files based on a de
Bruijn representation of variable binding and capture-avoiding substitution.
In addition to these generated definitions, Autosubst 2 provides a powerful
tactic \texttt{asimpl} that can be used to prove the equivalence of two terms
constructed using the primitive operators provided by the framework. This
tactic simplifies the reasoning about substitution as many
substitution-related properties about syntax are immediately discharged by
\texttt{asimpl}.

For other automation tasks that are not specific to binding, we use
the powerful \texttt{sauto} tactic provided by CoqHammer to write
short and declarative proofs. For example, here is a one-line proof of
the triangle property about parallel reduction, from which the diamond
property (Lemma~\ref{lemma:pardiamond}) follows as a corollary.
The triangle property states
that if $[[a => b]]$, then $[[b]]\Rightarrow [[a]]^*$, where $[[a]]^*$
is the Takahashi translation~\citep{takahashi-parallel-reduction}
which roughly corresponds to simultaneous reduction of the redexes in
$[[a]]$, excluding the new redexes that appear as a result of
reduction.
\begin{minted}{coq}
Lemma Par_triangle a : forall b, (a â‡’ b) -> (b â‡’ tstar a).
Proof.
  apply tstar_ind; hauto lq:on inv:Par use:Par_refl,Par_cong ctrs:Par.
Qed.
\end{minted}
In prose, the triangle property can be proven by induction over the graph of
\mintinline{coq}{tstar a}, the Takahashi translation. Options \texttt{inv:Par}
and \texttt{ctrs:Par} say that the proof involves inverting and constructing
of the derivations of parallel reduction. The option
\texttt{use:Par\_refl,Par\_cong} allows the automation tactic to use the
reflexivity and congruence properties of parallel reduction as lemmas.

The flag \texttt{lq:on} tunes CoqHammer's search algorithm.  While this flag
appears arcane, when developing our proof scripts we never specify this option
manually. Instead, we first invoke the \texttt{best} tactic provided by
CoqHammer, specifying only the \texttt{inv}, \texttt{ctrs}, and lemmas that we
want to use. The \texttt{best} tactic then iterates through possible
configurations and provides us with a replacement with the tuned performance
flags that save time for future re-execution of the proof script.

The automation provided by CoqHammer not only gives us a proof that is shorter
and more resilient to changes, but also provides useful documentation for
readers who wish to understand the mechanized proof. Although automation
performs extensive search, we can configure it to not use lemmas or invert derivations that
are not specified in the \texttt{use} or \texttt{inv} flags.

\ifextended
\subsection{Extraction of a conversion algorithm}
\label{sec:conversionalgo}
A constructive proof of the existence of $\beta\eta$-normal form
for well-typed terms (Corollary~\ref{corollary:exbetaeta}) induces a
normalization algorithm. From this normalization procedure, we can
derive a normalize-and-compare algorithm. Given two well-typed terms
$[[a]]$ and $[[b]]$, to know whether $[[a <=> b]]$, we apply the
normalization algorithm on $[[a]]$ and $[[b]]$ to obtain $\beta\eta$
normal forms $[[f0]]$ and $[[f1]]$. The algorithm then returns true
exactly when $[[f0]]$ and $[[f1]]$ are syntactically equal. This
algorithm is referred to as normalize-and-compare by
\citet{pierce2004advanced}.

The soundness of the algorithm is immediate. The completeness of the
algorithm is justified by the confluence property of the untyped
reduction relation. Suppose $[[a <=> b]]$, $[[a =>+ f0]]$, $[[b =>+
f1]]$ but $[[f0]]$ is syntactically distinct from $[[f1]]$. By the transitivity of convertibility, we have $[[f0 <=> f1]]$.
Because $[[f0]]$ and $[[f1]]$ are both in
$\beta\eta$-normal forms and can only reduce to themselves, we must
have $[[f0 = f1]]$, which is a contradiction.

In our development, since our countable universe hierarchy
relies on impredicativity from the metatheory, we encode our
properties in Coq's \texttt{Prop} sort, which is not very suitable for
code execution. On the other hand, due to our heavy reliance on
automated proof search, even if we were able to extract an algorithm from \texttt{Prop},
the algorithm would be unpredictable because its definition depends on
the specific choices made by the proof search algorithm.

However, % with a little more effort,
it should be possible to recover a
precise algorithm. The first step is to define a deterministic
small-step reduction relation that is normalizing; that is, the
relation can always find a normal form if there exists one. A good
candidate is the leftmost-outermost reduction strategy. Its
normalizing property is standard and can be proven using the
factorization technique discussed in
\citet{takahashi-parallel-reduction, factorization-essentially}. By
composing the existence of normal forms and the fact that the
deterministic relation is normalizing, we can derive the
accessibility of the deterministic reduction relation
(\mintinline{coq}{Acc} in Coq), and use the accessibility proof as
an induction metric to define an executable algorithm\footnote{Coq's
  singleton elimination principle allows \mintinline{coq}{Acc}, a
  \texttt{Prop} data type with a single constructor, to be eliminated
  to construct runtime relevant data.}. We leave the proof of the
factorization property and the extraction of a conversion algorithm as
part of our future work.
\fi
\section{Related Work}
\label{sec:relatedwork}


% \scw{Maybe it would be useful to include a chart here, so that readers can
%   easily keep track of the features of the various languages.  i.e. which ones
%   include large eliminations? type-directed equivalence? impredicative prop?
%   inductive datatypes? what are their line counts?}
% \yl{resolved}

\subsection{Logical relations for dependent types}
In the most general sense, a logical relation can be viewed as a
practical technique that uses a type-indexed relation to 
strengthen the induction hypothesis for the
property of interest. The original idea of this technique can be
traced back to
\citet{tait1967:reducibility}. This proof maps
types to sets of terms satisfying certain properties related to reduction.
The same idea is explained in \citet{girard1989proofs} and extended to
prove strong normalization of System F.
%
Tait's method has also been successfully applied to dependently typed
languages to prove strong normalization \cite{Martin-Lof-1973,luo1990extended,geuvers1994short, barendregt:lambda-calculi-with-types}.

However, the pen-and-paper representation of logical relations proofs
can be challenging to adapt to a theorem prover since many details
are hidden behind concise notations.
For example, \citet{geuvers1994short} presents the interpretation for types as
an inductively defined total function over the set of syntactically
well-formed types. \scw{The issue is that Geuver's metalogic is (untyped) set
theory, but Coq and Agda use type theory.}
In untyped set theory, it makes sense to define the logical relation as a
simply-typed function that takes a type and returns some set; however
in constructive type theory, the metalogic of Coq and Agda, the
interpretation function must be a dependently-typed function whose
return type depends on the derivation of the well-typedness of its
input. The well-typedness derivation and the proof of the
classification theorem\scw{what is the classification theorem}\yl{It
  says all well-formed terms A (either G |- A : .. or G |- .. : A)
then A is either an object, a type, or a kind. The classification is
used to guide erasure}
are examined in the body
of the interpretation function to decide whether an argument of an
application should be erased during interpretation. As a result, this
definition causes difficulties for modern proof assistants.
Due to the
impredicativity of the object language, \citet{geuvers1994short}'s
proof cannot be encoded in Agda, which has a predicative
metatheory. Due to the use of proof-relevant derivations, even in
Coq, a proof assistant that supports impredicativity, one would need
to constantly juggle between the impredicative but irrelevant sort
\texttt{Prop} sort and the predicative but relevant sort
\texttt{Type}.

More recent work such as \citet{Abel12}
and \citet{abel2008betaeta} make their definitions more explicit and
precise and thus more directly encodable in proof assistants.
Our logical relation resembles their definition of a
semantic universe hierarchy, although we close our relation
under expansion with respect to parallel reduction rather
than weak-head reduction. \scw{Why is this important?} \yl{if you are
  referring to the sentence about weak-head reduction, it can
  be deleted because it's discussed in Section 9 already}
Furthermore, \citet{Abel12} and
\citet{abel2008betaeta} use their semantic universe hierarchy as a
measure to define Kripke-style logical relations, from which they
derive the correctness of their conversion algorithms. In our work, we
use the semantic universe hierarchy directly in our definition of
semantic typing because it is sufficient for our purposes
(consistency and normalization).

\subsection{Mechanized logical relations for dependent types}

%\newcommand\dheader[1]{\rotatebox{60}{{#1}}}
\newcommand\header[1]{\rotatebox{0}{{#1}}}

\begin{figure}[h]

% \begin{minipage}{0.8\textwidth}
  \begin{tabular}{| l |  c  | l | c | c | c | l | l | }
    \hline
%      & \header{Universes} & \header{Inductives} & \header{Conversion}
%      & \header{Large Elim} & Main results \\
      & \header{U} & \header{Ind} & \header{C} 
      & \header{L E} & \header{A} &  Main results \\

    \hline
    \lang{} (this work) & $\mathbb{N}$ & Id, Bool & U & {\boxedsymbols âœ“} & 1 
    & Consistency and normalization \\
    % \hline
    $\lambda^\theta$ & 0 & Id, Nat & U & {\boxedsymbols âœ—} & 1 & Consistency \\
    Core Nuprl & $\mathbb{N}$ & W-Types & E & {\boxedsymbols âœ“} & 2 & Consistency \\
    NBE-in-Coq & 1 & Nat & T & {\boxedsymbols âœ—} & 2 & Correctness of NBE \\
    $\lambda^{\Pi U\mathbb{N}}$ & 1 & Nat, $\Sigma$ & T & {\boxedsymbols âœ“} & 2 & Decidability of conversion \\
    MLTT-\'a-la-Coq & 1 & Id, Nat, $\Sigma$ & T & {\boxedsymbols âœ“} & 2 & Decidibility of type checking \\
    \hline
  \end{tabular}


  \begin{tabular}{ll}
    \\
  \underline{U}niverses: & Countable ($\mathbb{N}$), Zero (0), One (1) \\
  \underline{Ind}uctives: & Identity types (Id), Natural numbers (Nat), $\Sigma$-types ($\Sigma$), W-types \\
  \underline{C}onversion:& Untyped (U), Typed (T),  Extensional (E) \\
  \underline{L}arge \underline{E}liminations:& Included ({\boxedsymbols âœ“}), not included ({\boxedsymbols âœ—}) \\
  \underline{A}rity of interpretation: & Sets of terms (1), Relations between terms (2) \\
  \\
  $\lambda^\theta$  & \citet{casinghino:combining-proofs-programs} (logical fragment only) \\
  Core Nuprl &\citet{anand2014towards} \\
  $\lambda^{\Pi U\mathbb{N}}$ &\citet{decagda} \\
  NBE-in-Coq& \citet{nbeincoq} \\
  MLTT-\'a-la-Coq &\citet{martin-lof-a-la-coq} \\
  \end{tabular}

  \caption{Feature matrix for dependently typed languages with
    mechanized logical relations}
  \label{fig:featurematrix}
\end{figure}
\scw{Extra columns in Fig 10:  Predicate vs. Relational interpretation (we can explain this) / Typed vs. Untyped interpretation}

\scw{Should we add a column for whether their logical relation is
  unary or binary?}\yl{Technically a Kripke-style model has the
  context as an argument to the logrel, too. By binary, we are really
  talking about whether the interpreted set is binary or not. I think
  it is tricky to explain what we actually mean. }
\scw{I don't think it is that trick}
Figure~\ref{fig:featurematrix} presents several mechanized proofs
that feature logical-relations arguments for dependently-typed languages.
Each of these proofs is significantly larger than than our development; but they
also prove more results about different object languages.% \scw{Can we mention the
% line counts in the text, when they are known?}
The table provides a comparison between the various features of their
object languages, but is not exhaustive. For example,
\citet{casinghino:combining-proofs-programs} and
\citet{anand2014towards} both have support for partial
programs. However, we include features that we believe to be most
impactful to the definition of the logical relation.

\citet{casinghino:combining-proofs-programs} introduce $\lambda^\theta$, a dependently typed
programming language that uses modality to distinguish between logical
proofs and programs. % The programmatic fragment
% includes recursive data types and supports general recursion at the
% cost of introducing divergence.
% However, the $\lambda^\theta$
% language is limited in its expressiveness since it does not support
% type-level computation or polymorphism and therefore is not a fully dependently
% typed language according to \citet{abel2013normalization}.
The consistency proof of $\lambda^\theta$'s logical fragment has been
mechanized in Coq through a step-indexed logical relation;
step-indexing is required to model the programmatic fragment, which
interacts with the logical fragment.
The lack of polymorphism and type-level computation means their
logical relation can be defined recursively for well-formed types using
a size metric, which has been used in~\citet{liu2023dependently}.
Their development is around 8,000 lines of nonblank, noncomment code.

\citet{decagda} mechanize in Agda the decidability of type
conversion rule for a dependently typed language with one predicative
universe level and a typed judgmental equality that includes the function
$\eta$ law. They
use a Kripke-style logical relation parameterized over a
type-directed equivalence relation satisfying certain
properties to facilitate the reuse of their definition. The
logical relation is defined using the induction-recursion scheme,
which is available in Agda but not in Coq. Their development involves
around 10,000 lines of Agda code.
\citet{martin-lof-a-la-coq} transports the logical relation
from \citet{decagda} in the predicative fragment of Coq and further
extends the decidability of type conversion result
from~\citet{decagda} to the decidability type checking of a
bidirectional type system. Their development has around
30,000 lines of Coq code.

\citet{anand2014towards} mechanize the metatheory of
Nuprl~\citep{constable1986implementing} in Coq. This metatheory is an
extensional type theory with features such as dependent functions,
inductive types, partial types, and a full universe hierarchy. They construct a PER
model in Coq to show the logical consistency of their language. Their
development has been further extended with features such as
intersection types, union types, and quotient types. The extensive
coverage of features results in a Coq development with around
330,000 lines of code.
\citet{nbeincoq}
mechanize the normalization-by-evaluation algorithm in Coq for a
dependently typed language with one predicative universe, similar to
\citet{decagda} and \citet{martin-lof-a-la-coq}. However, since their
type system has no elimination form for natural numbers, the
only base type from the object language, large elimination is not
supported despite the one predicative universe. Their development has
around 20,000 lines of Coq code.
Both \citet{anand2014towards} and \citet{nbeincoq} leverage the
impredicative \texttt{Prop} sort of Coq to define the interpretation
of dependent function types and thus are closely related to our
mechanization. \citet{anand2014towards} further show it is possible
to encode a finite universe hierarchy without the use of
either impredicativity or induction-recursion. Their encoding of a countable
universe hierarchy relies on impredicativity, similar to our
development.

\subsection{Other mechanized metatheory of dependent types}

\citet{barras2010sets, Wang2013SemanticsOI} assign
set-theoretic semantics to dependent type theory in Coq. Unlike the
previous efforts, which primarily focus on predicative
type theory and more direct reducibility models,
\citet{barras2010sets, Wang2013SemanticsOI} tackle extensions of
$CC^\omega$, a system that incorporates a predicative universe on top
of the impredicative sort in the Calculus of Constructions. We choose to
focus on a syntactic term model so we do not have to take the extra step
of mechanizing mathematical objects such as sets and domains.

There are other mechanized developments for dependently typed systems that
only involve properties that are derivable through syntactic means. For
example, \citet{coqcoqcorrect2019} prove the correctness of a type checker
for the Predicative, Cumulative Calculus of Inductive Constructions (PCUIC),
Coq's core calculus, assuming the strong normalization property of the object
language. \citet{weirich:systemd} define System D, a core calculus of
dependent Haskell, and prove the syntactic type soundness of the type
system. Because System D includes nontermination, they proved
the consistency of definitional equality from the confluence of 
parallel reduction.  

Compared to the systems described here, the most notable features we are
missing are cumulativity and impredicativity. Our semantic model already
satisfies the cumulativity property (Lemma~\ref{lemma:logrelcumulativity}),
but we need to extend our convertibility relation into a subtyping relation in
our syntactic typing rules. Impredicativity, on the other hand, is known to be
difficult to model when the impredicative sort is at the bottom of a
predicative universe hierarchy; in this scenario, the erasure technique from
\citet{geuvers1994short} is not
applicable~\citep{abel2013normalization}. Whether there is a similarly short
and simple treatment for impredicativity remains an open question.

\section{Discussion}
\label{sec:discuss}
\yl{I'm not sure how strong of a statement we can make about our proof
  technique. We've already demonstrated how to address type-level dependency
  when defining a logical relation through a simple example. Claiming that our
  proof structure is better seems quite ambitious, but I think there's a
  middle point where we claim that adding moderate features like typed
  reduction doesn't instantly make our code size expand all the way from 1000
  to 20,000 without saying the other developments are just verbose for no good
  reason}
\scw{I think we can find reasons for much of the differences. Am I missing any?
While it would be difficult to assign numbers to each of the deltas, I think it
is believable that when put together they add up to a lot.
\begin{itemize}
\item We don't include inductive or coinductive datatypes. We don't include
cumulativity. We don't include Prop. We don't include universe polymorphism.
\item We state our equality algorithmically instead of declaratively. On one
  hand, this gives us automatic inversion principles when working with
  definition. Furthermore, we don't need to prove the equivalence between an
  algorithmic version and a declarative specification.
\item Our equality requires a simple decision algorithm and isn't type directed.
\item We don't prove decidability of type checking. (And, it is not provable
  for our system, because we lack type annotations on functions. We should
  point this out.)
\item Our logical relation is unary and untyped. The latter means that we don't
  require the bookkeeping of a Kripke logical relation when reasoning about
  open terms. I don't know why unary relations are shorter.
\item CoqHammer leads to short proofs.
\end{itemize}
}
\yl{ Just one more technical point to add, though it's in the text already:
  the logical relation is closed backward by full reduction rather
  than weak-head/deterministic reduction. This requires an early
  confluence result to show that the logrel is
  deterministic/functional but simplifies everything else
  (e.g. conversion is justified immediately by our preservation
  theorem, but that is not the case if you use weak head reduction).\\ \\
  Also, regarding the first point, cumulativity only exists in
  Barras's work. Inductive, (maybe coinductive?), can be found in
  nuprl, metacoq, and maybe Barras's work.\\ \\
  The 20,000 - 30,000 LoC mechanization are all about small languages
  with pretty much the same features as our language except for your
  second and third bullet point. martin-lof a la coq, Abel's work, and
  nbe in coq aren't that richer in feature otherwise. None includes
  cumulativity (they only have one predicative universe)\\ \\
  The 400,000 NuPRL in Coq probably falls into a different category
  because they are trying to mechanize a full practical language}
\scw{The Coq-Coq-Correct paper (extended version) includes a (predicative) universe hierarchy, universe polymorphism, inductive/coinductive types. But they don't show consistency. Their development is 300k LOC.}

Our short consistency proof achieves the goal of
demonstrating the technique of proof by logical relation for dependently typed
languages. However, what remains unanswered is what makes our development
significantly shorter. Are we proving simpler results for
smaller languages, or making more use of automation, or is our proof
technique genuinely more efficient?
% focus on the following question: why is our proof,
% even with its extension to the existence $\beta\eta$-normal form, so much
% shorter than the other mechanized results from \citet{decagda,
% nbeincoq, martin-lof-a-la-coq}?


% First, the metatheoretic properties that we prove are indeed simpler.  Unlike
% developments that mechanize the correctness of a type-directed conversion
% algorithm, we only show the existence of normal forms for open and closed
% terms and state our properties in terms of an untyped reduction relation. This
% avoids a lot of the scaffolding related to the specification of the algorithm
% and the proof obligations that the algorithm is sound and complete with
% respect to the declarative specification of the type system.
% As a result, our logical relation, unlike the ones from \citet{decagda,
%   nbeincoq, martin-lof-a-la-coq, anand2014towards}, maps from types to
% predicates rather than relations. \scw{How does this follow? Why do we need unary relations where they need binary relations?} The need for a relational model is
% directly related to the metatheoretic property one wants to
% prove. \citet{anand2014towards} requires a relational model to capture
% the extensionality of their type system, whereas \citet{decagda,
%   martin-lof-a-la-coq, nbeincoq} uses a relational model to derive the
% injectivity of $\Pi$ types and justify the validity of
% $\eta$-conversion among other properties.
% Since \lang{} uses an untyped conversion rule, type conversion can be
% done through the \emph{normalize-and-compare}
% strategy described in \citet{pierce2004advanced}. The decidability of
% normalize-and-compare is implied by the existence of
% $\beta\eta$-normal form, which follows from our logical
% predicate.

First, the metatheoretic properties that we prove are indeed
simpler. Compared to Core Nuprl, our system
lacks extensionality, which would require a relational model to
justify consistency. Because the conversion rule for
\lang{} is untyped, we do not
need a Kripke-style relational model to prove $\Pi$-injectivity
among other properties, unlike systems with typed conversion. Furthermore, we prove the existence of normal
forms, which induces a simple \emph{normalize-and-compare}
procedure for type
conversion~\cite{pierce2004advanced}. \citet{nbeincoq, decagda}, on
the other hand, need
to show how their algorithmic conversion procedure is sound and complete
with respect to their respective declarative equational theory.
\scw{I'm getting confused by this paragraph. Does this reorganization sense:
  Our language is simpler than Nuprl, because it doesn't have extensional
  equality. It is simpler than Agda, because it doesn't have type-directed
  equality. Both of these cases require the definition of a binary logical
  relation, that defines a notion of semantic equality between terms. This
  relation justifies the injectivity of $\Pi$ types and justify the validity
  of $\eta$-conversion among other properties.}  \scw{Furthermore our proof is
  also simpler because we don't need prove the correctness of the NBE
  algorithm, which is used to show the decidability of Agda's type-directed
  equivalence. Therefore, we don't need to define this algorithm and show that
  it is sound and complete with respect to the type-directed
  equality. Instead, to show the decidability of our untyped equivalence, we
  need only show that terms have $\beta\eta$ normal forms. }
\yl{Makes sense. Though Abel's work doesn't use nbe but a recursive
  binary algorithm. Rewrote the paragraph above and commented out the original}


Second, the definition of our logical relation does
contribute to a more concise proof.
In \rref{I-Red, I-Bool}, we choose parallel reduction, a full
reduction relation, to close over our semantic interpretation of types
and terms. Parallel reduction is non-deterministic, but it satisfies
useful structural properties such as congruence
(Lemma~\ref{lemma:parcong}) and the diamond property
(Lemma~\ref{lemma:pardiamond}). We pay the price of using a
non-deterministic reduction relation when we want to prove that our
logical relation is a partial function; because of \rref{I-Red}, we
can have $[[A => B0]]$ and $[[A => B1]]$, where $[[B0]]$ and $[[B1]]$
each have their separate interpretations that we have to prove to be
equal. Fortunately, this complexity is reconciled by the
diamond property, which is easy to derive syntactically.

In contrast, \citet{decagda} and \citet{nbeincoq} employ a deterministic weak
head reduction relation. % The use of a
A deterministic reduction relation makes
the functionality of a logical relation trivial to prove, but fails to
satisfy the substitution property (Lemma~\ref{lemma:parsubst}), an
issue that has been observed by
\citet{casinghino:combining-proofs-programs}. If we had chosen to work
with a deterministic reduction relation, we would likely need
results such as the factorization theorem~\citep{takahashi-parallel-reduction,factorization-essentially} in our development before we
can prove the fundamental theorem, leading to a more complicated proof.
% With
% this alternative formulation, we would have to prove how the deterministic reduction relates to
% a full non-deterministic reduction relation to prove the fundamental
% theorem. This would amount to proving the factorization theorem of the
% deterministic reduction relation with respect to full non-deterministic
% reduction. Instead, using parallel reduction in \rref{I-Red} allows us to
% delay the factorization property until we need to justify the use
% leftmost-outermost reduction as a deterministic reduction strategy for
% normalization.



% In terms of our proof technique, the choice of parallel reduction, a full reduction
% relation, to close over our semantic interpretation in \rref{I-Red,I-Bool} has a non-negligible effect on the
% size of our development.

% we close over our semantically valid types
% and terms in \rref{I-Red, I-Bool} using the non-deterministic parallel
% reduction relation, while \citet{decagda,nbeincoq} employ a deterministic weak
% head reduction relation. Our use of a non-deterministic reduction strategy
% means that we need confluence to prove the functionality of our logical
% relation. However, the benefit is that it immediately gives us a semantic
% justification of the conversion rule (Lemma~\ref{lemma:logrelcoherence}).
% In particular, we obtain the confluence
% result of reduction at a very early stage before we even define our
% logical relation.

With untyped conversion,
we sidestep the relational, Kripke-style logical relation found in
other mechanized proofs. \scw{Need to define Kripke-style. Also the other
proofs need Kripke style because they are defining typed relations, not untyped
relations. } \yl{I wonder if
we can just assume some more technical knowledge from the readers in
this section.}
However, our early dependence on confluence
before the fundamental theorem is established can be alarming.
In a system with type-directed reduction,
confluence is not immediately available because it
depends on $\Pi$-injectivity, which is usually only proven after the
fundamental theorem.\scw{confluence depends on Pi injectivity? I thought
it was only needed for subject reduction}\yl{it's
transitive. Confluence depends on subject reduction, which in turn
depends on pi injectivity. Maybe it's worth spelling out the details}
Fortunately, there are syntactic workarounds for the $\Pi$-injectivity
problem that allow us to recover the confluence property independently
from the logical relation. \citet{siles2012pure} generalize the
notion of Type Parallel One Step Reduction from \citet{adams2006pure}
to syntactically prove $\Pi$-injectivity for arbitrary Pure Type
Systems. \citet{weirich:systemd} add $\Pi$-injectivity to their
equational theory, thus allowing subject reduction to be proven
independently from confluence. By adopting these techniques that allow
us to derive confluence early even for systems with type-directed
reduction, we believe our proof technique can significantly shorten
the existing logical relation proofs for systems with typed
judgmental equality. We leave that as part of our future work.
% Therefore, we believe that even in a
% system where type-directed reduction is required (e.g. a system with
% the unit $\eta$-law) in the logical relation, the proof can still be carried
% out in a structure similar to the one we have presented.
% \scw{Not sure that I understand this paragraph}
% \yl{Reworded slightly to emphasize it's future work that we haven't
%   done and it is speculative}

\ifextended
Finally, despite our earlier claim that our metatheoretic properties
are not as strong as some of the related work, we can strengthen our
results through syntactic means. For example, the
normalize-and-compare strategy does not induce an efficient algorithm
for type conversion, like the ones from \citet{decagda} and
\citet{martin-lof-a-la-coq}. However, with the standard syntactic
techniques from \citet{takahashi-parallel-reduction,
factorization-essentially}, we can prove that leftmost-outermost
reduction is a normalizing reduction strategy, from which we can
separately show the correctness a more efficient algorithm that
reduces the two terms to weak head normal form before recursively
comparing their subcomponents. We believe factoring such properties
out of a logical relation is valuable, as it helps us identify the
part of our proof that requires extra strength from the metatheory.
\scw{Why don't we just use leftmost-outermost reduction in the first place?
Do we even need nondeterministic parallel reduction?}
\yl{The conversion uses full reduction. Nondeterministic reduction
  makes it harder to show that convertible types have the same
  meaning. Maybe it would require us to prove factorization in our
  development but it definitely simplifies the determinism proof
  (confluence is no longer required before the fundamental
  lemma). }
\fi

\section{Conclusion}
\label{sec:conclusion}
In this work, we present a short and mechanized proof by logical relations for
a dependently typed language with a full universe hierarchy, large
eliminations, an intensional identity type, and dependent eliminators.  We
show the extensibility of our approach by proving the existence of
$\beta\eta$-normal forms with only small and mechanical changes to our proof
development.  Our Coq mechanization leverages existing Coq libraries for
reasoning about metatheory and for general purpose automation, allowing us to
significantly reduce the verbosity typically associated with mechanized
proofs. The result is a declarative proof style that rivals pen and paper.

Related work gives us confidence that we could extend our logical relation to
include features such as full inductive datatypes, irrelevant arguments, and
type-directed conversion; however, it is not clear how much of the brevity of
this development can be maintained.  Furthermore, we hope that mechanized
logical relations proofs will eventually grow to include other features found
in dependent type theories, such as impredicative universes, universe
polymorphism, and cumulativity.  Regardless, our development shows that proofs
by logical relations for dependent types are accessible and do not require
months of effort to implement.  We hope our proof can inspire researchers to
more frequently mechanize results, such as consistency and normalization, for
their dependent type theories.





% Type soundness can be proven through a syntactic
% approach~\citep{syntacticsoundness} as a corollary of two properties:
% progress and preservation. % The syntactic type soundness proof
% % varies in complexity depending on the underlying type
% % system. For example, a type system that tracks information flow would
% % require additional structural rules related to security levels. In
% % this paper, we focus on one specific type of complexity: the
% In Figure~\ref{fig:stlcsoundness}, we summarize the structure of the
% syntactic type soundness proof for the simply typed lambda
% calculus. Each lemma can be proven by structural induction over the
% typing derivation, while using the previous established results as
% lemmas for specific cases that do not immediately follow from the
% induction hypothesis. If we make our language more complex by adding
% full dependent type support, the overall structure remains almost
% identical.


% NbE in Coq

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

%%
%% If your work has an appendix, this is the place to put it.

\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
