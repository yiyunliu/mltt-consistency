\newif\ifcomments     %% include author discussion
\newif\ifanonymous    %% include author identities
\newif\ifextended     %% include appendix
\newif\ifsubmission   %% prepare the submitted version
\newif\ifpublic       %% version available for posting / final version

\commentstrue         %% toggle comments here
\extendedfalse
\anonymousfalse

\submissionfalse     %% at most one of these must be true (neither for draft version)
\publicfalse         %% but if you want to see comments, these should both be off

% If we are going to make a version public, i.e. on arXiv, we should
% make sure that there are no comments and our names are on it.
\ifpublic
\submissionfalse
\commentsfalse
\anonymousfalse
\fi

%% If we are submission, make sure there are no comments and our names
%% are NOT on it.
\ifsubmission
\publicfalse
\commentsfalse
\anonymoustrue
\fi

\documentclass[\ifpublic nolinenum\else\fi,online,OA]{jfp}

\let\Bbbk\relax % amsmath tries to redefine this
\usepackage{amsthm,amssymb}
\let\draftnote\relax % this is already defined in JFP
\usepackage{draft}
\usepackage[outputdir=latex.out]{minted} % latexrun puts everything in latex.out
\usepackage{xspace,stmaryrd,mathtools,pifont}
\usepackage[para]{footmisc}
\usepackage{ottalt}
\inputott{rules}

\ifcomments
\newnote{scw}{blue} % Stephanie Weirich
\newnote{yl}{purple} % Yiyun Liu
\newnote{jc}{red} % Jonathan Chan
\else
\newcommand{\scw}[1]{}
\newcommand{\yl}[1]{}
\newcommand{\jc}[1]{}
\fi

\newcommand{\dotv}[2]{\href{#1}{\texttt{#1}}{\texttt{:#2}}}
\newcommand{\lang}{$\lambda^{\Pi}$\xspace}
\newcommand{\CR}{\mathit{CR}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\DeclareTextCommand{\nobreakspace}{TU}{\leavevmode\nobreak\ }

\citestyle{acmauthoryear}

\begin{document}

\journaltitle{JFP}
\volno{35}
\edno{0}
\cpr{Cambridge University Press}
\doival{10.1017/}
\totalpg{\pageref{lastpage}}
\jnlDoiYr{2025}
\lefttitle{Y. Liu et al.}
\righttitle{Logical Relations for Type Theory}

\title{Functional Pearl: Short and Mechanized Logical Relation for Dependent Type Theories}

\begin{authgrp}
\author{Yiyun Liu}
% \orcid{0009-0006-8717-2498}
\affiliation{University of Pennsylvania (\email{\elink{liuyiyun@seas.upenn.edu}})}

% \author{Jonathan Chan}
% \orcid{0000-0003-0830-3180}
% \affiliation{University of Pennsylvania (\email{\elink{jcxz@seas.upenn.edu}})}

\author{Stephanie Weirich}
% \orcid{0000-0002-6756-9168}
\affiliation{University of Pennsylvania (\email{\elink{sweirich@seas.upenn.edu}})}
\end{authgrp}

\begin{abstract}
Proof by logical relation is a powerful technique that has been used
to derive metatheoretic properties of type systems, such as
consistency and parametricity. While there exists a
plethora of introductory materials about logical relations in the
context of simply typed or polymorphic lambda calculi, a streamlined
presentation of proof by logical relation for a dependently typed language
is lacking. In this paper, we present a short
consistency proof for a dependently typed language that contains a
rich set of features, including a countable universe
hierarchy, booleans, and a propositional equality type. We show that
the logical relation can be easily extended to prove the existence of
$\beta\eta$-normal forms.
We have
fully mechanized the consistency proof using the Coq proof assistant
in under 1000 lines of code, with 500 lines of additional code for the
$\beta\eta$-normal form extension.
\end{abstract}

\maketitle[T]

\section{Introduction}
This paper presents a \emph{short} and \emph{mechanized} proof of
logical consistency for \lang{}, a dependent type theory with a
predicative universe hierarchy, large elimination, a propositional equality
type, a boolean base type, and dependent elimination forms.

Our goal with this work is to demonstrate the application of the proof
technique of \emph{syntactic logical relations} to dependent type theories.
Logical relations are a powerful proof technique, and have been used to show
diverse properties such as strong normalization~\citep{girard1989proofs,
  geuvers1994short}, contextual equivalence~\citep{constable1986implementing},
representation independence~\citep{pitts1998existential},
noninterference~\citep{bowman2015noninterference}, compiler
correctness~\citep{benton2009biorthogonality,perconti2014compiler}, and the
decidability of conversion
algorithms~\citep{harper2005equivalence,Abel12,abel2013normalization}.

However, tutorial material on syntactic logical
relations~\citep{skorstengaard2019introduction, harpertait, harperkripke,
  pierce2002types, pierce2004advanced,harper2016practical} is primarily
focused on simple or polymorphic types. In that context,
syntactic logical relations can be defined as simple recursive functions over
the structure of types, or in the case of recursive types, defined over the
evaluation steps of the computation. Yet neither of these techniques can be
used to define a logical relation in the context of a predicative dependent
type theory, so a novice researcher may have the impression
that logical relations are not applicable to dependent types.

But this is not the case. 
% \scw{Add a sentence about pen-and-paper LR proofs
%  for dependent type theories?}\yl{There is a long paragraph in Sec 8 about how
%  pnp proofs like Geuvers' can't be easily mechanized, but I'm not sure if we
%  can also argue that they are hard to read, too (I personally think that's the
%  case, but I don't know if the reviewers will agree)} 
Recent authors have developed tour-de-force
mechanizations for the metatheory of modern proof
assistants~\citep{nbeincoq,decagda,martin-lof-a-la-coq,anand2014towards}, and
rely on logical relations as part of their
developments. However, because these proofs show diverse results about real
systems and algorithms, these developments range in size from 10,000 to
300,000 lines of code. As a result, their uses of logical relations are
difficult to isolate from the surrounding contexts and inaccessible
to casual readers.

Our paper instead provides a gentle and accessible introduction to a powerful
technique for dependent type theories.
It is accompanied by a short mechanized proof development
of fewer than 1000 lines of code, developed using the Coq proof
assistant~\citep{coq}.
We have streamlined our proof through a number of means: the careful selection
of the features that we include in the object type system and the results that
we prove about it, in addition to the judicious use of automation.

Our language is small, but includes enough to be illustrative. For example, we
eschew inductive datatypes and W types, but we do include propositional
equality and booleans to capture the challenges presented by indexed types and
dependent pattern matching. We do not show the decidability of type checking,
nor do we develop a PER semantics, but we prove logical consistency, which
states that empty types are not inhabited in an empty context,
and extend our consistency proof to show the existence of $\beta\eta$-normal
forms for well-typed closed \emph{and} open terms,
at a moderate cost of 600 lines of code.
We include a full predicative universe hierarchy with large elimination
to demonstrate the logical strength of the approach.
% JC: I changed "type-level computation" to "large elimination"
% since I think it's usually the latter that gets referred to
% when talking about logical strength and expressivity.

%%
%% SCW: save specific comparisons with other systems for later
%%
% The big difference in the size of the developments does not
% necessarily imply that our proof technique leads to a more concise
% proof due to the differing expressiveness of the languages and the
% different metatheoretic results being established. For
% example, \citet{anand2014towards} mechanizes the metatheory of
% Nuprl~\citep{constable1986implementing}, which, on top of all the
% features that \lang{} supports, includes W-types and partial types and
% requires a PER semantics to model its extensional typed
% equality. The object language from \citet{decagda} does not support
% identity types and only has one predicative universe, but additionally
% supports $\Sigma$ types. However, for a researcher who wishes to learn
% the underlying techniques for mechanized logical relation for
% dependent type theory, a small development like ours is much easier to
% navigate and understand. Furthermore, we show how our simple
% consistency proof can be easily extended to show the existence of
% $\beta\eta$-normal form for well-typed open \emph{and} closed terms,
% giving us a metatheoretic result almost as strong as the one from
% \citet{decagda} at the moderate cost of around 500 lines of extra code.

Concretely, our paper makes the following contributions.
\begin{itemize}
\item In Section~\ref{sec:spec}, we introduce \lang{}, the dependent type
  theory of interest. A key design choice that impacts our proofs is the use
  of an untyped conversion rule, inspired by Pure Type
  Systems~\citep{barendregt1991introduction}, and specified through parallel
  reduction~\citep{takahashi-parallel-reduction,
    barendregt:lambda-calculi-with-types}.
\item In Section~\ref{sec:logreldep}, we formulate logical consistency for
  \lang{}, the property of interest, to motivate a logical relation. We
  define the relation inductively, prove its functionality,
  define semantic typing in terms of the relation, and prove the
  fundamental theorem, from which consistency follows as a corollary
  (Section~\ref{sec:logrelproof}). Our proof
  showcases the special treatment required to model many common
  features of dependent type theories, thus making our proof applicable to a
  broad range of type systems.
\item We strengthen our logical relation to prove the existence of
  $\beta$-normal forms (Section~\ref{sec:extension}) and $\beta\eta$-normal
  forms (Section~\ref{sec:betaeta}) for well-typed open terms. The
  modifications made to our initial logical relation are small and closely
  mirror the necessary extensions for a simply typed language. This shows
  that once we have established the base techniques, we can port
  ideas from proofs about simpler languages to the dependently typed setting.
\item We mechanize all our proofs in Coq, with 983
  lines of code for the consistency proof with $\beta$ rules and a moderate
  increase to 1596 lines of code for the normalization proof with $\beta\eta$
  rules. We discuss our choice of using Coq as our metatheory,
  including the use of off-the-shelf
  semantic engineering infrastructure and automation tools, in
  Section~\ref{sec:logrelmech}. Our proof development is available as
  supplementary material.
\item We compare our work to existing proofs by logical relation and other
  proof techniques for proving consistency and normalization.  We provide an
  overview of prior work (Section~\ref{sec:relatedwork}) and explain
  how various design decisions affect the size of our
  proof and its extensibility to additional features
  (Section~\ref{sec:discuss}).
\end{itemize}

The result of our work is an artifact targeted towards researchers
familiar with the syntax of dependent types, as well as logical relations
for simple or polymorphic types, and who wish to understand using logical
relations \emph{for} dependent types.
The short mechanized proof is accompanied here by a pen-and-paper description
using set-theoretic notation and terminology so that it is accessible to
readers with a general mathematical background. This pen-and-paper proof
purposefully follows the mechanized proof closely while avoiding Coq-specific
details as much as possible, and lemmas are linked directly to their
counterparts in the mechanization.

Not only does this close connection aid readers who wish to adopt proof
assistants for mechanizing metatheory, this precision is also important for
conveying the proof technique itself. Unlike properties derivable through
purely syntactic means, proofs by logical relation make demands on the
strength of the metatheory in which they are expressed. An informal proof that
attempts to be agnostic or ambiguous about the underlying metatheory requires
substantial effort from the reader to understand whether it is definable in a
given ambient logic.

%% SCW: I think we said this already in the 2nd paragraph above.
% Finally, our approach does not result in a verbose proof on paper, thanks to
% the already short mechanization. While we do make a few simplifications
% to our typeset proof, such as avoiding the clunky syntax for well-founded
% recursion in Coq, we keep the overall structure of our typeset proofs
% consistent with our mechanization.

\section{Specification of a Dependent Type Theory}
\label{sec:spec}

% \scw{This section needs to say:
%  \begin{itemize}
% \item definitional equality is untyped so that we can prove
%  properties about it independent of the type system
% \item definitional equality is based on parallel reduction so that we can take
%  advantage of the algorithmic structure later. This definition is easier to
%  invert because there are fewer derivation. That is important when reasoning
%  about our logical relation.
% \item can prove that parallel reduction is equivalent to other definitions of
%  equality using standard means (cite Barendregt's book).  (we go halfway
%  there by showing it is an equivalence relation). Can also prove equivalent
%  to typed relation. (cite Siles, also CoreSpec?)
% \item All of the proofs in this section are standard, well known, and use
%  techniques that are well-suited for proof assistants. In fact, de Bruijn's
%  paper that introduces de Bruijn indices was part of a mechanized confluence
%  proof for the untyped lambda calculus.
% \item extensions: forward reference to eta equivalence for functions in later
%  section. type-directed equivalence is future work
%  \end{itemize}
%}

\begin{figure}[h]
\begin{align*}
[[a]],[[b]],[[c]],[[p]],[[A]],[[B]],[[C]] & \Coloneqq && \textit{Terms} \\
            & \mid [[Set i]] \mid [[x]] \mid [[Void]] \mid [[absurd b]]
              && \textrm{universes, variables, empty type, explosion} \\
            & \mid [[Pi x : A . B]] \mid [[\ x . a]] \mid [[a b]]
              && \textrm{function types, abstractions, applications} \\
            & \mid [[a ~ b : A ]] \mid [[refl]] \mid [[J c p]]
              && \textrm{equality types, reflexivity proof, J eliminator} \\
            & \mid [[Bool]] \mid [[true]] \mid [[false]] \mid [[if a b0 b1]]
              && \textrm{boolean type, true, false, conditionals} \\
            % & \mid [[if a b0 b1]]
            %   && \textrm{conditional expression} \\
[[G]] & \Coloneqq [[empty]] \mid [[G ++ x : A]] && \textit{Typing contexts} \\
[[rho]] & \in [[SVar -> STm]] && \textit{Substitutions}
\end{align*}
\caption{Syntax of \lang}
\label{fig:syntax}
\end{figure}

In this section, we present the dependent type theory \lang{}, whose logical
consistency will be proven in Section~\ref{sec:logrelproof}.
Its syntax is given in Figure~\ref{fig:syntax}. As a dependent
type theory, terms and types are collapsed into the same syntactic
category. We use a Curry-style (extrinsic) syntax, where terms are not
annotated by types, and only terms which participate in reduction are part of
introduction and elimination forms.

The type $[[Set i]]$ represents a universe at level $i$,
a natural number. Abstractions $[[ \ x . a ]]$ and
dependent function types $[[Pi x : A . B]]$ are binding forms for the variable
$x$ in the body of the function and codomain of the function type.%
\footnote{In the exposition in this paper, binding forms are equal up to
  $\alpha$-conversion and we adopt the Barendregt Variable
  Convention~\cite{barendregt:lambda-calculus}, which lets us assume that
  bound variables are distinct.  In some places, we are informal about the
  treatment of variables and substitution; our mechanized proofs make these
  notions precise by using de Bruijn indices~\citep{debruijn1994automath}. }
We use the notation $[[A -> B]]$ when the output type $[[B]]$ is not dependent
on the input variable. Type annotations in abstraction forms are omitted, and
we discuss how the inclusion of type annotations can affect
our development in Section~\ref{sec:betaeta}, where we extend our
consistency result to the existence of $\beta\eta$-normal forms.
Propositional equality types $[[a ~ b : A]]$, represent an
equality between terms $[[a]]$ and $[[b]]$ of the same type $[[A]]$, whose
canonical inhabitant is the reflexive proof $[[refl]]$. Equality proofs $[[p]]$
can be eliminated by the J eliminator $[[J c p]]$, which casts the term $[[c]]$
from one end of the equality to the other. Finally, we have booleans along with
the standard boolean values and conditional expressions.

Our reduction and typing relations are defined in terms of \emph{simultaneous
substitutions} $[[rho]]$, which are mappings from variables to terms. Simultaneous
substitutions are more convenient to reason about than single substitutions,
especially when dealing with semantic typing in Section~\ref{sec:logrelproof}.
We use $[[idtm]]$ as the identity substitution, and the extension operator
$[[rho .: x -> a]]$ updates the substitution $[[rho]]$ to map the variable
$[[x]]$ to $[[a]]$ rather than to $[[rho x]]$.
% When traversing under binders (e.g. in the
% $[[(\ x . a) { rho }]]$ case), it must be the case that $[[rho]]$ maps the
% bound variable to itself and that the bound variable does not appear freely in
% the application of the substitution to any other variable.
%
\scw{Maybe we should handwave more here, and note that we aren't covering all
  of the details of the treatment of variable binding in the text. If readers
  want to understand that part, they should look at the Coq code, which uses
  de Bruijn indices anyways. Our goal is to convey the understanding of the
  Coq proof. }
\yl{I agree. Should mention upfront that the presentation is not
  watertight and rigorous, though we have one in our mechanization}
The substitution operation $[[a {rho}]]$ replaces every free variable $[[x]]$ of
$[[a]]$ by $[[rho x]]$ simultaneously. A single substitution corresponds to
composing extension and identity:
$[[a { b / x }]] \coloneqq [[a { idtm .: x -> b }]]$.

\subsection{Definitional equality via parallel reduction}

Before we specify the typing rules, we first specify the equational theory
used in the conversion \rref{T-Conv} in Figure~\ref{fig:typing},
known as \textit{definitional equality} in dependent type theories because it
defines the equivalence that the syntactic type system works up to. 
The definitional equality we use is \emph{convertibility}:
two terms are convertible if they reduce to a common form.
The reduction that we use is \emph{parallel reduction}, written
$[[a => b]]$, with $[[a =>+ b]]$ indicating its reflexive, transitive closure.
The parallel reduction relation is defined in Figure~\ref{fig:par},
which omits reflexivity and congruence rules for brevity.

\begin{definition}[Convertibility]
  Two terms $[[a0]]$ and $[[a1]]$ are \emph{convertible}, written
  $[[a0 <=> a1]]$, if there exists some term $[[b]]$ such that $[[a0 =>+ b]]$
  and $[[a1 =>+ b]]$.
\end{definition}

\begin{figure}
\drules[P]{$[[a => b]]$}{Parallel reduction}{AppAbs, IfTrue, IfFalse, JRefl}
\caption{Parallel reduction ($\beta$ rules only) }
\label{fig:par}
\end{figure}

We prove, through standard
techniques~\citep{takahashi-parallel-reduction,plfa22.08}, the following
properties of parallel reduction.

\begin{lemma}[Reflexivity (parallel reduction)\footnote{\dotv{join.v}{Par\_refl}}]
  \label{lemma:parrefl}
  For all terms $[[a]]$, $[[a => a]]$.
\end{lemma}
\begin{lemma}[Congruence (p.r.)\footnote{\dotv{join.v}{par\_cong}}]
  \label{lemma:parcong}
  If $[[a0 => a1]]$ and $[[b0 => b1]]$, then $[[a0 { b0 / x } => a1 {
    b1 / x }]]$.
\end{lemma}
\begin{corollary}[Substitution (p.r.)\footnote{\dotv{join.v}{par\_subst}}]
  \label{lemma:parsubst}
  If $[[a0 => a1]]$, then $[[a0 {b / x } => a1 {b / x}]]$ for arbitrary $[[b]]$.
\end{corollary}
\begin{lemma}[Diamond property\footnote{\dotv{join.v}{par\_confluent}}]
  \label{lemma:pardiamond}
  If $[[a => b0]]$ and $[[a => b1]]$, then there exists some term
  $[[c]]$ such that $[[b0 => c]]$ and $[[b1 => c]]$.
\end{lemma}

Convertibility is an equivalence relation. The key step in
proving transitivity is showing the diamond property for parallel reduction.

\begin{lemma}[Reflexivity (convertibility)\footnote{\dotv{join.v}{Coherent\_reflexive}}]
  \label{lemma:coherencerefl}
  For all terms $[[a]]$, $[[a <=> a]]$.
\end{lemma}
\begin{lemma}[Symmetry (convertibility)\footnote{\dotv{join.v}{Coherent\_symmetric}}]
  \label{lemma:coherencesym}
  If $[[a <=> b]]$, then $[[b <=> a]]$.
\end{lemma}
\begin{lemma}[Transitivity (convertibility)\footnote{\dotv{join.v}{Coherent\_transitive}}]
  \label{lemma:coherencetrans}
  If $[[a0 <=> a1]]$ and $[[a1 <=> a2]]$, then $[[a0 <=> a2]]$.
\end{lemma}

The convertibility relation that we use for definitional equality is unusual
in that it is directly defined via parallel reduction, instead of using the
related notion of $\beta$-equivalence~\citep{barendregt1991introduction,coquand1990:cic}.
This choice does not change the language definition; a detailed argument of the
equivalence between $[[a <=> b]]$ and untyped $\beta$-equivalence can be found
in \citet{barendregt:lambda-calculi-with-types} and
\citet{takahashi-parallel-reduction}. However, this choice simplifies later
proofs, as we discuss in Section~\ref{sec:discuss}.

Our definitional equality is untyped: the judgment does not require the two
terms to type check and have the same type. The use of an untyped relation for
conversion is similar to Barendregt's Pure Type
Systems~\cite{barendregt1991introduction} and differs
from MLTT~\citep{Martin-Lof-1973}, where definitional
equality takes the form $\Gamma \vdash a \equiv b : A$.
% from which one can
% usually derive $[[G |- a : A]]$ and $[[G |- b : A]]$ after proving subject
% reduction.
By working with an untyped judgment, we can establish its
properties independently of the type system and the logical relation, using
well-established syntactic approaches.
%
\citet{siles2012pure} show the equivalence of Barendregt's Pure Type
System, which employs untyped equality, and
its variant that uses typed equality. This assures us that we do
not lose generality working with a system with untyped
conversion.
We compare this
definition with type-directed approaches to equality in
Section~\ref{sec:discuss}.

\subsection{Syntactic Typing}

\begin{figure}
\begin{mathpar}
  \fbox{$[[ |- G]]$} \hfill \textit{(Context well-formedness)} \\
  \drule{Ctx-Empty} \drule[width=3in]{Ctx-Cons}
\end{mathpar}
\drules[T]{$[[G |-  a : A]]$}{Typing}{Var, Set, Pi, Abs, App, Conv, Void, Absurd, Bool, True, False}
\begin{mathpar}
  \drule[width=3in]{T-If} \drule[width=2in]{T-Eq} \\
  \drule{T-Refl} \drule[width=4in]{T-J}
\end{mathpar}
\caption{Syntactic typing}
\label{fig:typing}
\end{figure}

Figure~\ref{fig:typing} gives the complete typing rules. The premises
highlighted in \colorbox{lightgray}{gray} can be shown to be admissible
syntactically, though they are required to strengthen the inductive
hypothesis of the fundamental theorem.

These rules are standard for dependent
type theories.  The variable rule, \rref{T-Var}, uses the auxiliary relation
$[[x : A in G]]$ that holds when a variable declaration is found in the
typing context. \Rref{T-Set} ensures that each universe belongs to the
next higher level. \Rref{T-Pi} ensures predicative quantification by
requiring that the domain and codomain types be typeable at the same universe
level. \Rref{T-Abs} ensures that all functions have well-formed dependent
types. In the application \rref{T-App}, the argument is substituted for the
variable in the result type.
\Rref{T-Conv} uses the convertibility relation from earlier
as our equality judgment for type conversion.

The elimination form for booleans in \rref{T-If} demonstrates dependent pattern
matching: the type of the expression \emph{depends} on the term being matched.
This result type involves a \emph{motive} $[[A]]$ abstracted over a boolean.
For the overall expression, $[[x]]$ is filled in by the match target $[[a]]$,
while in the $[[true]]$ (resp. $[[false]]$) branch, $[[x]]$ is filled in by
$[[true]]$ (resp. $[[false]]$). Informally, the type of each branch is more
precise than those in a nondependent match expression, since it knows which
branch it is in. An alternative perspective is that to prove $[[A {a/x}]]$, it
suffices to show that it holds for the two canonical cases that $[[a]]$ is
$[[true]]$ or is $[[false]]$.

% The elimination form for booleans, \rref{T-If}, demonstrates dependent pattern
% matching.  The result type of this expression, $[[A {a / x}]]$, is composed of some
% \emph{motive} $[[A]]$, a type where its single free variable has been replaced
% with the condition of the if expression. When typing the true branch, this
% substitution replaces the variable by $[[true]]$, and similarly for the false
% branch. As a result, the type system communicates the information gained from
% the test to each of the branches of the expression.

Dependent pattern matching similarly occurs when eliminating equality types.
Their well-formedness in \rref{T-Eq} enforces that the endpoints of the
equality have the same type, and there is a single canonical proof introduced
by \rref{T-Refl}. The elimination form in \rref{T-J} takes a proof of an
equality $[[p]]$ between $[[a]]$ and $[[b]]$, as well as an elimination body
$[[c]]$. Here, the motive $[[B]]$ is abstracted over the right endpoint $[[x]]$
and an equality $[[y]]$ between it and the fixed left endpoint $[[a]]$. The
eliminator states that to prove $[[B{b, p / x, y}]]$, it suffices to prove the
canonical case that $[[p]]$ is $[[refl]]$ (and consequently that $[[b]]$ is
$[[a]]$), which is witnessed by $[[c]]$.

% In \rref{T-J}, the elimination form, the subterm
% $p$ is a proof of an equality between $[[a]]$ and $[[b]]$. The subterm $[[c]]$
% is the body of the elimination form. In this rule, $[[B]]$ is the motive and
% has two free variables.  When checking $[[c]]$, the substitution for these
% variables changes from $[[b]]$ to $[[a]]$ and from $[[p]]$ to $[[refl]]$,
% witnessing the information gained through dependent pattern matching.

The universe hierarchy and the boolean base type give the
ability to compute a type using a term as input, referred to as
\emph{large elimination}. For example, the well-typed function
$[[\ x . if x Bool Void]]$ returns either $[[Bool]]$ or $[[Void]]$
depending on whether its input is $[[true]]$ or $[[false]]$.
% Working with a system with untyped equality has the huge benefit that
% the confluence result for untyped parallel reduction
% (Lemma~\ref{lemma:pardiamond}) is easily derivable without having to
% resort to the complex syntatic (resp. semantic) technique from
% \citet{siles2012pure} (resp. \citet{decagda}) to resolve
% the circularity of subject reduction and $\Pi$-injectivity.
% Section~\ref{sec:extension} explains how we
% generalize our technique to include $\eta$-law for functions and
% show the existence of normal form for well-typed (open and closed)
% terms, achieving a similar level of expressiveness of the type system
% and strength of metatheoretic property as \citet{decagda}.

% Finally, since our system has an infinite universe hierarchy, we can
% present the system Ã  la Russell by using the same judgment form
%$[[G |- a : A]]$ regardless of whether $[[a]]$ is a term or a type. There
% is no need to distinguish between big types and small types
% and duplicate our typing specification.
% \scw{Is there an advantage for Tarski universes even with infinite
% hierarchy? or no?}

% working with a system with untyped equality not only preserves the
% same level of generality,
% Of course,
% but also enables us to derive confluence
% (Lemma~\ref{lemma:parconfluent}) early on without having to use the
% intricate techniques from \citet{lemma:}

% TODO, where terms ... and ... are known to be
% well-typed. The equivalence of such systems and a system that uses
% untyped equality are explored in detail in ...

% Without fancy eta laws, it is easy to embed a typed language into an
% untyped language.

% We note that a more conventional presentation of
% \rref{T-Conv} would instead use full beta reduction as the base for
% the definition of coherence. However, since full beta reduction
% doesn't satisfy the diamond property, one typically needs parallel
% reduction as an auxilliary definition to derive the confluence of full
% beta reduction. Our formulation of \lang through parallel reduction
% is slightly more economical.

\section{Logical Relation}
\label{sec:logreldep}

% \scw{This section pre-supposes that we want to define a logical relation,
% but doesn't precisely state what we want to use it to prove, and why a logical
% relation is suitable. (And why the property you want to prove is difficult to
% show!) Should add more motivation here.}
\scw{We need to explicitly point out that the key ideas of this paper are
  discussed, here, in this section.
  We need to explicitly remark on why logical relations are difficult to
  define for dependent type theory and explain why this setting is more
  difficult than with simple types (STLC) or with polymorphic types (System F).
  \begin{itemize}
  \item Large eliminations
  \item Definitional equality (not all types look like types)
  \end{itemize}
  Should we be more explicit in our comparison with Girard's trick for polymorphic type?
  There, the definition stays recursive because it doesn't substitute for the variables
  in the function types. But that approach is not available in this setting, because not
  all quantified things are types. And we might need that information to interpret, say,
  equality types in the right way.
}
\scw{ We also need to explicitly point out that our logical relation is untyped.
  This has two benefits: it allows semantic typing to be meaningful independent from
  syntactic typing (cite Derek, forward reference to next section) and it avoids
  significant bookkeeping, especially in the case of Kripke logical relations (we need to define
  what these are).
  Is there a cost to an untyped relation?
}

Our ultimate goal is to prove the following consistency property for our type theory.

\begin{theorem}[Logical Consistency]
  \label{theorem:consistency}
  The judgment $[[empty |- a : Void ]]$ is not derivable.
\end{theorem}
The property can be formulated in a simply typed language, where
$[[Void]]$ is a type with no constructors. A related
property, the \emph{termination property} (for closed terms),
is commonly used in introductory materials such as
\citet{skorstengaard2019introduction}, \citet{pierce2002types}, and
\citet{harpertait} to motivate the need for a logical relation.

A na\"ive attempt at proving Theorem~\ref{theorem:consistency} by
induction on the derivation $[[empty |- a : Void]]$ would succeed in
almost all cases except for \rref{T-App}. In the application
case, we are given $[[empty |- b : Pi x : A . B]]$ and $[[empty |- a : A]]$, and
the equality that $[[B {a / x} = Void]]$. Our goal is to show that
$[[empty |- b a : Void]]$ is not possible. Unfortunately, there is
nothing we know about $[[b]]$ or $[[a]]$ from the induction hypothesis
because neither $[[Pi x : A . B]]$ nor $[[A]]$ are equal to $[[Void]]$,
so we have no way of deriving a contradiction from $[[empty |- b a :
Void]]$. The takeaway from this failed attempt is that, in order to
derive the consistency, we need to know something about types other
than $[[Void]]$. From a pragmatic point of view, proof by logical
relation can be seen as a sophisticated way of strengthening the
induction hypothesis. From the strengthened property, the fundamental
theorem, we will be able to derive consistency as a corollary.

\jc{Minor comment but ``proof by logical relation'' sounds grammatically strange to me,
but I'm not sure whether ``proof by logical relations'' or
``proof by a logical relation'' would be any more correct...}

The challenge behind applying proofs by logical relation to dependent types
stems from the difficulty in defining the logical relation itself. In simply
typed languages, the logical relation is a recursive function over the type
$[[A]]$. In dependently typed languages, the type $[[A]]$ can take the form
$[[(\ x . x ) Bool]]$, for example. To assign meaning to this type, we need to
first reduce it to $[[Bool]]$. However, we cannot write a function that
performs the reduction because we do not know the termination of well-typed
terms a priori. As a result, we define the logical relation as an inductively
defined relation, reminiscent of how we specify the reduction graph of a
partial function.
We later recover functionality of the relation in Lemma~\ref{lemma:logreldeter}.

\subsection{Definition of the Logical Relation}

\begin{figure}
\drules[I]{$[[Interp I i A S]]$}{Logical relation}{Set, Void, Bool, Eq, Red}
\[ \drule[width=5in]{I-Pi} \]
\caption{Logical relation for \lang{}}
\label{fig:logrel}
\end{figure}

The logical relation\footnote{\dotv{semtyping.v}{InterpExt}} for \lang{},
which takes the form $[[Interp I i A S]]$,
is defined as an inductively generated relation in Figure~\ref{fig:logrel}.
% The logical relation takes the form $[[Interp I i A S]]$.
The metavariables $[[A]]$ and $[[i]]$ are terms and naturals, respectively.
% \scw{Many introductory texts define the relation as a recursive function
% over type structure, or step-indices. You use an inductive relation instead, why?}
%\yl{resolved}
% \scw{Can we view this inductive relation as the graph of the partial function
% that is defined recursively over types?} \yl{reolsved}
% \scw{What is this form extensible too? impredicative quantification? recursive
% types? } \yl{resolved? impredicativity is hard}
% \scw{Is it worth observing here that this definition is not over sets of typed
% terms. That it characterizes all terms that look like booleans (i.e. evaluate to
% true or false) or all terms that look like proofs (i.e. evaluate to refl). The
% fact that there is no connection between p and a and b in the I-Eq case is strange
% looking. Need to explain.  }
% \yl{resolved}
The metavariables $[[I]]$ and $[[S]]$ are
sets with the following signatures,
using $[[PowerSet STm]]$ to denote the powerset of the set of terms.
\[
    [[I]] \in [[ { j | j < i  } ->  PowerSet STm ]] \qquad\qquad\qquad
    [[S]] \in [[PowerSet STm]]
\]
The function $[[I]]$ is a family of sets of terms indexed by natural numbers
strictly less than $[[i]]$, which represents the current universe level.
In \rref{I-Set}, the function $[[I]]$ is used to define the meaning of
universes that are strictly smaller than the current level $[[i]]$. The
restriction $[[j < i]]$ in \rref{I-Set} ensures predicativity of the system.

Predicativity allows us to tie the knot and obtain an interpretation for all
universe levels as the judgment $[[InterpR i A S]]$, which says that the type
$[[A]]$ is a type at universe level $[[i]]$, \emph{semantically} inhabited by
terms from the set $[[S]]$.

\begin{definition}[Logical relation for all universe levels]
\label{fig:logrelrec}
$[[InterpR i A S]]$ is defined recursively by well-foundedness of the $<$
relation on natural numbers.
\begin{equation*}
    [[InterpR i A S]] := [[ Interp I i A S  ]], \text{where } [[I j]]
    := [[{A | exists S , InterpR j A S}]] \text{ for } [[j < i]]
\end{equation*}
\end{definition}

Our system is predicative because the interpretation of the $[[i]]$th universe
is dependent only on universes strictly below $[[i]]$.
% which have already been defined.
This restriction ensures that the relation is well defined; otherwise, the
definition of $[[InterpR i A S]]$ would not be well founded, and
$[[Interp I i A S]]$ could call $[[I]]$ on universe levels greater than
or equal to $[[i]]$, which are yet to be defined.
%% SCW: removing the constraing wouldn't result in any system at all
% Removing the ordering constraint would result in a
% system where one can encode Girard's
% paradox~\citep{girard-thesis}. 

By unfolding Definition~\ref{fig:logrelrec}, we can
show that the same introduction rules for $[[Interp I i A S]]$ are
admissible for $[[InterpR i A S]]$. For example, 
we can prove the following derived rules:
\begin{center}
\drule[]{IR-Void} \qquad \drule[]{IR-Set}
\end{center}

In most informal presentations, instead of defining the logical
relation in two steps as we have shown above, the rules for $[[InterpR
i A S]]$ are given directly, with the implicit understanding that the
relation is an inductive definition nested inside a recursive
function over the universe level $[[i]]$. We choose
the more explicit definition not only because it is directly definable
in proof assistants that lack induction--recursion,
but also because it makes clear the induction
principle we are allowed to use when reasoning about $[[InterpR i A S]]$.

% The most general format of the induction principle over
% $[[InterpR i A S]]$ is first by strong induction over the universe level $[[i]]$
% followed by structural induction over $[[Interp I i A S]]$. As
% examples, ... (\rref{I-Set} and \rref{I-Red}).

We next take a closer look at the remaining rules for the inductive relation
$[[Interp I i A S]]$ in Figure~\ref{fig:logrel}.
\Rref{I-Void, I-Bool} capture terms that \emph{behave} like the inhabitants
of the $[[Void]]$ and $[[Bool]]$ types under an empty context. In particular,
the $[[Void]]$ type has no inhabitants, while the $[[Bool]]$ type only contains
terms that reduce to $[[true]]$ or $[[false]]$. Note that the characterization
of $[[Bool]]$ (and other inhabited types) in our logical relation does not
always correspond to well-typed or even closed terms. For example, the term
$[[if false (# Void true #) true]]$ is ill typed under the empty context,
but still belongs to the set $[[{ a | a =>+ true \/ a =>+ false }]]$ since it
evaluates to $[[true]]$.
The independence of syntactic typing in our logical relation allows
our semantic typing definition in Section~\ref{sec:logrelproof} to be
meaningful on its own. Furthermore, not having to embed scoping
information into the logical relation avoids extra bookkeeping and the
need for a Kripke-style logical relation when we extend our logical
relation to prove the existence of $\beta$-normal forms in Section~\ref{sec:extension}.
\yl{Not sure what to cite from Derek Dreyer. I know his blog post
  about semantic type soundness but is there a good paper to cite? one
of the rust papers?}
\jc{Should there be an explanation of what Kripke-style logical relations are here?}

\Rref{I-Eq} states that an equality type $[[a ~ b : A]]$ corresponds to the
set of terms that reduce to $[[refl]]$ when $[[a <=> b]]$ also holds and
otherwise corresponds to the empty set. Conditions like $[[a <=> b]]$ are
typically required for indexed types, of which equality types
are an instance. \Rref{I-Red} enables us to reduce types in order
to assign meanings. Recalling expression $[[(\ x . x ) Bool]]$, \rref{I-Red}
says that to know that $[[Interp I i  (\ x . x) Bool S ]]$ for some $[[S]]$,
it suffices to show that $[[Interp I i Bool S]]$, since
$[[(\ x . x) Bool => Bool]]$.
The derivation for $[[Interp I i (\ x . x) Bool { a | a =>+ true \/ a =>+ false }]]$
therefore follows by composing \rref{I-Red} and \rref{I-Bool}.

\Rref{I-Pi} is the most complex rule in our logical relation.
It states that, to build the interpretation of a dependent function type,
we first require the interpretation $[[S]]$ of its domain $[[A]]$.
Because the codomain depends on the function input,
rather than a single interpretation, we require an interpretation $[[F a]]$
of the codomain $[[B {a / x}]]$ for each $[[a in S]]$.
The interpretation of the overall function type is the set of terms $[[b]]$
such that for every term $[[a]]$ in the interpretation $[[S]]$ of $[[A]]$,
$[[b a]]$ is in the interpretation $[[F a]]$ of $[[B {a / x}]]$.

However, this form of the rule is less convenient to work with, since it
requires constructing the sets $[[F]]$ separately from the proof that they are
indeed interpretations of $[[B]]$. A more convenient form is the following
\rref{I-PiAlt}, which combines the existence of interpretations of the codomain
with their proofs.

\begin{center}
  \drule[]{I-PiAlt}
\end{center}

The second precondition now states that for every $[[a in S]]$,
there must exist some interpretation $[[S0]]$ of $[[B {a / x}]]$.
The interpretation of the overall function type then states that $[[b a]]$
must be in every interpretation $[[S0]]$ of $[[B {a / x}]]$.
Later, Lemma~\ref{lemma:logreldeter} proves functionality of the interpretation
of types, which guarantees that there must only be one such interpretation.
This alternate rule for the interpretation of function types follows from \rref{I-Pi}.

\begin{lemma}[I-PiAlt derivability\footnote{\dotv{semtyping.v}{InterpExt\_Fun\_nopf}}]
  \label{lemma:piintroalt}
  \Rref{I-PiAlt} is derivable from \rref{I-Pi}.
\end{lemma}

\begin{proof}
  The precondition
  $[[forall a, (# a in S implies (# exists S0 , Interp I i B { a / x } S0 #) #)]]$
  from \rref{I-PiAlt} immediately induces a function
  $[[F in S -> PowerSet STm]]$ such that
  $[[forall a, (# a in S implies Interp I i B { a / x } F a #)]]$,
  which is exactly what we need to apply \rref{I-Pi}.
\end{proof}

While \rref{I-PiAlt} is an instantiation of \rref{I-Pi}, by functionality,
the two rules are equivalent in the sense that every derivation involving
\rref{I-Pi} can be systematically replaced by \rref{I-PiAlt}.
Furthermore, functionality uniquely determines the function
$[[F in S -> PowerSet STm]]$ to be the functional relation
$[[{ (a , S0 ) | a in S implies Interp I i B { a /x } S0 }]]$.
This result is shown by Lemma~\ref{lemma:piinvalt},
the corresponding inversion lemma for \rref{I-PiAlt}.

Unfortunately, we cannot directly define the interpretation of function types
by \rref{I-PiAlt}, since the occurrence of $[[Interp I i B {a/x} S0]]$ in its
conclusion not only violates the syntactic strict positivity constraint on
inductive definitions required by proof assistants,
but is genuinely non-monotone when we treat the inductive definition
as the fixed point of an endofunction over the domain of relations.
\jc{What?}
Intuitively, the failure of monotonicity stems from the fact that the witness
picked in the precondition is not necessarily the same witness being referred
to in the postcondition as the relation grows, \jc{Why?} whereas the function $[[F]]$
in \rref{I-Pi} ``fixes'' the witnesses $[[S0]]$ as $[[F a]]$ for each
$[[a in S]]$, thus preventing the set of witnesses from growing. While it might
be possible to restrict the domain with additional constraints such as
functionality and inversion properties to justify the well-definedness of our
inductive relation with \rref{I-PiAlt}, we opt for our current
\rref{I-Pi} that immediately produces a
well-defined inductive relation and usable induction principle.
Then by deriving \rref{I-PiAlt} and its inversion lemma,
we avoid needing to manipulate the function $[[F]]$ directly.

\subsection{Properties of the Logical Relation}

In the rest of this section, we develop the theory of our logical relation
with the goal of showing four key properties: irrelevance
(Lemma~\ref{lemma:logrelcoherence}), functionality
(Lemma~\ref{lemma:logreldeter}), cumulativity
(Lemma~\ref{lemma:logrelcumulativity}), and the backward closure property
(Lemma~\ref{lemma:logrelbackclos}).
These properties semantically correspond to ones that hold for syntactic typing,
and allow us to prove that syntactic typing implies semantic typing, which we
define in the next section.

In particular, irrelevance states that convertible
types have the same interpretation, which is used to interpret \rref{T-Conv}.
Functionality with cumulativity shows that all interpretations of a type are
the same across all universe levels, which allows us to treat a type that
appears twice in a judgment uniformly, such as the function domain type in
\rref{T-App}. Finally, backward closure ensures that anything that reduces to
a term in the interpretation of a type is itself in the interpretation, which
we use to handle $\beta$-reductions in the cases for introduction forms.

For the majority of the properties that we prove in this section, we need no
information about the parameterized function $[[I]]$.  Each property about
$[[InterpR i A S]]$ follows as a corollary of a property about
$[[Interp I i A S]]$ with no or few assumptions imposed on $[[I]]$. As a result,
we state most of our lemmas in terms of $[[Interp I i A S]]$ without duplicating
them in terms of $[[InterpR i A S]]$.

First, we prove inversion principles for our logical relation. Given
$[[Interp I i A S]]$ where $[[A]]$ is in some head form such as $[[Bool]]$ or
$[[Pi x : A . B]]$, the inversion lemma allows us to say something about the set
$[[S]]$. The proofs are simple, but we sketch out the case for
functions to help readers confirm their understanding of \rref{I-Pi}.

\begin{lemma}[Inversion (logical relation)]
  \label{lemma:interpinv}\leavevmode
  \begin{enumerate}
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Void\_inv}} If $[[Interp I i Void S]]$, then $[[S = emptyset]]$.
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Bool\_inv}} If $[[Interp I i Bool S]]$, then $[[S = { a | a =>+ true \/ a =>+ false   }]]$.
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Eq\_inv}} If $[[Interp I i a ~ b : A S]]$, then $[[S = { p | p =>+ refl /\ a <=> b  }]]$.
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Fun\_inv}}
    If $[[Interp I i Pi x : A . B S1]]$, then there exist
    $[[S in PowerSet STm]]$, $[[F in S -> PowerSet STm]]$ such that:
    \vspace{-0.5\baselineskip}
    \begin{itemize}
    \item $[[Interp I i A S ]]$
    \item $[[forall a, (# a in S implies Interp I i B { a / x } F a #)]]$
    \item $[[S1 = { b | forall a, (# a in S implies b a in F a #) }]]$
    \end{itemize}
  \item\footnote{\dotv{semtyping.v}{InterpExt\_Univ\_inv}} If $[[Interp I i Set j S]]$, then $[[j < i]]$ and $[[S = I j]]$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  We show only the inversion property for the function type.
  We start by induction on the derivation of $[[Interp I i Pi x : A . B S1]]$.
  There are only two possible cases we need to consider.
  \begin{description}
  \item[\Rref{I-Pi}:] Immediate.
  \item[\Rref{I-Red}:]
    We are given that $[[Pi x : A . B => C]]$ and that $[[Interp I i C S1]]$.
    By inversion on parallel reduction, we have that $[[C]] = [[Pi x : A0 . B0]]$
    for some $[[A0]], [[B0]]$, and that $[[A => A0]]$ and $[[B => B0]]$.
    From the induction hypothesis on $[[Interp I i (# Pi x : A0 . B0 #) S1]]$,
    there exist $[[S in PowerSet STm]]$ and $[[F in S -> PowerSet STm]]$ such that:
    \begin{itemize}
      \item $[[Interp I i A0 S ]]$
      \item $[[forall a, (# a in S implies Interp I i B0 { a / x } F a #)]]$
      \item $[[S1 = { b | forall a, (# a in S implies b a in F a #) }]]$
    \end{itemize}
    By Lemma~\ref{lemma:parsubst}, we have $[[B {a /x} => B0 {a/x}]]$ for all
    $[[a]]$. As a result, $[[S]]$ and $[[F]]$ satisfy the following properties
    by \rref{I-Red} on $[[A => A0]]$ and $[[B {a /x} => B0 {a/x}]]$, respectively:
    \begin{itemize}
      \item $[[Interp I i A S]]$
      \item $[[forall a, (# a in S implies Interp I i B { a / x } F a #)]]$
    \end{itemize}
    These properties are exactly what we need to finish the proof.
  \end{description}
  \vspace{-2\baselineskip}
\end{proof}

An immediate consequence of the inversion principle for the interpretation
of universes is that if a type is in such an interpretation, then that type
itself has an interpretation.

\begin{corollary}\hspace{-0.5em}\footnote{\dotv{semtyping.v}{InterpUnivN\_Univ\_inv'}} \label{lemma:interpinvSet}
  If $[[InterpR i Set j S]]$ and $[[A in S]]$,
  then there exists some set $[[S0]]$ such that $[[InterpR j A S0]]$ and $[[j < i]]$.
\end{corollary}

\Rref{I-Red} bakes into the logical relation the backward preservation
property. That is, given $[[Interp I i A S]]$, if $[[B =>+ A]]$, then
$[[Interp I i B S]]$ also holds. The following property shows that
preservation holds in the usual forward direction as well.

\begin{lemma}[Forward preservation (l.r.)\footnote{\dotv{semtyping.v}{InterpExt\_preservation}}]
  \label{lemma:interppreservation}
  If $[[Interp I i A S]]$ and $[[A => B]]$, then $[[Interp I i B S]]$.
\end{lemma}

\begin{proof}
  We carry out the proof by induction on the derivation of
  $[[Interp I i A S]]$.

  The only interesting case is \rref{I-Red}. Given that
  $[[A => B0]]$ and $[[Interp I i B0 S]]$, we need to show
  for all  $[[B1]]$ such that $[[A => B1]]$, we have $[[Interp I i B1
  S]]$. By the diamond property of parallel reduction
  (Lemma~\ref{lemma:pardiamond}), there exists some term $[[B]]$ such
  that $[[B0 => B]]$ and $[[B1 => B]]$. By the induction hypothesis,
  we deduce $[[Interp I i B S]]$ from $[[B0 => B]]$ and $[[Interp I i
  B0 S]]$. By \rref{I-Red} and $[[B1 => B]]$, we conclude that
  $[[Interp I i B1 S]]$.

  The remaining cases all fall from the induction hypotheses and the basic
  properties of convertibility and parallel reduction established in
  Section~\ref{sec:spec}.
\end{proof}

From forward preservation and \rref{I-Red}, we can easily
derive the following corollary that two convertible types always interpret
into the same set. We adopt the terminology from \citet{martin-lof-a-la-coq}
and refer to this property as \emph{irrelevance}.

\begin{corollary}[Irrelevance (l.r.)\footnote{\dotv{semtyping.v}{InterpUnivN\_Coherent}}]
  \label{lemma:logrelcoherence}
  If $[[Interp I i A S]]$ and $[[A <=> B]]$, then $[[Interp I i B S]]$.
\end{corollary}

Because the definition of our logical relation is an inductive relation, it is
not immediately obvious that each type $[[A]]$ interprets to a unique set $[[S]]$.
The following lemma shows that our logical relation is indeed functional.

\begin{lemma}[Functionality (l.r.)\footnote{\dotv{semtyping.v}{InterpExt\_deterministic}}]
  \label{lemma:logreldeter}
  If $[[Interp I i A S0]]$ and $[[Interp I i A S1]]$, then $[[S0 = S1]]$.
\end{lemma}

\begin{proof}
  The proof proceeds by induction on the derivation of the first
  premise $[[Interp I i A S0]]$.
  All cases that are not \rref{I-Red} follow immediately from
  Lemma~\ref{lemma:interpinv}, the inversion properties.

  In \rref{I-Red}, there exists some $[[B]]$ such that $[[A => B]]$ and
  $[[Interp I i B S0]]$. Our goal is to show that given $[[Interp I i A S1]]$
  for some $[[S1]]$, we have $[[S0 = S1]]$. By forward preservation
  (Lemma~\ref{lemma:interppreservation}) and $[[A => B]]$,
  we know that $[[Interp I i B S1]]$.
  Then $[[S0 = S1]]$ immediately follows from the induction hypothesis.
\end{proof}

Functionality allows us to prove an inversion lemma for the derivable
\rref{I-PiAlt}, which does not mention the function $[[F]]$ found in \rref{I-Pi}.

\begin{lemma}[Alternate inversion of function types (l.r.)\footnote{\dotv{semtyping.v}{InterpExt\_Fun\_inv\_nopf}}]
  \label{lemma:piinvalt}\leavevmode
  If $[[Interp I i Pi x : A . B S]]$, then there exists some $[[S0]]$
  such that:
  \begin{itemize}
    \item $[[Interp I i A S0]]$
    \item $[[forall a, (# a in S0 implies (# exists S1 , Interp I i B {a / x} S1 #) #)]]$
    \item $[[S = { b | forall a, (# a in S0 implies forall S1, (# Interp I i B {a / x} S1 implies  b a in S1 #) #) }]]$
  \end{itemize}
\end{lemma}

\begin{proof}
  Immediate from Lemmas~\ref{lemma:interpinv} and \ref{lemma:logreldeter}.
\end{proof}

The next lemma shows cumulativity of the logical relation:
if a type has an interpretation at a lower universe level,
then we obtain the same interpretation at a higher level.

\begin{lemma}[Cumulativity (l.r.)\footnote{\dotv{semtyping.v}{InterpExt\_cumulative}}]
  \label{lemma:logrelcumulativity}
  If $[[Interp I i A S]]$ and $[[i < j]]$, then $[[Interp I j A S]]$.
\end{lemma}

\begin{proof}
  Trivial by structural induction over the derivation of $[[Interp I i A S]]$.
\end{proof}

Note that in the statement of cumulativity, we implicitly assume that $[[I]]$
is defined on naturals strictly less than $[[j]]$. Then by cumulativity and
trichotomy of the order on naturals (i.e. for any $i, j$, either $i < j$ or
$i = j$ or $i > j$), we show that functionality holds even when considering
interpretations of type at different universe levels.

\begin{corollary}[Level-irrelevant functionality (l.r.)\footnote{\dotv{semtyping.v}{InterpExt\_deterministic'}}]
  \label{lemma:logreldeterhet}
  If $[[Interp I i0 A S0]]$ and $[[Interp I i1 A S1]]$, then $[[S0 = S1]]$.
\end{corollary}

\begin{proof}
  Immediate from Lemmas~\ref{lemma:logreldeter} and
  \ref{lemma:logrelcumulativity}.
\end{proof}

The final property we want to show is that the output set $[[S]]$ from
the logical relation is closed under expansion. Unlike the previous
lemmas, we directly state the lemma
in terms of $[[InterpR i A S]]$ rather than $[[Interp I i A S]]$
because we need the actual instantiation of $I$ to prove the \rref{I-Set} case.

\begin{definition}[Closure under expansion]
  We say that a set of terms $[[S]]$ is closed under expansion if
  given $[[a in S]]$, then $[[b in S]]$ for all $[[b => a]]$.
\end{definition}

\begin{lemma}[Backward closure of interpretations (l.r.)\footnote{\dotv{semtyping.v}{InterpUnivN\_back\_clos}}]
  \label{lemma:logrelbackclos}
  If $[[InterpR i A S]]$, then $[[S]]$ is closed under expansion.
\end{lemma}

\begin{proof}
  By the definition of $[[InterpR i A S]]$, we unfold $[[InterpR i A S]]$ into
  $[[Interp I i A S]]$ where $[[I j]] := [[{A | exists S, InterpR j A S}]]$ for
  $[[j < i]]$. We then proceed by induction on the derivation of
  $[[Interp I i A S]]$.

  All cases are trivial except for the \rref{I-Set} case, where the goal is to
  show that $[[I j]]$ is closed under expansion for all $[[j < i]]$;
  that is, if $[[B => A]]$ and $[[A in I j]]$, then $[[B in I j]]$.
  By the definition of $[[I]]$, this is equivalent to showing that if
  $[[B => A]]$ and $[[Interp I j A S]]$ for some $[[S]]$,
  then there exists some $[[S0]]$ such that $[[Interp I j B S0]]$.
  Letting $[[S0]]$ be $[[S]]$, by \rref{I-Red},
  we have that $[[Interp I j B S]]$ as required.
\end{proof}

\section{Semantic Typing and Consistency}
\label{sec:logrelproof}

In this section, we show that all closed, well-typed terms are contained
in the interpretation of their types. In other words, $[[empty |- a : A]]$
implies that there exists a set $[[S]]$ such that $[[InterpR i A S]]$ and
$[[a in S]]$ hold; we write $[[a in InterpR i A]]$ as shorthand. This result
gives us consistency because $[[InterpR i Void emptyset]]$ holds, so if there
were a closed, well-typed term of type $[[Void]]$, it would be a member of the
empty set, which is a contradiction.

To prove this result, we define a notion of semantic typing based on the
logical relation and prove the fundamental lemma, which states that syntactic
typing implies semantic typing. Semantic typing extends the logical relation
from being a type-indexed family of predicates on closed terms to a
type-indexed family of predicates on open terms.

The necessity of semantic typing as an extra layer of definitions on top of the
logic relation is understood in the simply typed 
setting~\citep{skorstengaard2019introduction, harpertait,pierce2002types}.
In our setting,
\jc{Is this trying to say that our setting is different from the simply typed setting,
or the same as the simply typed setting? Are the definitions to follow on semantic typing
taken from the works cited above, or are they our own modifications?}  
attempting to show that $[[empty |- a : A]]$ implies $[[a in InterpR i A]]$ by
induction on the derivation of $[[empty |- a : A]]$ will fail for \rref{T-Abs},
where the induction hypothesis is not helpful since the body of the lambda term
is typed under a nonempty context. Using the definition of semantic typing, we
can state a strengthened property that is actually provable.

\begin{definition}[Semantically well formed substitutions\footnote{\dotv{soundness.v}{$\rho$\_ok}}]
  We say that a substitution $[[rho]]$ is semantically well formed with respect
  to the context $[[G]]$, written as $[[rho |= G]]$, when
  \[ \forall [[x : A in G]], [[i]], [[S]],
  [[InterpR i A { rho } S ]] \text{ implies } [[rho x in S]]. \]
\end{definition}

% To generalize our logical relation to open
% terms, we define the semantic typing judgment by closing the open
% terms with a substitution whose codomain consists of terms that
% respect the interpretation of the types from the context.
% The full
% definitions of well-formed substitution ($[[rho |= G]]$), semantic
% typing ($[[ G |= a : A]]$), and semantic context well-formedness
% ($[[|= G]]$) are presented in Figure~\ref{fig:semtyping}.

$[[rho |= G]]$ states that for every variable $[[x]]$ and its type $[[A]]$ in the $[[G]]$,
$[[rho x]]$ is a term that inhabits every interpretation of $[[A {rho}]]$.
While functionality of the logical relation tells us each type has at most one
interpretation, quantifying over all possible interpretations $[[S]]$ makes the
proofs slightly simpler. Since $[[rho |= G]]$ typically appears as a hypothesis,
$[[S]]$ is easy to instantiate. The following structural properties handle the
few cases where we need to prove $[[rho |= G]]$ as a goal; the second property
depends on functionality.

\begin{lemma}[Well-formed $[[rho]]$ w.r.t. empty\footnote{\dotv{soundness.v}{$\rho$\_ok\_nil}}]
  \label{lemma:rhowfempty}
  $[[rho |= empty]]$ holds for any $[[rho]]$.
\end{lemma}

\begin{lemma}[Well-formed $[[rho]]$ w.r.t. cons\footnote{\dotv{soundness.v}{$\rho$\_ok\_cons}}]
  \label{lemma:rhowfcons}
  If $[[a in InterpR i A]]$ and $[[rho |= G]]$, then
  $[[rho .: x -> a |= G ++ x : A]]$.
\end{lemma}

Semantic well-typedness is then defined using well formed substitutions
to handle the contexts of open terms.

\begin{definition}[Semantic typing\footnote{\dotv{soundness.v}{SemWt}}]
  We say that $[[a]]$ is semantically typed as $[[A]]$ under the context $[[G]]$,
  written $[[G |= a : A]]$, when
  \[ \forall [[rho |= G]], \exists [[j]] \text{ such that } [[a {rho} in InterpR j A {rho}]]. \]
\end{definition}

Semantic typing $[[G |= a : A]]$ says that for all well-formed substitutions $[[rho |= G]]$,
$[[a {rho}]]$ is in the interpretation of $[[A { rho }]]$ at some universe level.
This definition is standard, though dependency of types on terms requires
also applying the substitution $[[rho]]$ to the type $[[A]]$, and that
$[[A { rho }]]$ have an interpretation.
Finally, we define semantic well-formedness of contexts, analogous to the
relation $[[|-G]]$.

\begin{definition}[Semantic context well-formedness\footnote{\dotv{soundness.v}{SemWff}}]
  We say that the context $[[G]]$ is semantically well formed, written $[[|= G]]$, when
  \[ \forall [[x : A in G]], \exists i \text{ such that } [[G |= A : Set i]]. \]
\end{definition}

Recall that $[[|- G]]$ is defined inductively in terms of the
syntactic typing judgment. We take a different approach here for its
semantic counterpart $[[|= G]]$. The definition of
$[[|= G]]$ is not telescopic: for $[[|- G]]$, a variable appearing
earlier in the context is well-scoped under a truncated context,
whereas for $[[|= G]]$, the types are only required to be
semantically well-formed with respect to the full context, regardless of their
position in $[[G]]$. While the definition of $[[|= G]]$ could be strengthened,
the simpler definition is sufficient for showing the fundamental lemma.
We can recover the structural rules for $[[|= G]]$ as lemmas.

\begin{lemma}[Empty context well-formedness\footnote{\dotv{soundness.v}{SemWff\_nil}}]
  \label{lemma:semwffempty}
  $[[|= empty]]$ holds.
\end{lemma}
\begin{lemma}[Cons context well-formedness\footnote{\dotv{soundness.v}{SemWff\_cons}}]
  \label{lemma:semwffcons}
  If $[[|= G]]$ and $[[G |= A : Set i]]$, then $[[|= G ++ x : A]]$.
\end{lemma}

The following lemma makes statements of the form $[[G |= A : Set i]]$ easier to
work with.

\begin{lemma}[Set Inversion\footnote{\dotv{soundness.v}{SemWt\_Univ}}]
  \label{lemma:setinv}
  The following are equivalent:
  \begin{itemize}
  \item $[[G |= A : Set i]]$;
  \item $\forall [[rho |= G]]$, there exists a set $[[S]]$ such that
    $[[InterpR i (A {rho}) S]]$.
  \end{itemize}
\end{lemma}

\begin{proof}
  The forward direction is immediate by Lemma~\ref{lemma:interpinvSet}.
  We now consider the backward direction
  and show that $[[G |= A : Set i]]$ given the second bullet.

  Suppose $[[rho |= G]]$. We know that there exists some $[[S]]$
  such that $[[InterpR i (A {rho}) S]]$. By the definition of semantic
  typing, it suffices to show that there exists some $[[j]]$ and
  $[[S0]]$ such that  $[[InterpR j Set i S0]]$ and $[[A {rho} in
  S0]]$.
  Pick $[[Suc i]]$ for $[[j]]$ and $[[ { A | exists S , InterpR i A S }
  ]]$ for $[[S0]]$; it is trivial to verify the conditions hold.
\end{proof}

Next, we show some non-trivial cases of the fundamental theorem as
top-level lemmas, namely for \rref{T-Var}, \rref{T-Set}, \rref{T-Pi},
\rref{T-Abs}, and \rref{T-App}.

\begin{lemma}[ST-Var]
  \label{lemma:stvar}
  If $[[|= G]]$ and $[[x : A in G]]$, then $[[G |= x : A]]$.
\end{lemma}
\begin{proof}
  Suppose $[[rho |= G]]$. By the definition of semantic typing, we
  need to show that there exists some $[[i]]$ and $[[S]]$ such that
  $[[InterpR i A { rho } S]]$ and $[[rho x in S]]$ hold.
  By the definition of semantic context well-formedness, we deduce
  from $[[|= G]]$ and $[[x : A in G]]$ that there exists some universe
  level $[[i]]$ such that $[[G |= A : Set i]]$. By
  the equivalence from Lemma~\ref{lemma:setinv}, there exists an $[[S]]$
  such that $[[InterpR i A {rho} S]]$. Finally, by the definition of
  $[[rho |= G]]$, we know that $[[rho x in S]]$.
\end{proof}

% Next, we show some non-trivial cases of the fundamental theorem as
% top-level lemmas.
% We first formulate the definition of valid renamings and prove that
% semantic typing satisfies renaming so we can weaken the context when
% reasoning about the variable case of the fundamental lemma
% (Lemma~\ref{lemma:stvar}). Intuitively, given a valuation $[[rho |= G ++ D]]$, it is easy to show that we can extract some valuation
% $[[rho0]]$ such that $[[rho0 |= G]]$, where $[[rho0]]$ is obtained by
% ``truncating'' $[[rho]]$. As a result, if we know that $[[G |= a :A]]$, then we can conclude that $[[G ++ D |= a0 : A0 ]]$, where
% $[[a0]]$ and $[[A0]]$ are obtained by shifting $[[a]]$ and $[[A]]$
% after weakening the context; this implication holds because $[[rho |=G ++ D]]$ induces a context $[[rho0]]$ such that $[[rho0 |= G]]$ so we
% can make use of the premise $[[G |= a : A]]$ to derive what we need
% for the conclusion. We recommend the readers to skip the proofs of
% Lemmas~\ref{lemma:validtruncate} through \ref{lemma:semrenaming}
% during the first read as long as they have an intuitive understanding
% of what the renaming property is meant to capture.

% \scw{Make this a definition? Would it be useful to create notation
%   for the relation, such as $[[xi]]:[[G]]\Rightarrow[[D]]$? }
% We say that $[[xi]]$ is
% valid from the context $[[G]]$ to the context $[[D]]$ if the
% following condition holds.

% \scw{Need a transition here that you are starting to explain the semantic typing
% rules.}

\begin{lemma}[ST-Set]
  \label{lemma:stset}
  If $[[i < j]]$, then $[[G |= Set i : Set j]]$.
\end{lemma}
\begin{proof}
  Immediate by Lemma~\ref{lemma:setinv} and \rref{IR-Set}.
\end{proof}

\begin{lemma}[ST-Pi]
  \label{lemma:stpi}
  If $[[G |= A : Set i]]$ and $[[G ++ x  : A |= B : Set i]]$, then $[[G |= Pi
  x : A . B : Set i]]$.
\end{lemma}
\begin{proof}
  Applying Lemma~\ref{lemma:setinv} to the
  conclusion, it now suffices to show that given $[[rho |= G]]$, there
  exists some $[[S]]$ such that $[[InterpR i (Pi x : A . B){rho} S]]$.
  From Lemma~\ref{lemma:setinv} and $[[G |= A : Set i]]$, we know that
  there exists some set $[[S0]]$ such that $[[InterpR i A {rho} S0]]$.
From $[[G ++ x : A |= B : Set i]]$, we know that there must
exist $[[S]]$ such that $[[InterpR i B {rho .: x -> a} S]]$ for every $[[a
in S0]]$. The conclusion immediately follows from the admissible \rref{I-PiAlt}.
\end{proof}

\begin{lemma}[ST-Abs]
  \label{lemma:stabs}
  If $[[G |= Pi x : A . B : Set i]]$ and $[[G ++ x : A |= b : B]]$, then $[[G |=
  \ x . b : Pi x : A . B]]$.
\end{lemma}
\begin{proof}
  By unfolding the definition of $[[G |= \ x . b : Pi x : A . B]]$, we need to
  show that given some $[[rho |= G]]$, there exist some $[[i]]$ and
  $[[S]]$ such that $[[InterpR i (Pi x : A . B){rho}
  S]]$ and $[[(\ x . b) {rho} in S]]$.

  By Lemma~\ref{lemma:setinv} and the premise $[[G |= Pi x : A . B : Set
  i]]$, there exists some set $[[S]]$ such that
  $[[InterpR i (Pi x : A . B){rho} S]]$. It now suffices to show that
  $[[(\ x . b){rho} in S
  ]]$. By Lemma~\ref{lemma:piinvalt}, the alternate inversion
  principle for \rref{I-Pi}, there exists some $[[S0]]$ such
  that the following hold:
  \begin{itemize}
  \item $[[InterpR i A{rho} S0]]$;
  \item $[[forall a, (# a in S0 implies (# exists S1 , InterpR i B
    {rho .: x -> a}
    S1 #) #)]]$; and
  \item $[[S = { b | forall a, (# a in S0 implies forall
      S1, (# InterpR i B {rho .: x -> a} S1 implies  b a in S1 #) #) }]]$.
  \end{itemize}
  To show that $[[(\ x . b){rho} in S]]$, we need to prove
  that given $[[a in S0]]$ and
  $[[Interp I i B {rho .: x -> a} S1]]$, we have $[[(\x . b ){rho}
  a in S1]]$.
  By Lemma~\ref{lemma:logrelbackclos}, the set $[[S1]]$ is closed
  under expansion. Since $[[( \ x . b ){rho}
  a => b {rho .: x -> a}]]$, it suffices to show that
  $[[b {rho .: x -> a} in S1]]$.
  By $[[G ++ x : A |= b : B]]$, Lemma~\ref{lemma:rhowfcons}, and $[[a in S0]]$,
  we know that there exist some $[[j]], [[S2]]$ such that
  $[[InterpR j B {rho .: x -> a} S2]]$ and $[[b {rho .: x -> a} in S2]]$.
  Finally, by level-irrelevant functionality of the logical relation
  (Lemma~\ref{lemma:logreldeterhet}), we have that $[[S1 = S2]]$
  and thus $[[b {rho .: x -> a} in S1]]$.
\end{proof}

\begin{lemma}[ST-App]
  \label{lemma:stapp}
  If $[[G |= b : Pi x : A . B]]$ and $[[G |= a : A]]$, then $[[G |= b
  a : B {a / x}]]$.
\end{lemma}
\begin{proof}
Suppose $[[rho |= G]]$. The goal is to show that there exists some
$[[i]]$ and $[[S1]]$
such that  $[[b {rho} a {rho} in S1 ]]$ and $[[InterpR i B {a / x} {rho}
S1]]$, or equivalently $[[InterpR i B {rho .: x -> a {rho}} S1]]$, since
$[[B {a / x}{rho}]] = [[B {rho .: x -> a {rho}}]]$. By the premise $[[G |= b :
Pi x : A . B]]$, Lemma~\ref{lemma:setinv}, and Lemma~\ref{lemma:piinvalt},
there exists some $[[i]]$ and $[[S0]]$ such that the following hold:
  \begin{itemize}
  \item $[[InterpR i A{rho} S0]]$;
  \item $[[forall a0, (# a0 in S0 implies (# exists S1 , InterpR i B
    {rho .: x ->  a0}
    S1 #) #)]]$; and
  \item $[[forall a0, (# a0 in S0 implies (# forall
      S1, (# InterpR i B {rho .: x -> a0} S1 implies  b {rho} a0 in S1 #) #) #)]]$.
  \end{itemize}
  Instantiating the variable $[[a0]]$ from the last two bullets with
  the term $[[a {rho}]]$, the conclusion immediately follows.
\end{proof}

\begin{theorem}[The Fundamental Theorem\footnote{\dotv{soundness.v}{soundness}}]
  \label{theorem:soundness}\leavevmode
  \begin{itemize}
  \item If $[[G |- a : A]]$, then $[[G |= a : A]]$.
  \item If $[[|- G]]$, then $[[|= G]]$.
  \end{itemize}
\end{theorem}
\begin{proof}
  By mutual induction over the derivation of $[[G |- a : A]]$ and $[[|- G]]$.
  The cases related to context well-formedness immediately follow
  from Lemmas~\ref{lemma:semwffempty} and \ref{lemma:semwffcons}.
  The semantic typing rules
  (Lemmas~\ref{lemma:stvar},~\ref{lemma:stset},~\ref{lemma:stpi},~\ref{lemma:stabs},~\ref{lemma:stapp})
  are used to discharge their syntactic counterparts
  (e.g. Lemma~\ref{lemma:stabs} for \rref{T-Abs}). The remaining
  cases not covered by those lemmas are similar to the ones already shown.
\end{proof}

We now give a proof of logical consistency (Theorem~\ref{theorem:consistency}),
which states that the judgment $[[empty |- a : Void]]$ is not derivable,
using the fundamental theorem.

\begin{proof}
  Suppose $[[empty |- a : Void]]$ is derivable. By the
  fundamental theorem, we have $[[empty |= a : Void]]$, which states
  that for all $[[rho |= empty]]$, $[[j]]$, $[[S]]$ such
  that $[[InterpR j Void S]]$, we have $[[a {rho} in S]]$.
  By Lemma~\ref{lemma:rhowfempty}, any $[[rho]]$ we pick trivially
  satisfies $[[rho |= G]]$; for convenience, we pick $[[idtm]]$,
  so that we have $[[a in S]]$.
  However, by the $[[Void]]$ case of the inversion property
  (Lemma~\ref{lemma:interpinv}), $[[S]]$ must be the empty set,
  which is a contradiction.
\end{proof}

The fundamental theorem also tells us that closed terms of type 
$[[Bool]]$ reduce to either $[[true]]$ or $[[false]]$.

\begin{corollary}[Canonicity\footnote{\dotv{soundness.v}{canonicity}}]
If $[[empty |- b : Bool]]$, then either $[[b =>+ true]]$ or $[[b =>+ false]]$.
\end{corollary}
\begin{proof}
  Similar to above, using instead the $[[Bool]]$ case of the inversion property.
\end{proof}

\section{Existence of \texorpdfstring{$\beta$}{beta}-normal forms}
\label{sec:extension}

In this section, we extend the logical relation in Figure~\ref{fig:logrel}
to show the existence of $\beta$-normal forms for both open and closed
well-typed terms. More precisely, we prove it possible to repeatedly apply
parallel reduction to reduce a term to its unique normal form. Consequently,
this shows that the convertibility relation is decidable.

This extension of the logical relation demonstrates that our proof technique
can also be used to reason about the reduction properties of open terms, not
just of terms after closing substitutions. Reasoning about open terms is
especially important for dependently typed languages, as type checking involves
working with open terms.
\scw{
  Add when we can find a reference:
  However, even non dependently-typed languages employ such techniques,
  especially in the case of relational semantics.
}
While the extension use well-known techniques,
\jc{Does this need a citation?}
it remains short and demonstrates the robustness of our initial framework.

\begin{figure}
  \begin{align*}
    [[e]] & ::= [[x]] \mid [[e f]] \mid [[J e f]] \mid [[if e f f]] \mid [[absurd e]]
    && \textit{Neutral terms} \\
    [[f]] & ::= [[e]] \mid [[Set i]] \mid [[Pi x : f . f]] \mid [[Bool]] \mid [[f ~ f : f]] \mid [[Void]] \mid [[\ x . f]] \mid [[true]] \mid [[false]] \mid [[refl]]
    && \textit{Normal terms}
  \end{align*}
  \caption{$\beta$-neutral and $\beta$-normal forms}
  \label{fig:nenf}
\end{figure}

Figure~\ref{fig:nenf} gives the $\beta$-neutral forms $[[e]]$ and
$\beta$-normal forms $[[f]]$ of \lang{}. Neutral terms consist of variables and
elimination forms that do not reduce any further, while normal terms consist of
neutral terms and other introduction forms. We use the judgment forms $[[ne a]]$
and $[[nf a]]$ to indicate that there exists a term $[[e]]$ (resp. $[[f]]$)
such that $[[a = e]]$ (resp. $[[a = f]]$). Additionally, we define predicates
to describe terms that weak normalization: reduction to neutral or normal forms
by parallel reduction.

% The syntactic forms $[[e]]$ and $[[f]]$ (Figure~\ref{fig:nenf}) capture the
% neutral terms and normal forms with respect to $\beta$-reduction\scw{Why not say parallel reduction here? We can be specific and say that that the only reductions available for these terms are identity reductions and cite
% \footnote{\dotv{normalform.v}{nf\_refl}} }.\yl{I always think of
% normal form as the specific definition that says the relation can't
% step. We can refer to nfrefl but that requires some explanation about
% the definition of normal form and the complication that it doesn't
% hold in $\eta$ (maybe it's fine if we just don't mention it in the
% $\eta$ case)}\scw{A terminal form is a syntactic characterization of terms that don't step 
% according to a particular relation. A normal form is a terminal form of a normalizing relation.}

\begin{definition}[Weak normalization]
  We define weak normalization of a term $[[a]]$ to a neutral form, written
  $[[wne a]]$, or to a normal form, written $[[wn a]]$, as the following:
  \begin{align*}
    [[wne a]] &\iff \exists [[e]], [[a =>+ e]] \\
    [[wn a]]  &\iff \exists [[f]], [[a =>+ f]]
  \end{align*}
\end{definition}

Weakly normalizing terms are composed of other weakly normalizing terms
following the structure of neutral and normal forms. For example, an
application normalizes to neutral form if the function normalizes to neutral
form and the argument to normal form. Similar lemmas hold for
function types\footnote{\dotv{normalform.v}{wn\_pi}},
abstractions\footnote{\dotv{normalform.v}{wn\_abs}},
conditionals\footnote{\dotv{normalform.v}{wne\_if}},
and absurdity\footnote{\dotv{normalform.v}{wne\_absurd}}.

\begin{lemma}[Application (wne)\footnote{\dotv{normalform.v}{wne\_app}}]
  \label{lemma:wnewn}
  If $[[wne a]]$ and $[[wn b]]$, then $[[wne (a b)]]$.
\end{lemma}

\begin{proof}
  By induction over the length of reduction sequences in
  $[[wne a]]$ and $[[wn b]]$.
\end{proof}

The modifications to the logical relation\footnote{\dotv{semtypingopen.v}{InterpExt}}
are given in Figure~\ref{fig:logrelopen}. The new \rref{I-Ne} asserts that
only terms that reduce to neutral forms inhabit the interpretation of neutral
types. The remaining rules are updates to existing rules; we omit those for
function types and universes as they are unchanged.

\begin{figure}
  \drules[I]{$[[Interp I i A S]]$}{Extended logical relation}{Ne, VoidNew, BoolNew, EqNew}
  \caption{Extended logical relation for \lang{} (new and changed rules)}
  \label{fig:logrelopen}
\end{figure}

The changes in \rref{I-BoolNew} and \rref{I-VoidNew} follow the same pattern. An open
term of type $[[Bool]]$ may reduce to $[[true]]$ or $[[false]]$ as before, but
may also reduce to a neutral term, such as a variable. Likewise, while in an
empty context the interpretation of $[[Void]]$ remains empty, if the context
is not empty, then it may contain, for instance, a variable in the context of
type $[[Void]]$ or that can be eliminated into type $[[Void]]$.

In \rref{I-EqNew}, we add preconditions asserting that $[[a]]$, $[[b]]$, and
$[[A]]$ are normal so that we only model normalizing equality types.
Furthermore, we only require $[[a <=> b]]$ when the equality proof reduces to
$[[refl]]$. If the proof term reduces to a neutral term, there is nothing we
can assert about the relationship between $[[a]]$ and $[[b]]$.

Because we are working with open terms, we need additional lemmas for parallel
reduction. First, it preserves neutral and normal forms.

\begin{lemma}[Preservation of neutral and normal forms (p.r.)\footnote{\dotv{normalform.v}{nf\_ne\_preservation}}]
  \label{lemma:parnenf}
  If $[[a => b]]$, then $[[ne a]]$ implies $[[ne b]]$,
  and $[[nf a]]$ implies $[[nf b]]$.
\end{lemma}

Since neutral and normal forms have no redexes, if $[[a]]$ is neutral or normal,
parallel reduction never $\beta$-reduces, so $[[a = b]]$ additionally holds,
but preservation of neutral and normal forms is sufficient for our proof.

Next, parallel reduction also satisfies an antirenaming property. A
\emph{renaming} $[[xi]]$ is a special case of substitution where all variables
are replaced by other variables. In this special case, if a renamed term
reduces, then we can separate the reduction from the renaming, asserting that
renaming never introduces new redexes.

\begin{lemma}[Par antirenaming\footnote{\dotv{normalform.v}{Par\_antirenaming}}]
  \label{lemma:parantirenaming}
  If $[[a < xi > => b0]]$, then there exists some $[[b]]$ such that
  $[[b < xi > = b0]]$ and $[[a => b]]$.
\end{lemma}

Together, these lemmas prove an inversion property for weakly normalizing forms:
if an application of a term to a variable is weakly normalizing, then the term
must also be weakly normalizing.

\begin{lemma}\hspace{-0.5em}\footnote{\dotv{normalform.v}{ext\_wn}}
  \label{lemma:extwn}
  If $[[wn (a x)]]$, then $[[wn a]]$.
\end{lemma}
\begin{proof}
  By induction over the length of the reduction sequence in $[[wn (a x)]]$.
  When the reduction is a $\beta$-reduction, $[[a]]$ is a function and the
  application reduces to a renaming by $[[x]]$, so the conclusion follows
  from Lemma~\ref{lemma:parantirenaming}.
\end{proof}

All the properties we have shown in
Section~\ref{sec:logreldep} and \ref{sec:logrelproof} before the fundamental
lemma can be proven in the same order, where the new cases due to \rref{I-Ne}
and the modifications to \rref{I-Void, I-Eq, I-Bool} can be discharged by
Lemma~\ref{lemma:parnenf}.

% \jc{Don't mention this right now; delay to $\eta$ section.}
% Furthermore, Lemma~\ref{lemma:parnenf}, in its current weaker form, would
% still hold after we extend our equational theory with the function $\eta$
% rule, where parallel reduction can take $\eta$ steps but still preserves
% $\beta$-normal form.

Before we can prove the fundamental theorem and derive the normalization
property as its corollary, we need to formulate and prove an \emph{adequacy}
property of the logical relation, which states that the interpretation of each
type is a \emph{reducibility candidate} ($\CR$), which characterizes sets of
weakly normalizing terms. In contrast to proving consistency, which only
requires knowing that the interpretation of $[[Void]]$ is empty, adequacy
allows us to conclude that all terms in \emph{every} interpretation have a
normal form.

To prove adequacy, we need to strengthen it to with more information about
neutral terms as we proceed by induction. In particular, we need to know that
all terms that reduce to neutral forms are contained within the interpretation.
Therefore, we define reducibility candidates as follows, inspired by
\citet{girard1989proofs}, but modified for weak normalization only. Adequacy
then states that interpretations are in $\CR$.

\begin{definition}[Reducibility Candidates ($\CR$)\footnote{\dotv{semtypingopen.v}{CR}}]
  A set of terms $[[S]]$ is in $\CR$
  if and only if conditions $\CR_1$ and $\CR_2$ hold.
  \begin{itemize}
  \item $[[S]] \in \CR_1 \iff\ [[forall a, (# wne a implies a in S #)]]$
  \item $[[S]] \in \CR_2 \iff\ [[forall a, (# a in S implies wn a #)]]$
  \end{itemize}
\end{definition}

\begin{lemma}[Adequacy]
  \label{lemma:adequacy}
  \footnote{\dotv{semtypingopen.v}{adequacy}}
  If $[[InterpR i A S]]$, then $[[S]] \in \CR$.
\end{lemma}

\begin{proof}
  We start by strong induction over $[[i]]$. We are given the
  induction hypothesis that for all $[[j < i]]$, $[[InterpR j A S]]$
  implies $[[S]] \in \CR$. Our goal is to show that $[[InterpR i A S]]$
  implies $[[S]] \in \CR$.

  By Definition~\ref{fig:logrelrec}, we have $[[Interp I i A S]]$,
  where $[[I j]] := [[{A | exists S , InterpR j A S}]]$ for $[[j < i]]$.
  We then proceed by structural induction on the derivation of
  $[[Interp I i A S]]$. The interesting cases are \rref{I-Pi} and \rref{I-Set}.
  For \rref{I-Pi}, proving $S \in \CR_1$ requires Lemma~\ref{lemma:wnewn}. When
  proving $S \in \CR_2$, by the structural induction hypotheses, we have that
  $[[wn (b a)]]$ for any $[[wne a]]$, while the goal is to show that $[[wn b]]$.
  We pick an arbitrary $[[x]]$ and conclude by applying Lemma~\ref{lemma:extwn}
  to $[[b x]]$.

  The \rref{I-Set} case is the most interesting. We must show that
  for all $[[j < i]]$, $[[{A | exists S, InterpR j A S}]] \in \CR$.
  We immediately know that $[[{A | exists S, InterpR j A S}]] \in \CR_1$
  by \rref{I-Ne}. It remains to show that
  $[[{A | exists S, InterpR j A S}]] \in \CR_2$, or equivalently that
  $[[Interp I j A S]]$ implies $[[wn A]]$, where $[[I]]$ is defined as above
  over $k < j$.
  
  We assume $[[Interp I j A S]]$ and proceed once more by structural induction
  on its derivation. All cases are trivial except for \rref{I-Pi}. The induction
  hypothesis immediately gives $[[wn A]]$. To derive $[[wn (Pi x : A . B)]]$,
  it remains to show $[[wn B]]$. We use the outermost induction hypothesis on
  $[[Interp I j A S]]$ to show that an arbitrary $[[y]]$ semantically inhabits
  $[[A]]$ by $\CR_1$, followed by the structural induction hypothesis to derive
  $[[wn (B {y / x})]]$. We conclude $[[wn B]]$ by applying antirenaming
  (Lemma~\ref{lemma:parantirenaming}).
\end{proof}

The formulation of semantic well-typedness and the fundamental lemma from
Section~\ref{sec:logrelproof} remains unchanged. The proof of the fundamental
lemma\footnote{\dotv{soundnessopen.v}{soundness}} is still carried out by
induction over the typing derivation, where the additional neutral term related
cases are handled by adequacy. The normalization property then follows as a
corollary of the fundamental theorem.

\begin{corollary}[Existence of $\beta$-normal forms\footnote{\dotv{soundnessopen.v}{mltt\_normalizing}}]
  \label{corollary:exbetanf}
  If $[[G |- a : A]]$, then $[[wn a]]$ and $[[wn A]]$.
\end{corollary}

\begin{proof}
  By the fundamental lemma, we know that $[[G |= a : A]]$. That is,
  for all $[[rho |= G]]$, there exists some $[[i]]$ and $[[S]]$ such
  that $[[InterpR i A {rho} S]]$ and $[[a {rho} in S]]$.
  We pick the $[[rho]]$ to be the identity substitution $[[idtm]]$, and the
  condition $[[idtm |= G]]$ is satisfied by adequacy, which says that variables,
  as neutral terms, semantically inhabit the interpretations of the types in
  $[[G]]$. Performing the identity substitutions, we then know that there exist
  $[[i]]$ and $[[S]]$ such that $[[InterpR i A S]]$ and $[[a in S]]$. Finally,
  by adequacy again, we conclude that $[[wn a]]$ and $[[wn A]]$.
\end{proof}

% Due to the non-deterministic nature of parallel reduction, we need to take a
% few more steps to convert the existence of $\beta$-normal form into a decision
% procedure for type conversion. More specifically, we can show that a
% deterministic evaluation strategy such as leftmost-outermost reduction can
% always find the $\beta$-normal form if there exists one. Then the termination
% of that strategy immediately follows from Corollary~\ref{corollary:exbetanf}
% However, we omit such proofs since they can can be formulated on untyped
% lambda terms and thus are orthogonal to the specifics of dependently typed
% systems. Instead, we redirect readers to \citet{factorization-essentially,
%   takahashi-parallel-reduction} for the details.

The extension of our logical relation to prove normalization of open
\emph{and} closed terms closely mirrors the progression from
normalization of closed terms~\citep{harpertait} to normalization of
open terms~\citep{harperkripke} in the simply typed lambda calculus.
Indeed, \citet{abel2019poplmark} mechanize normalization generalized
to open terms. In this setting, as above,
adequacy must be proven before the
fundamental theorem so that they can handle elimination rules such as
\rref{T-App} where the function is a neutral term. % In
% \citet{abel2019poplmark}, a variant of Lemma~\ref{lemma:extwn} is used
% in the exact same way to show the normalization of lambda forms
%
Dependent types make the adequacy proof more complicated because we
also need to know that every \emph{type} has a normal form, not just
terms. This complicates our proof specifically in the \rref{I-Set} case for
our adequacy property (Lemma~\ref{lemma:adequacy}).

Overall, despite the dependently typed setting, it is reassuring that
once we have laid the foundational technique for handling dependent types in
our logical relation, the extension to open terms boils down to
properties that can be derived independently from the logical relation through
syntactic means.

\section{Existence of \texorpdfstring{$\beta\eta$-}{beta-eta--}normal forms}
\label{sec:betaeta}

\jc{Delete this section}

\citet{nbeincoq,decagda,martin-lof-a-la-coq}
include the $\eta$ law for functions in their equational theory and
use relational models to justify its validity.
In our system, we can easily incorporate the function $\eta$ law to the equational
theory of \lang{} by adding the following parallel reduction rule.
\begin{center}
  \drule[width=2.5in]{P-AbsEta}
\end{center}
In this section, we show how we easily extend the existence of $\beta$-normal
forms from Section~\ref{sec:extension} to the existence of
$\beta\eta$-normal forms after this addition. 

First, we recover the same confluence result about parallel reduction using the
standard techniques from \citet{barendregt:lambda-calculi-with-types,
  takahashi-parallel-reduction}, though anti-renaming
(Lemma~\ref{lemma:parantirenaming}) must be proven before the diamond property
(Lemma~\ref{lemma:pardiamond}). Another complication is that
the anti-renaming property and the diamond property for parallel reduction are now proven through
induction on a size metric of lambda terms; \rref{P-AbsEta} reduces a term
that is not a strict subterm.

Note that, after this extension, the specification of our logical
relation does not require
any updates. The proof of the fundamental theorem also remains
identical since the complications introduced by $\eta$ are hidden
behind the proofs of the diamond property and the anti-renaming property.
As before, $\ottkw{ne}$ and $\ottkw{nf}$
represent $\beta$-neutral and $\beta$-normal forms, and the
fundamental theorem shows us that every well-typed term has a
$\beta$-normal form. However, in the presence of the $\eta$ reduction
rule, Lemma~\ref{lemma:parnenf} tells us that $\eta$ reduction
preserves $\beta$-normal forms (i.e. does not produce new
$\beta$-redexes). Furthermore, since the $\eta$ reduction rule for
functions strictly decreases the size of the term, the existence of
$\beta\eta$ normal form trivially follows.
\begin{corollary}[Existence of $\beta\eta$-normal form]
\label{corollary:exbetaeta}
If $[[G |- a : A]]$, then $[[a]]$ has $\beta\eta$-normal form.
\end{corollary}

%% SCW: This is out of place, and redundant with the next paragraph.
% In contrast to our approach, related work \cite{nbeincoq,decagda,martin-lof-a-la-coq}
% employs a relational model to
% justify the $\eta$-law for functions.
% Instead, we prove
% confluence for our parallel reduction extended with $\eta$-law for functions
% and continue using a logical predicate (i.e. a unary logical relation) to justify
% $\eta$-law as part of our equational theory.
A well-known issue with our approach is the failure of syntactic
confluence when the lambda term contains type annotations. A simple
counterexample is $[[\- y : B . ((\- x : A . a) y)]]$ where $[[y notin
fv (\- x : A . a)]]$; depending on
whether \rref{P-AbsEta} is performed on the whole term or
\rref{P-AppAbs} is used on the inner $\beta$ redex, we end up with the
terms $[[\- x : B . a]]$ (after $\alpha$-conversion) or $[[\- x : A . a]]$, where $[[A]]$ and $[[B]]$ are not
necessarily syntactically equal terms. \citet{choudhury:ddc} resolve
this problem by stating their confluence result in terms of an
equivalence relation that quotients out parts of the terms that are
computationally irrelevant; the annotations of lambda terms are
ignored since the behavior of a lambda term is not affected by its
type annotation. We believe the same approach is applicable to our
proof.

The bigger issue is extensions such as $\eta$-laws for unit and
products. Surjective pairing, for example, is not confluent for untyped lambda
terms~\cite{KLOP198997}. The relational, type-annotated, and Kripke-style models from
\citet{nbeincoq,decagda,martin-lof-a-la-coq} can be more easily
extended to support these rules.
We note, however, that the issue with $\eta$ rules is not exclusive to dependently
typed languages and has been studied in more limited languages that
are either simply
typed~\citep{pierce2004advanced,pfenning1997computation} or
dependently typed but without large
eliminations~\citep{harper2005equivalence,
abel2005untypedconvsurjective}. Common workarounds include
type-directed conversion and shifting the focus to obtaining
$\eta$-long forms~\cite{Abel12}.

While not without limitations, our simple proof demonstrates the core
building blocks of more complex arguments, thus paving the way for experimentation
and eventual extension to more expressive systems.\scw{I tried to reword your
sentence, but I am still not happy with it.}

% \scw{Is this specific to $\eta$? Or is this also true for $\beta$? Can we move this
% sentence elsewhere? I'd like to use the previous sentence as a conclusion for
% this section.}
% Finally, due to technical
% details, our mechanized proof about the existence of $\beta\eta$ form does not
% immediately induce a decision procedure for type conversion in
% \lang{}. We discuss this issue and its workaround in
% Section~\ref{sec:conversionalgo}.

\section{Mechanization}
\label{sec:logrelmech}

\begin{figure}[h]
  \centering
  \begin{tabular}{ | l |  c  | c | c | c | }
    \hline
    & Consistency & Normalization & Safety\\
    \hline
    Libraries            &  15 & $=\joinrel=$ & $=\joinrel=$ \\
    Syntactic typing     &  87 & $=\joinrel=$ & $=\joinrel=$ \\
    Untyped reduction    & 345 & $=\joinrel=$ & $=\joinrel=$ \\
    Neutral/normal forms & --- & 287  & ---  \\
    Logical relation     & 295 & 385  & ---  \\
    Semantic soundness   & 195 & 215  & ---  \\
    Syntactic soundness  & --- & ---  & 655  \\
    \hline
    Total                & 934 & 1331 & 1099 \\
    \hline
  \end{tabular}
  \caption{Nonblank, noncomment lines of code of the Coq development.
    The $=\joinrel=$ marker indicates that the line count is the same as the column to the left.
    The --- marker indicates the file does not contribute to the total.}
  \label{fig:linecount}
\end{figure}

To demonstrate the scale of our proof scripts, Figure~\ref{fig:linecount}
shows the number of non-blank, non-comment lines of code%
  \footnote{calculated by \texttt{tokei}:
    \url{https://github.com/XAMPPRocky/tokei} \hfill}
for each file of our development,
including the base consistency proof from Sections~\ref{sec:logreldep} and
\ref{sec:logrelproof}, along with the extension to $\beta$-normalization from
Section~\ref{sec:extension}. For comparison, we have also proven syntactic
type safety through preservation
\footnote{\dotv{syntactic\_soundness.v}{subject\_reduction}} and
progress\footnote{\dotv{syntactic\_soundness.v}{wt\_progress}}.

% The $\beta\eta$-normalization proof from Section~\ref{sec:betaeta} comprises
% 1549 lines of non-blank, non-comment lines of code. We choose not to include
% it in the chart, because of slight differences in lemma dependencies for
% untyped reduction and normal forms that make the comparison less
% informative. However, when compared to the $\beta$-normalization extension,
% the $\beta\eta$ extension has the same line count in the definition of the
% logical relation and the semantic soundness proof.

The Autosubst 2 tool takes our 16-line syntax specification, written in
higher-order abstract syntax, and generates the Coq syntax specification,
renaming and substitution functions, and lemmas and tactics that allow
reasoning about those functions. The autogenerated syntax file and other
Autosubst library files (516 LOC total) are not included in the figure.

\subsection{Axioms}

Our Coq development assumes two axioms: functional extensionality
and propositional extensionality. The former is also required by
the Autosubst 2 libraries. Both axioms are known to be consistent
with Coq's metatheory.
%
These axioms bridge the gap between our mechanization and our informal
proofs. For example, in set theory, to show that two sets $[[S0]]$ and
$[[S1]]$ are equal, it suffices to show the extensional property that
$\forall x, x \in [[S0]] \iff x \in [[S1]]$. We use this property in a few
proofs, notably for inversion (Lemma~\ref{lemma:interpinv}) and functionality
(Lemma~\ref{lemma:logreldeter}), where we deal with equalities of
the interpretations, which are sets of terms.

In the Coq mechanization, sets of terms $[[PowerSet STm]]$ are encoded as the
predicates over terms \texttt{tm -> Prop} asserting whether the term is in the
set. We want to be able to show that two predicates $P$ and $Q$ are equal when
$\forall x, P(x) \iff Q(x)$ holds, but this predicate extensionality property
does not hold in axiom-free Coq, and requires both functional and propositional
extensionality.

\subsection{Encoding the logical relation}

As discussed, the logical relations $[[Interp I i A S]]$ in
Figures~\ref{fig:logrel} and \ref{fig:logrelopen} are encoded as inductive
definitions indexed by sets of terms representing the semantic inhabitants of
$[[A]]$. $[[S]]$ therefore has type \texttt{tm -> Prop}, while $[[I]]$ has type
\texttt{forall j, j < i -> tm -> Prop}, representing a set of types in
universes strictly smaller than $[[i]]$. Because the overall inductive
definition represents a functional relation between a type at a universe level
and its interpretation as a set of terms, it too lives in \texttt{Prop}.

Due to the limitations of eliminating inductives in \texttt{Prop}, the encoding
of the rule for function types differs from the presentation in \rref{I-Pi},
reproduced below.
%
\begin{mathpar}
  \drule[width=\textwidth]{I-Pi}
\end{mathpar}

While the first and last premises are propositions, $[[S]]$ and $[[F]]$
represent sets of terms, which are not propositions. While this is not an issue
when proving properties of the logical relation, which never require projecting
out these sets, it is an issue when proving the fundamental theorem, and in
particular proving the function case in Lemma~\ref{lemma:stpi}. By the definition
of semantic typing, the induction hypothesis (after some instantiation) gives
%
$$\forall [[a in S]], \exists [[i]], [[S0]] \text{ such that }
  [[InterpR i B {a / x} S0]] \text{ and } [[b a in S0]].$$

We would like to define $[[F]]$ as the function that projects out $[[S1]]$ from
the induction hypothesis, but the existential quantification being a proposition
prevents us from doing so without the axiom of choice. To avoid relying on
classical reasoning, we instead encode $[[F]]$ as a functional relation
$[[R in S * PowerSet STm]]$ such that
$\forall [[a]], \exists [[S0]], [[( a , S0 ) in R]]$ holds. The actual encoding
is given as \rref{I-PiCoq} below.
%
\begin{mathpar}
  \drule[width=\textwidth]{I-PiCoq}
\end{mathpar}

From this rule, we are still able to derive \rref{I-PiAlt} as before. The
benefit of working with the derived rule is that it does not involve $[[R]]$,
which is uniquely determined to be
$[[{ (a , S0 ) | a in S implies Interp I i B { a /x } S0 }]]$ as before.

An alternative to these various encodings in Coq is to use
\emph{induction--recursion} (IR)~\citep{IR}, which is available in Agda~\citep{agda}.
IR permits mutually defining an inductive definition along with a recursive function
on the inductive. We can change our perspective on the logical relation and
view it as two separate pieces: an inductive interpretation $\llbracket A \rrbracket^{i}$
of types, and a recursive interpretation $[[a in InterpR i A]]$ of types as sets of terms.
The rule for function types and its interpretation would be defined as follows.
%
\begin{mathpar}
  \drule[width=\textwidth]{IIR-Pi} \\
  [[b in InterpIR i Pi x : A . B]] \coloneqq
    [[forall a, (# a in InterpIR i A implies b a in InterpIR i B {a / x} #)]]
\end{mathpar}

Because the inductive definition no longer needs to quantify over all sets of terms,
we can work entirely within Agda's lowest \texttt{Set} universe without worrying
about impredicativity or universe polymorphism. However, to define the above,
we still need an intermediate definition involving the interpretation $[[I]]$
of types at $[[j < i]]$ and tie the knot as before. While the same properties
are provable, the inductive--recursive logical relation changes the structure
and the order of these proofs; for instance, a weaker form of functionality
is provable before and is required to prove forward preservation.

Further details of the IR encoding can be found in our Agda mechanization of
the consistency of \lang. Due to the lack of automation support, proofs about
syntactic properties of terms and proofs using well-founded induction on levels
are written manually. Even so, the total number of nonblack, noncomment lines
of code remains small at below 1300 lines, and functional extensionality is
still the only axiom used.

% First, this definition requires the use of Coq's impredicatve \texttt{Prop}
% sort. In particular, the function $[[F]]$ in \rref{I-Pi} requires the use of
% impredicativity because $[[F]]$ must later be instantiated into something
% defined in terms of the logical relation itself (e.g. in
% Lemma~\ref{lemma:piintroalt}).
% In the Coq mechanized proof,
% we encode $[[PowerSet STm]]$ as the type \texttt{tm -> Prop}, a predicate over lambda terms. 
% the definition of $[[Interp I i A S]]$ has type
% \texttt{Prop}, where \texttt{I} has type \texttt{nat -> tm -> Prop} and \texttt{S} has type \texttt {tm -> Prop}.

% The inductive
% definition of the logical relation in Figure~\ref{fig:logrel} requires
% the impredicativity of Coq's \texttt{Prop} sort since\scw{or ``so that''?} in \rref{I-Pi},
% the function $[[F]]$ can be later instantiated into the logical
% relation itself (e.g. in the proof of Lemma~\ref{lemma:piintroalt}).

% However, if desired, we could consistently replace the use of \texttt{Prop}
% with Coq's predicative sort \texttt{Type} in the definition of
% $[[Interp I i A S]]$. This alternative definition could be part of
% the interpretation for any \emph{finite} number of universes. The use of
% \texttt{Type} becomes troublesome only when we attempt to define
% $[[InterpR i A S]]$, the top-level logical relation 
% (Definition~\ref{fig:logrelrec}) that recursively calls itself at smaller universe
% levels. Therefore, the one feature of \lang{} that truly requires
% impredicativity is its countable universe hierarchy.

% The definition of $[[Interp I i A S]]$ has an almost one-to-one
% correspondence to the Coq definition. The main difference is the
% specification of $[[I]]$. In
% Section~\ref{sec:logreldep}, we define $[[I]]$ as a function over
% numbers less than $[[i]]$, the universe level. In Coq, we only require
% $[[I]]$ to be a function with the set of natural numbers as its domain.
% In the Coq encoding of $[[InterpR i A S]]$, we define $[[I]] \in [[SNat -> PowerSet STm]]$ as follows.
% \begin{equation*}
%   \begin{split}
%     [[I j]] &=
%      \begin{cases}
%       \ [[{A | exists S , InterpR j A S}]] & \text{when } j < i \\
%       \ [[emptyset]] & \text{otherwise}
%     \end{cases}
%   \end{split}
% \end{equation*}
% Since $[[I]]$ is only applied to numbers strictly less than $[[i]]$ in
% \rref{I-Set}, we can retroactively show that the set we return in the $j
% \geq i$ case is junk data that does not affect the result of the logical
% relation. This property allows us to recover the simple equation for $[[InterpR i A S]]$ shown in Definition~\ref{fig:logrelrec}.

% \Rref{I-PiCoq} shows how \rref{I-Pi} is actually encoded in our
% mechanized proof.
% \begin{center}
%   \drule[width=5in]{I-PiCoq}
% \end{center}
% Compared to \rref{I-Pi}, \rref{I-PiCoq} replaces the function
% $[[F]]$ with a total relation $[[R]]$. The equivalence of these two
% rules follows from the fact that the logical relation is a
% partial function (Lemma~\ref{lemma:logreldeter}). In set-theoretic
% notation, \rref{I-Pi} is more readable. However, if we want to encode
% the same rule in Coq, we must encode $[[F]]$ as a relation (with type
% \texttt{tm -> (tm -> Prop) -> Prop}) that satisfies the functionality constraint:
% \mintinline{coq}{forall a S0 S1, F a S0 -> F a S1 -> S0 = S1}.
% In comparison, \rref{I-PiCoq} does not require this side condition and
% results in a simpler definition.

% We note that we cannot ascribe $[[F]]$ the type \texttt{tm -> (tm ->
%   Prop)} since Coq requires functions of such type to be
% computable. While defining $[[F]]$ as a computable Coq function rather than a
% functional relation does result in a concise encoding of \rref{I-Pi},
% we will have trouble instantiating $[[F]]$ with the logical relation,
% which is defined as a relation that we prove to be functional, rather
% than a computable function.

% In Coq, there is a distinction between computable functions and
% relations that can later be proven to be functional. The former can be
% viewed as a strict subset of the latter in axiom-free Coq. To be more
% precise, given a relation \texttt{R : A -> B -> Prop} subject to the
% totality and functionality constraints ($\forall$ \texttt{a} $\in$
% \texttt{A}, there exists a unique \texttt{b} $\in$ \texttt{B} such
% that \texttt{R a b} is inhabited), we do not immediately obtain a function
% \texttt{F : A -> B} such that $\forall$ \texttt{a} $\in$ \texttt{A},
% \texttt{R a (F a)} is inhabited. However, the functional side
% conditions of a relation is clunky to express and tend to block
% automation. A simple workaround is to assume the axiom of unique choice, which
% is known to be consistent with Coq and allows us to induce a function \texttt{F
% : A -> B} once we have shown the relation \texttt{R : A -> B -> Prop}
% is functional. This approach would make our Coq development match
% the text version of our proof from Section~\ref{sec:logrelproof} more
% closely.

% However, we choose instead an axiom-free workaround and define
% \rref{I-Pi} as follows in our Coq mechanization.
% It should be easy to verify that the preconditions of
% \rref{I-Pi}, \rref{I-PiAlt}, and \rref{I-PiCoq} are all
% equivalent. After establishing Lemma~\ref{lemma:logreldeter}, it is
% possible to further show that the conclusions of the rules are
% equivalent, too.\scw{If it is easy, you should have already done it.
% Unless it is really long and boring.}

% This formulation allows us to derive Lemma~\ref{lemma:piintroalt}
% before we even show that our logical relation is
% functional/deterministic, but does not affect the proof structure
% otherwise.
% We choose to keep this discrepancy between the Coq development and the
% description of the logical relation presented in Section~\ref{sec:logreldep} since the
% skolemization process is more intuitively expressed in terms of
% function symbols rather than relation symbols. Otherwise, we do not
% see a clear advantage of \rref{I-PiCoq} over \rref{I-Pi} in set
% theory, where there is no distinction between computable functions and
% functions in general. \scw{not sure I follow this last bit}

\subsection{Automation in Coq}
\label{sec:automation}

Our Coq mechanization heavily uses automation, supported by the tools
Autosubst 2~\citep{autosubst2} and CoqHammer~\citep{czajka2018hammer}.

We use the Autosubst 2 framework to produce Coq syntax files based on a de
Bruijn representation of variable binding and capture-avoiding substitution.
In addition to these generated definitions, Autosubst 2 provides a powerful
\texttt{asimpl} tactic used to prove the equivalence of two terms
constructed using the primitive operators provided by the framework. This
tactic simplifies the reasoning about substitution as many
substitution-related properties about syntax are immediately discharged by
\texttt{asimpl}.

For other automation tasks that are not specific to binding, we use
the powerful \texttt{sauto} tactic provided by CoqHammer to write
short and declarative proofs. For example, here is a one-line proof of
the triangle property about parallel reduction, from which the diamond
property (Lemma~\ref{lemma:pardiamond}) follows as a corollary.
The triangle property states
that if $[[a => b]]$, then $[[b]]\Rightarrow [[a]]^*$, where $[[a]]^*$
is the Takahashi translation~\citep{takahashi-parallel-reduction},
which roughly corresponds to simultaneous reduction of the redexes in
$[[a]]$, excluding the new redexes that appear as a result of
reduction.
\begin{minted}[escapeinside=||,mathescape=true]{coq}
Lemma Par_triangle a : forall b, (a |$\Rightarrow$| b) -> (b |$\Rightarrow$| tstar a).
Proof.
  apply tstar_ind; hauto lq:on inv:Par use:Par_refl,Par_cong ctrs:Par.
Qed.
\end{minted}
In prose, the triangle property can be proven by induction over the graph of
\mintinline{coq}{tstar a}, the Takahashi translation. Options \texttt{inv:Par}
and \texttt{ctrs:Par} say that the proof involves inverting and constructing
of the derivations of parallel reduction. The option
\texttt{use:Par\_refl,Par\_cong} allows the automation tactic to use the
reflexivity and congruence properties of parallel reduction as lemmas.

The flag \texttt{lq:on} tunes CoqHammer's search algorithm, which is never
specified manually. Instead, we first invoke the \texttt{best} tactic provided by
CoqHammer, specifying only the \texttt{inv}, \texttt{ctrs}, and lemmas that we
want to use. The \texttt{best} tactic then iterates through possible
configurations and provides us with a replacement with the tuned performance
flags that save time for future re-execution of the proof script.

The automation provided by CoqHammer not only gives us a proof that is shorter
and more resilient to changes, but also provides useful documentation for
readers who wish to understand the mechanized proof. Although automation
performs extensive search, we can configure it to only use lemmas or invert
derivations specified by the \texttt{use} or \texttt{inv} flags.

\jc{Just a digression but I find that CoqHammer automated proofs are \emph{not}
good documentation compared to how we might have otherwise written the proofs
manually. While the flags indicate what lemmas and inversions \emph{may} be used,
the \texttt{best} tactic does not delete unused ones, so there's no way of knowing
which ones actually \emph{are} used, and the automation hides away \emph{how} they
are used. Automation makes proofs resilient to change when things go right, but
when things go wrong, it becomes much harder to discern what lemmas I need to add
when I don't even know where the existing ones are used.}

\ifextended
\subsection{Extraction of a conversion algorithm}
\label{sec:conversionalgo}
A constructive proof of the existence of $\beta\eta$-normal form
for well-typed terms (Corollary~\ref{corollary:exbetaeta}) induces a
normalization algorithm. From this normalization procedure, we can
derive a normalize-and-compare algorithm. Given two well-typed terms
$[[a]]$ and $[[b]]$, to know whether $[[a <=> b]]$, we apply the
normalization algorithm on $[[a]]$ and $[[b]]$ to obtain $\beta\eta$
normal forms $[[f0]]$ and $[[f1]]$. The algorithm then returns true
exactly when $[[f0]]$ and $[[f1]]$ are syntactically equal. This
algorithm is referred to as normalize-and-compare by
\citet{pierce2004advanced}.

The soundness of the algorithm is immediate. The completeness of the
algorithm is justified by the confluence property of the untyped
reduction relation. Suppose $[[a <=> b]]$, $[[a =>+ f0]]$, $[[b =>+
f1]]$ but $[[f0]]$ is syntactically distinct from $[[f1]]$. By the transitivity of convertibility, we have $[[f0 <=> f1]]$.
Because $[[f0]]$ and $[[f1]]$ are both in
$\beta\eta$-normal forms and can only reduce to themselves, we must
have $[[f0 = f1]]$, which is a contradiction.

In our development, since our countable universe hierarchy
relies on impredicativity from the metatheory, we encode our
properties in Coq's \texttt{Prop} sort, which is not very suitable for
code execution. On the other hand, due to our heavy reliance on
automated proof search, even if we were able to extract an algorithm from \texttt{Prop},
the algorithm would be unpredictable because its definition depends on
the specific choices made by the proof search algorithm.

However, % with a little more effort,
it should be possible to recover a
precise algorithm. The first step is to define a deterministic
small-step reduction relation that is normalizing; that is, the
relation can always find a normal form if there exists one. A good
candidate is the leftmost-outermost reduction strategy. Its
normalizing property is standard and can be proven using the
factorization technique discussed in
\citet{takahashi-parallel-reduction, factorization-essentially}. By
composing the existence of normal forms and the fact that the
deterministic relation is normalizing, we can derive the
accessibility of the deterministic reduction relation
(\mintinline{coq}{Acc} in Coq), and use the accessibility proof as
an induction metric to define an executable algorithm\footnote{Coq's
  singleton elimination principle allows \mintinline{coq}{Acc}, a
  \texttt{Prop} data type with a single constructor, to be eliminated
  to construct runtime relevant data.}. We leave the proof of the
factorization property and the extraction of a conversion algorithm as
part of our future work.
\fi

\section{Extensions}

In this section, we discuss how the normalization
proof of \lang can be extended to incorporate features including
natural numbers, dependent pairs, cumulative
universes with subtyping, and the $\eta$ law for functions. These
extensions have been implemented in our development. More advanced
features such as type-directed $\eta$ laws are out of the scope of
this paper, and we instead point the readers to existing work for
techniques that can be readily adapted to our minimal development.

\jc{We may want to briefly mention the LOC for each addition?}

\subsection{Natural Numbers}
\begin{figure}
  \drules[T]{$[[G |- a : A]]$}{Typing (naturals)}{Nat, Zero, Suc}
  \vspace{-\baselineskip}
  \begin{mathpar}
    \drule[width=\textwidth]{T-Ind}
  \end{mathpar}
  \drules[I]{$[[Interp I i A S]]$}{Logical relation (naturals)}{Nat}
  \drules[N]{$[[NatOk a]]$}{Natural values}{Zero, Neu, Suc}
\caption{Rules for natural numbers}
\label{fig:nat}
\end{figure}

We can replace the boolean type with the natural number type as our
observable type. By introducing an infinite data type to \lang,
we significantly increase its expressiveness.

The syntactic and semantic rules for natural numbers can be found in
Figure~\ref{fig:nat}. \Rref*{T-Zero, T-Suc} allow us to construct natural
numbers. \Rref*{T-Ind} allows us to eliminate natural numbers
through the induction principle where $[[a]]$ is the base case,
$[[b]]$ the inductive case, and $[[c]]$ the natural number being
eliminated.

\Rref{I-Nat} gives us the semantic interpretation of natural number:
a term $[[a]]$ semantically inhabits $[[Nat]]$ if $[[a]]$ reduces to
some term $[[b]]$ such that $[[NatOk b]]$ holds.
The $[[NatOk b]]$ judgment inductively characterizes the
normal form of (potentially open) terms of type $[[Nat]]$ and is
particularly useful in the \rref*{T-Ind} case of the fundamental
theorem. Let $[[rho |= G]]$, the induction hypothesis $[[G |= c :
Nat]]$ tells us that $[[c { rho } =>+ b ]]$ for $[[NatOk b]]$. The proof
then follows by induction over the derivation of $[[NatOk b]]$. When
$[[b]]$ is neutral, we conclude the proof with
\nameref{lemma:adequacy}. When $[[b]]$ is $[[zero]]$, we conclude the
proof with the outer induction hypothesis $[[G |= a : A {zero / x}]]$.
When $[[b]]$ is a successor, we conclude the proof with the outer
induction hypothesis $[[G ++ x : Nat ++ y : A |= b : A { succ x / x
}]]$.

\subsection{Cumulativity}
\label{sec:cumulativity}
The semantic model we build in \cref{sec:logreldep} satisfies
\nameref{lemma:logrelcumulativity}.

\subsection{\texorpdfstring{$\beta\eta$}{beta-eta}-normal forms}

\section{Related Work}
\label{sec:relatedwork}

% \scw{Maybe it would be useful to include a chart here, so that readers can
%   easily keep track of the features of the various languages.  i.e. which ones
%   include large eliminations? type-directed equivalence? impredicative prop?
%   inductive datatypes? what are their line counts?}
% \yl{resolved}

\subsection{Logical relations for dependent types}
In the most general sense, a logical relation is a
practical technique that uses a type-indexed relation to 
strengthen the induction hypothesis for the
property of interest. This technique originates from
\citet{tait1967:reducibility}, who maps
types to sets of terms satisfying certain properties related to reduction.
The same idea is explained by \citet{girard1989proofs} and extended to
prove strong normalization of System F.
Tait's method has also been successfully applied to dependently typed
languages to prove strong normalization \citep{Martin-Lof-1973,luo1990extended,geuvers1994short, barendregt:lambda-calculi-with-types}.

However, the pen-and-paper representation of logical relations proofs
can be challenging to adapt to a theorem prover since many details
are hidden behind concise notation.
For example, \citet{geuvers1994short} presents the interpretation for types as
an inductively defined total function over the set of syntactically
well-formed types.
In untyped set theory, it makes sense to define the logical relation as a
function that takes a type and returns a set; however,
in constructive type theory, the metalogic of Coq and Agda, the return type of
the interpretation function depends on the derivation of the well-typedness of
its input. The body of the interpretation function uses the well-typedness
derivation, along with a proof classifying whether the input is a kind, a type,
or a term, to determine whether the argument of an application should be erased
during interpretation.

As a result, Geuvers' interpretation causes difficulties for modern proof
assistants. Due to the impredicativity of the object language, the proof
cannot be encoded in Agda, whose metatheory is predicative. However, even
though Coq supports impredicativity, the proof relevance of the well-typedness
derivations would require juggling between the impredicative but irrelevant
\texttt{Prop} sort and the predicative but relevant \texttt{Type} sort.

More recent work such by \citet{Abel12} and \citet{abel2008betaeta} makes the
definitions more explicit and precise and thus more directly encodable in proof
assistants. Unlike \lang, their systems support type-directed $\eta$ laws.
These type-directed equational rules complicate the proof significantly
since the confluence of full reduction is not available until after the
fundamental theorem is proven. Therefore, instead of a logical predicate,
they need a relational model with syntactic type annotations to remove the early
reliance on confluence, in spite of the object type theory being intensional.

\citet{coquand:canonicity,coquand:prop} uses a \emph{proof-relevant} logical
relation to prove canonicity and normalization for intensional
dependent type theories. In contrast to the proof-irrelevant logical predicate
we present, Coquand's proof does not rely on the confluence of reduction,
and can be easily extended to more $\eta$ laws without relying on a
complex relational model. Instead of working with concrete syntax,
Coquand works directly with an algebraic representation of dependent
type theory using categories with families (CwF)~\citep{hofmann:cwf},
though the idea of proof-relevant logical relations for dependent
types can be expressed independently of category
theory~\citep{barras2012semantical}.

More abstractly, Coquand's proof technique is an instance of a categorical
construction called Artin gluing~\citep{kaposi:gluing}. Different
metatheoretic properties, such as normalization, parametricity, and
$\Pi$-injectivity, can be obtained by gluing the initial CwF with the
corresponding CwF model. \citet{sterling:tait} introduces the notion of
synthetic Tait computability, which allows further
simplification of such proofs by working in the internal language
formed by gluing.
However, proof-relevant logical relations are more demanding on the
expressive power of the metatheory. For example, the mechanization of
NbE by \citet{altenkirch:normalisation} relies on quotient
inductive--inductive types (QIIT)~\citep{kaposi:qiit}.
\jc{I don't understand how the QIIT model relates to gluing,
or what using QIIT has to do with proof relevance of the logical relation.}

So far, the works mentioned interpret types as sets or relations on deep
representations of terms. In contrast, shallow embeddings interpret types as
sets of \emph{meta-level} terms in some type theory. The logical relation
interprets the object language into the meta language, and its soundness holds
by virtue of the interpretation function being well typed and definable.
Examples include work by \citet{kipling}, who mixes deep and shallow
representations, and work by \citet{shallow}, who directly implement the
signature of a CwF as Agda terms. The key property of shallow embeddings is
that object-level conversion is represented as meta-level equality, which has
the benefit of delegating handling conversion to the type checker, but with the
drawback that object conversion must coincide with meta equality. Conversion
cannot be customized to convert more terms than dictated by their
interpretations.

% To support type-directed $\eta$ laws, \citet{Abel12,abel2008betaeta}
% use a partial equivalence relation to model their type system. The 
% prove the injectivity of $\Pi$ types cannot be derived 
% Furthermore, \citet{Abel12} and
% \citet{abel2008betaeta} use their semantic universe hierarchy as a
% measure to define Kripke-style logical relations, from which they
% derive the correctness of their conversion algorithms. In our work, we
% use the semantic universe hierarchy directly in our definition of
% semantic typing because it is sufficient for our purposes
% (consistency and normalization).

\subsection{Mechanized logical relations for dependent types}

\begin{figure}
  \begin{tabular}{| l |  c  | l | c | c | c | l | l | r | }
    \hline
%     \newcommand\header[1]{#1}
%     & \header{Universes} & \header{Inductives} & \header{Conversion}
%     & \header{Large Elim} & Main results \\
      & U & Ind & C & LE & A & Main results & LOC \\
    \hline
    \lang{} (this work) & $\mathbb{N}$ & Eq, Bool & U & \cmark & 1 
    & Consistency, normalization & 1331 \\
    $\lambda^\theta$ & 0 & Eq, Nat & U & \xmark & 1 & Consistency & \textasciitilde{}8k\\
    Core Nuprl & $\mathbb{N}$ & W types & E & \cmark & 2 & Consistency & \textasciitilde{}330k \\
    NBE-in-Coq & 1 & Nat & T & \xmark & 2 & Correctness of NbE & \textasciitilde{}20k \\
    $\lambda^{\Pi U\mathbb{N}}$ & 1 & Nat, $\Sigma$ & T & \cmark & 2 & Decidability: conversion & \textasciitilde{}10k \\
    MLTT-\`a-la-Coq & 1 & Eq, Nat, $\Sigma$ & T & \cmark & 2 & Decidibility: type checking & \textasciitilde{}30k \\
    \hline
  \end{tabular}

  \begin{tabular}{ll} \\
  \underline{U}niverses: & Countable ($\mathbb{N}$), zero (0), one (1) \\
  \underline{Ind}uctives: & Equality types (Eq), naturals (Nat), dependent pairs ($\Sigma$), W types \\
  \underline{C}onversion: & Untyped (U), typed (T),  extensional (E) \\
  \underline{L}arge \underline{E}limination: & Included (\cmark), not included (\xmark) \\
  \underline{A}rity of interpretation: & Sets of terms (1), relations between terms (2) \\
  \\
  $\lambda^\theta$  & \citet{casinghino:combining-proofs-programs} (logical fragment only) \\
  Core Nuprl &\citet{anand2014towards} \\
  $\lambda^{\Pi U\mathbb{N}}$ &\citet{decagda} \\
  NBE-in-Coq& \citet{nbeincoq} \\
  MLTT-\`a-la-Coq &\citet{martin-lof-a-la-coq} \\
  \end{tabular}

  \caption{Feature matrix for dependently typed languages with
    mechanized logical relations}
  \label{fig:featurematrix}
\end{figure}
\scw{Extra columns in Fig 10:  Predicate vs. Relational interpretation (we can explain this) / Typed vs. Untyped interpretation}

Figure~\ref{fig:featurematrix} presents several mechanized proofs
that feature logical relations for dependently typed languages.
Each of these proofs is significantly larger than our development, but they
also prove more results about different object languages.
The table provides a comparison between the various features of their
object languages, but is not exhaustive. For example,
\citet{casinghino:combining-proofs-programs} and
\citet{anand2014towards} both have support for partial
programs. However, we include features that we believe to be most
impactful to the definition of the logical relation.
For a basic comparison of development size, we include the approximate
nonblank, noncomment lines of proof code for each project.

\citet{casinghino:combining-proofs-programs} introduce $\lambda^\theta$,
a dependently typed programming language that uses modalities to distinguish
logical proofs from programs.
% The programmatic fragment
% includes recursive data types and supports general recursion at the
% cost of introducing divergence.
% However, the $\lambda^\theta$
% language is limited in its expressiveness since it does not support
% type-level computation or polymorphism and therefore is not a fully dependently
% typed language according to \citet{abel2013normalization}.
The consistency proof of $\lambda^\theta$'s logical fragment has been
mechanized in Coq using a step-indexed logical relation;
step-indexing is required to model the programmatic fragment, which
interacts with the logical fragment.
The lack of polymorphism and type-level computation means their
logical relation can be defined recursively for well-formed types using
a size metric, which has been used in~\citet{liu2023dependently}.

\citet{decagda} mechanize in Agda the decidability of type
conversion rule for a dependently typed language with one predicative
universe level and a typed judgmental equality that includes the function
$\eta$ law. They
use a Kripke-style logical relation parameterized over a
type-directed equivalence relation satisfying certain
properties to facilitate the reuse of their definition. The
logical relation is defined using induction--recursion,
which is available in Agda but not in Coq.
\citet{martin-lof-a-la-coq} adapts the logical relation
by \citet{decagda} to the predicative fragment of Coq and further
extends the decidability of type conversion result to the decidability of
type checking a bidirectional type system.

\citet{anand2014towards} mechanize the metatheory of
Nuprl~\citep{constable1986implementing} in Coq. The object theory is an
extensional type theory with dependent functions, inductive types,
partial types, and a full universe hierarchy. They construct a PER
model in Coq to show the logical consistency of the language. Their
development has been further extended with intersection types,
union types, and quotient types.
\citet{nbeincoq} mechanize a normalization-by-evaluation algorithm in Coq for a
dependently typed language with one predicative universe, similar to
\citet{decagda} and \citet{martin-lof-a-la-coq}. However, since their
type system has no elimination form for natural numbers, the only base type,
large elimination is not supported.
Both \citet{anand2014towards} and \citet{nbeincoq} leverage
Coq's impredicative \texttt{Prop} sort to define the interpretation
of dependent function types and thus are closely related to our
mechanization. \citet{anand2014towards} further show it possible
to encode a finite universe hierarchy using neither impredicativity
nor induction--recursion. Their encoding of a countable
universe hierarchy relies on impredicativity, similar to our
development.
\jc{Wait I'm confused, do they use impredicativity or not?}

\subsection{Other mechanized metatheory of dependent types}

\Citet{barras2010sets} and \citet{Wang2013SemanticsOI} assign
set-theoretic semantics to dependent type theory in Coq. Unlike the
previous efforts, which focus on predicative type theory and direct
reducibility models, they tackle extensions of
CC$^\omega$, which extends the Calculus of Constructions with predicative
universes on top of the impredicative sort. We choose to
focus on a syntactic term model to avoid mechanizing mathematical objects
such as sets and domains.

There are other mechanized developments for dependently typed systems that
only involve properties that are derivable through syntactic means. For
example, \citet{coqcoqcorrect2019} prove the correctness of a type checker
for the Predicative Calculus of Cumulative Inductive Constructions (pCuIC),
Coq's core calculus, assuming strong normalization.
\citet{weirich:systemd} define System D, a core calculus of
dependent Haskell, and prove the syntactic type safety of the type
system. Because System D includes nontermination, they prove
consistency of definitional equality from the confluence of 
parallel reduction.
\jc{Why specifically mention System D? Surely there are other more notable
(in reviewers' eyes) dependent languages for which type safety have been proven?}

Compared to the systems described here, the most notable feature we are
missing is impredicativity, which is known to be
difficult to model when the impredicative sort is at the bottom of a
predicative universe hierarchy.
\jc{This makes it sound like it's easy when it's at the top of a predicative
universe hierarchy, which I don't think is what you meant; did you mean when
impredicativity is combined with some sort of large elimination of base types/
type-level computation?}
In this scenario, the erasure technique from \citet{geuvers1994short} is not
applicable~\citep{abel2013normalization}. Whether there is a similarly short
and simple treatment for impredicativity remains an open question.

\section{Discussion}
\label{sec:discuss}
\yl{I'm not sure how strong of a statement we can make about our proof
  technique. We've already demonstrated how to address type-level dependency
  when defining a logical relation through a simple example. Claiming that our
  proof structure is better seems quite ambitious, but I think there's a
  middle point where we claim that adding moderate features like typed
  reduction doesn't instantly make our code size expand all the way from 1000
  to 20,000 without saying the other developments are just verbose for no good
  reason}
\scw{I think we can find reasons for much of the differences. Am I missing any?
While it would be difficult to assign numbers to each of the deltas, I think it
is believable that when put together they add up to a lot.
\begin{itemize}
\item We don't include inductive or coinductive datatypes. We don't include
cumulativity. We don't include Prop. We don't include universe polymorphism.
\item We state our equality algorithmically instead of declaratively. On one
  hand, this gives us automatic inversion principles when working with
  definition. Furthermore, we don't need to prove the equivalence between an
  algorithmic version and a declarative specification.
\item Our equality requires a simple decision algorithm and isn't type directed.
\item We don't prove decidability of type checking. (And, it is not provable
  for our system, because we lack type annotations on functions. We should
  point this out.)
\item Our logical relation is unary and untyped. The latter means that we don't
  require the bookkeeping of a Kripke logical relation when reasoning about
  open terms. I don't know why unary relations are shorter.
\item CoqHammer leads to short proofs.
\end{itemize}
}
\yl{ Just one more technical point to add, though it's in the text already:
  the logical relation is closed backward by full reduction rather
  than weak-head/deterministic reduction. This requires an early
  confluence result to show that the logrel is
  deterministic/functional but simplifies everything else
  (e.g. conversion is justified immediately by our preservation
  theorem, but that is not the case if you use weak head reduction).\\ \\
  Also, regarding the first point, cumulativity only exists in
  Barras's work. Inductive, (maybe coinductive?), can be found in
  nuprl, metacoq, and maybe Barras's work.\\ \\
  The 20,000 - 30,000 LoC mechanization are all about small languages
  with pretty much the same features as our language except for your
  second and third bullet point. martin-lof a la coq, Abel's work, and
  nbe in coq aren't that richer in feature otherwise. None includes
  cumulativity (they only have one predicative universe)\\ \\
  The 400,000 NuPRL in Coq probably falls into a different category
  because they are trying to mechanize a full practical language}
\scw{The Coq-Coq-Correct paper (extended version) includes a (predicative) universe hierarchy, universe polymorphism, inductive/coinductive types. But they don't show consistency. Their development is 300k LOC.}

Our short consistency proof achieves the goal of
demonstrating the technique of proof by logical relation for dependently typed
languages. However, what remains unanswered is what makes our development
significantly shorter. Are we proving simpler results for
smaller languages, or making more use of automation, or is our proof
technique genuinely more efficient?
% focus on the following question: why is our proof,
% even with its extension to the existence $\beta\eta$-normal form, so much
% shorter than the other mechanized results from \citet{decagda,
% nbeincoq, martin-lof-a-la-coq}?

% First, the metatheoretic properties that we prove are indeed simpler.  Unlike
% developments that mechanize the correctness of a type-directed conversion
% algorithm, we only show the existence of normal forms for open and closed
% terms and state our properties in terms of an untyped reduction relation. This
% avoids a lot of the scaffolding related to the specification of the algorithm
% and the proof obligations that the algorithm is sound and complete with
% respect to the declarative specification of the type system.
% As a result, our logical relation, unlike the ones from \citet{decagda,
%   nbeincoq, martin-lof-a-la-coq, anand2014towards}, maps from types to
% predicates rather than relations. \scw{How does this follow? Why do we need unary relations where they need binary relations?} The need for a relational model is
% directly related to the metatheoretic property one wants to
% prove. \citet{anand2014towards} requires a relational model to capture
% the extensionality of their type system, whereas \citet{decagda,
%   martin-lof-a-la-coq, nbeincoq} uses a relational model to derive the
% injectivity of $\Pi$ types and justify the validity of
% $\eta$-conversion among other properties.
% Since \lang{} uses an untyped conversion rule, type conversion can be
% done through the \emph{normalize-and-compare}
% strategy described in \citet{pierce2004advanced}. The decidability of
% normalize-and-compare is implied by the existence of
% $\beta\eta$-normal form, which follows from our logical
% predicate.

First, the metatheoretic properties that we prove are indeed
simpler. Compared to Core Nuprl, our system
lacks extensionality, which would require a relational model to
justify consistency. Because the conversion rule for
\lang{} is untyped, we do not
need a Kripke-style relational model to prove $\Pi$-injectivity
(among other properties), unlike systems with typed conversion.
\jc{Why do we need Pi-injectivity? Why would a Kripke-style model be required
to prove it for typed conversion? What is a Kripke-style model?}
Furthermore, we prove the existence of normal
forms, which induces a simple \emph{normalize-and-compare}
procedure for type
conversion~\cite{pierce2004advanced}. \Citet{nbeincoq} and \citet{decagda}, on
the other hand, need
to show how their algorithmic conversion procedure is sound and complete
with respect to their respective declarative equational theories.
\scw{I'm getting confused by this paragraph. Does this reorganization sense:
  Our language is simpler than Nuprl, because it doesn't have extensional
  equality. It is simpler than Agda, because it doesn't have type-directed
  equality. Both of these cases require the definition of a binary logical
  relation, that defines a notion of semantic equality between terms. This
  relation justifies the injectivity of $\Pi$ types and justify the validity
  of $\eta$-conversion among other properties.}  \scw{Furthermore our proof is
  also simpler because we don't need prove the correctness of the NBE
  algorithm, which is used to show the decidability of Agda's type-directed
  equivalence. Therefore, we don't need to define this algorithm and show that
  it is sound and complete with respect to the type-directed
  equality. Instead, to show the decidability of our untyped equivalence, we
  need only show that terms have $\beta\eta$ normal forms. }
\yl{Makes sense. Though Abel's work doesn't use nbe but a recursive
  binary algorithm. Rewrote the paragraph above and commented out the original}

Second, the definition of our logical relation does
contribute to a more concise proof.
In \rref{I-Red, I-Bool}, we choose parallel reduction, a full
reduction relation, to close over our semantic interpretation of types
and terms. Parallel reduction is nondeterministic, but it satisfies
useful structural properties such as congruence
(Lemma~\ref{lemma:parcong}) and the diamond property
(Lemma~\ref{lemma:pardiamond}). We pay the price of using a
nondeterministic reduction relation when we want to prove that our
logical relation is a partial function; because of \rref{I-Red}, we
can have $[[A => B0]]$ and $[[A => B1]]$, where $[[B0]]$ and $[[B1]]$
each have their separate interpretations that we have to prove to be
equal. Fortunately, this complexity is reconciled by the
diamond property, which is easy to derive syntactically.

In contrast, \citet{decagda} and \citet{nbeincoq} employ a deterministic weak
head reduction relation. % The use of a
A deterministic reduction relation makes
the functionality of a logical relation trivial to prove, but fails to
satisfy the substitution property (Lemma~\ref{lemma:parsubst}), an
issue that has been observed by
\citet{casinghino:combining-proofs-programs}. If we had chosen to work
with a deterministic reduction relation, we would likely need
results such as the factorization
theorem~\citep{takahashi-parallel-reduction,factorization-essentially}
in our development before we can prove the fundamental theorem,
leading to a more complicated proof.

% With
% this alternative formulation, we would have to prove how the deterministic reduction relates to
% a full non-deterministic reduction relation to prove the fundamental
% theorem. This would amount to proving the factorization theorem of the
% deterministic reduction relation with respect to full non-deterministic
% reduction. Instead, using parallel reduction in \rref{I-Red} allows us to
% delay the factorization property until we need to justify the use
% leftmost-outermost reduction as a deterministic reduction strategy for
% normalization.

% In terms of our proof technique, the choice of parallel reduction, a full reduction
% relation, to close over our semantic interpretation in \rref{I-Red,I-Bool} has a non-negligible effect on the
% size of our development.

% we close over our semantically valid types
% and terms in \rref{I-Red, I-Bool} using the non-deterministic parallel
% reduction relation, while \citet{decagda,nbeincoq} employ a deterministic weak
% head reduction relation. Our use of a non-deterministic reduction strategy
% means that we need confluence to prove the functionality of our logical
% relation. However, the benefit is that it immediately gives us a semantic
% justification of the conversion rule (Lemma~\ref{lemma:logrelcoherence}).
% In particular, we obtain the confluence
% result of reduction at a very early stage before we even define our
% logical relation.

With untyped conversion,
we sidestep the relational, Kripke-style logical relation found in
other mechanized proofs. \scw{Need to define Kripke-style. Also the other
proofs need Kripke style because they are defining typed relations, not untyped
relations. } \yl{I wonder if
we can just assume some more technical knowledge from the readers in
this section.}
However, our early dependence on confluence
before the fundamental theorem is established can be alarming.
\jc{The rest of this paragraph is about confluence and Pi-injectivity,
not untyped conversion Kripke-style, so maybe the first sentence of the
paragraph shouldn't start with talking about them?}
In a system with type-directed reduction,
confluence is not immediately available because it
depends on $\Pi$-injectivity, which is usually only proven after the
fundamental theorem.\scw{confluence depends on Pi injectivity? I thought
it was only needed for subject reduction}\yl{it's
transitive. Confluence depends on subject reduction, which in turn
depends on pi injectivity. Maybe it's worth spelling out the details}
Fortunately, there are syntactic workarounds for the $\Pi$-injectivity
problem that allow us to recover the confluence property independently
from the logical relation. \citet{siles2012pure} generalize the
notion of typed parallel one-step reduction by \citet{adams2006pure}
to syntactically prove $\Pi$-injectivity for arbitrary Pure Type
Systems. \citet{weirich:systemd} add $\Pi$-injectivity to their
equational theory, thus allowing subject reduction to be proven
independently from confluence. By adopting these techniques that allow
us to derive confluence early even for systems with type-directed
reduction, we believe our proof technique can significantly shorten
the existing logical relation proofs for systems with typed
judgmental equality. We leave that as part of our future work.
% Therefore, we believe that even in a
% system where type-directed reduction is required (e.g. a system with
% the unit $\eta$-law) in the logical relation, the proof can still be carried
% out in a structure similar to the one we have presented.
% \scw{Not sure that I understand this paragraph}
% \yl{Reworded slightly to emphasize it's future work that we haven't
%   done and it is speculative}

\ifextended
Finally, despite our earlier claim that our metatheoretic properties
are not as strong as some of the related work, we can strengthen our
results through syntactic means. For example, the
normalize-and-compare strategy does not induce an efficient algorithm
for type conversion, like the ones from \citet{decagda} and
\citet{martin-lof-a-la-coq}. However, with the standard syntactic
techniques from \citet{takahashi-parallel-reduction,
factorization-essentially}, we can prove that leftmost-outermost
reduction is a normalizing reduction strategy, from which we can
separately show the correctness a more efficient algorithm that
reduces the two terms to weak head normal form before recursively
comparing their subcomponents. We believe factoring such properties
out of a logical relation is valuable, as it helps us identify the
part of our proof that requires extra strength from the metatheory.
\scw{Why don't we just use leftmost-outermost reduction in the first place?
Do we even need nondeterministic parallel reduction?}
\yl{The conversion uses full reduction. Nondeterministic reduction
  makes it harder to show that convertible types have the same
  meaning. Maybe it would require us to prove factorization in our
  development but it definitely simplifies the determinism proof
  (confluence is no longer required before the fundamental
  lemma). }
\fi

\section{Conclusion}
\label{sec:conclusion}
In this work, we present a short and mechanized proof by logical relation for
a dependently typed language with a predicative universe hierarchy, large
elimination, a propositional equality type, and dependent eliminators.
From the fundamental theorem for the logical relation, we prove consistency
and canonicity, then show the extensibility of our approach by proving the
existence of $\beta$($\eta$)-normal forms and adding natural numbers and
cumulativity to the language. These extensions only require small and mechanical
changes to our proof development. Our Coq mechanization leverages existing Coq
libraries for reasoning about metatheory and for general purpose automation,
allowing us to significantly reduce the verbosity typically associated with
mechanized proofs. The result is a declarative proof style that rivals pen and paper.

Related work gives us confidence that we could extend our logical relation to
include features such as full inductive datatypes, irrelevant arguments, and
type-directed conversion; however, it is not clear how much of the brevity of
this development can be maintained. Furthermore, we hope that mechanized
logical relations proofs will eventually grow to include other features found
in dependent type theories, such as impredicative universes, universe
polymorphism, and cumulativity. Regardless, our development shows that proofs
by logical relations for dependent types are accessible and do not require
months of effort to implement. We hope our proof can inspire researchers to
more frequently mechanize results, such as consistency and normalization, for
their dependent type theories.

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

\label{lastpage}

\end{document}
